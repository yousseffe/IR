<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1.0" />
        <title>Comprehensive Information Retrieval Quiz</title>
        <style>
            /* Added for smooth scrolling */
            html {
                scroll-behavior: smooth;
            }
            body {
                font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
                max-width: 900px;
                margin: 0 auto;
                padding: 20px;
                background: linear-gradient(135deg, #2c3e50 0%, #4ca1af 100%);
                min-height: 100vh;
            }
            .container {
                background: white;
                border-radius: 15px;
                padding: 30px;
                box-shadow: 0 10px 30px rgba(0, 0, 0, 0.2);
            }
            h1,
            h2 {
                text-align: center;
                color: #333;
                margin-bottom: 10px;
            }
            h1 {
                font-size: 2.5em;
            }
            .subtitle {
                text-align: center;
                color: #666;
                margin-bottom: 30px;
                font-style: italic;
            }
            .category-grid {
                display: grid;
                grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
                gap: 20px;
                margin-top: 30px;
            }
            .category-btn {
                background: linear-gradient(45deg, #2c3e50, #4ca1af);
                color: white;
                border: none;
                padding: 20px;
                border-radius: 10px;
                cursor: pointer;
                font-size: 1.1em;
                font-weight: bold;
                transition: all 0.3s ease;
                text-align: center;
            }
            .category-btn:hover {
                transform: translateY(-5px);
                box-shadow: 0 8px 15px rgba(0, 0, 0, 0.2);
            }
            .question {
                margin: 25px 0;
                padding: 20px;
                border: 2px solid #e0e0e0;
                border-radius: 10px;
                background: #f9f9f9;
            }
            .question-text {
                font-weight: bold;
                margin-bottom: 15px;
                color: #333;
                font-size: 1.1em;
            }
            .options {
                margin-left: 20px;
            }
            .option {
                margin: 8px 0;
                display: flex;
                align-items: center;
            }
            .option input {
                margin-right: 10px;
                transform: scale(1.2);
            }
            .option label {
                cursor: pointer;
                flex: 1;
                padding: 3px 6px;
                border-radius: 5px;
                transition: background-color 0.3s ease;
            }
            .check-btn {
                background: linear-gradient(45deg, #2c3e50, #4ca1af);
                color: white;
                border: none;
                padding: 10px 20px;
                border-radius: 8px;
                cursor: pointer;
                font-size: 14px;
                margin-top: 15px;
            }
            .result {
                margin-top: 15px;
                padding: 15px;
                border-radius: 8px;
                font-weight: bold;
                display: none;
            }
            .correct {
                background-color: #d4edda;
                border: 1px solid #c3e6cb;
                color: #155724;
            }
            .incorrect {
                background-color: #f8d7da;
                border: 1px solid #f5c6cb;
                color: #721c24;
            }
            .option label.correct-answer-highlight {
                background-color: #d4edda;
                border: 1px solid #c3e6cb;
                font-weight: bold;
                color: #155724;
            }
            .option label.incorrect-answer-highlight {
                background-color: #f8d7da;
                border: 1px solid #f5c6cb;
                text-decoration: line-through;
                color: #721c24;
            }
            .progress-bar {
                width: 100%;
                height: 20px;
                background-color: #e0e0e0;
                border-radius: 10px;
                margin: 20px 0;
                overflow: hidden;
            }
            .progress-fill {
                height: 100%;
                background: linear-gradient(90deg, #2c3e50, #4ca1af);
                width: 0%;
                transition: width 0.3s ease;
            }
            .score-display {
                text-align: center;
                font-size: 1.2em;
                margin: 20px 0;
                color: #333;
            }
            .nav-btn {
                background: #6c757d;
                color: white;
                border: none;
                padding: 12px 24px;
                border-radius: 8px;
                cursor: pointer;
                font-size: 16px;
                margin: 5px;
            }
            #quiz-section {
                display: none;
            }

            /* Styles for Pagination */
            .pagination-controls {
                display: none;
                justify-content: space-between;
                align-items: center;
                margin: 20px 0;
                border-top: 1px solid #ddd;
                padding-top: 20px;
            }
            #pagination-controls-bottom {
                border-bottom: 1px solid #ddd;
                padding-bottom: 20px;
                border-top: none;
            }
            .pagination-controls .nav-btn:disabled {
                background: #b0b5b9;
                cursor: not-allowed;
            }
            .page-info-span {
                font-size: 1.1em;
                font-weight: bold;
                color: #333;
            }
            .footer-nav {
                text-align: center;
                margin-top: 30px;
            }

            /* Floating Score Widget Styles */
            #floating-score-widget {
                position: fixed;
                bottom: 20px;
                right: 20px;
                background: #2c3e50;
                color: white;
                padding: 15px;
                border-radius: 12px;
                box-shadow: 0 5px 15px rgba(0, 0, 0, 0.3);
                z-index: 1000;
                width: 220px;
                opacity: 0;
                transform: translateY(100%);
                transition: opacity 0.4s ease, transform 0.4s ease;
                pointer-events: none;
            }
            #floating-score-widget.visible {
                opacity: 1;
                transform: translateY(0);
            }
            #floating-score-text {
                font-size: 0.9em;
                font-weight: bold;
                text-align: center;
                display: block;
                margin-bottom: 8px;
            }
            #mini-progress-bar {
                width: 100%;
                height: 8px;
                background-color: #5f7e9d;
                border-radius: 4px;
                overflow: hidden;
            }
            #mini-progress-fill {
                height: 100%;
                background: #4ca1af;
                width: 0%;
                transition: width 0.3s ease;
                border-radius: 4px;
            }

            /* Ensure long text wraps gracefully */
            .question-text,
            .option label {
                overflow-wrap: anywhere;
                word-wrap: break-word;
            }

            /* Responsive layout tweaks for small screens */
            @media (max-width: 600px) {
                body {
                    padding: 10px;
                }
                .container {
                    padding: 20px;
                }
                h1 {
                    font-size: 1.8em;
                }
                h2 {
                    font-size: 1.2em;
                }
                .question {
                    padding: 15px;
                }
                .question-text {
                    font-size: 1em;
                }
                .option label {
                    font-size: 0.9em;
                }
                .category-btn {
                    padding: 15px;
                    font-size: 1em;
                }
                .check-btn,
                .nav-btn {
                    padding: 8px 16px;
                    font-size: 14px;
                }
            }
        </style>
    </head>
    <body>
        <div class="container">
            <h1>üîç Comprehensive Information Retrieval Quiz</h1>
            <p class="subtitle">Test your knowledge on the core concepts of search.</p>

            <div id="category-selection">
                <h2>Choose a Category</h2>
                <div id="category-grid" class="category-grid"></div>
            </div>

            <div id="quiz-section">
                <h2 id="quiz-category-title"></h2>
                <div class="score-display" id="main-score-display">
                    <span id="score"></span>
                </div>
                <div class="progress-bar">
                    <div class="progress-fill" id="progressBar"></div>
                </div>

                <!-- Top Pagination Controls -->
                <div id="pagination-controls-top" class="pagination-controls">
                    <button class="nav-btn" id="prev-btn-top" onclick="prevPage()">
                        Previous Page
                    </button>
                    <span id="page-info-top" class="page-info-span"></span>
                    <button class="nav-btn" id="next-btn-top" onclick="nextPage()">
                        Next Page
                    </button>
                </div>

                <div id="quiz"></div>

                <!-- Bottom Pagination Controls (with updated onclick handlers) -->
                <div id="pagination-controls-bottom" class="pagination-controls">
                    <button class="nav-btn" id="prev-btn-bottom" onclick="prevPageAndScroll()">
                        Previous Page
                    </button>
                    <span id="page-info-bottom" class="page-info-span"></span>
                    <button class="nav-btn" id="next-btn-bottom" onclick="nextPageAndScroll()">
                        Next Page
                    </button>
                </div>

                <div class="footer-nav">
                    <button class="nav-btn" onclick="showCategorySelection()">
                        Back to Categories
                    </button>
                </div>
            </div>
        </div>

        <div id="floating-score-widget">
            <span id="floating-score-text"></span>
            <div id="mini-progress-bar">
                <div id="mini-progress-fill"></div>
            </div>
        </div>

        <script>
            const allQuestions = [
                {
                    category: 'Modern NLP & Neural IR',
                    question:
                        'Which Word2Vec model predicts the surrounding context words given a target word?',
                    options: [
                        'LSA',
                        'GloVe',
                        'FastText',
                        'Skip-gram Model',
                        'Continuous Bag of Words (CBOW)',
                    ],
                    correct: 3,
                    explanation:
                        'The Skip-gram model in Word2Vec is structured to predict the context (surrounding words) based on a given input (target) word. The Continuous Bag of Words (CBOW) model does the opposite.',
                },
                {
                    category: 'Modern NLP & Neural IR',
                    question:
                        'In the context of Retrieval-Augmented Generation (RAG), what is the role of the "retrieval" component?',
                    options: [
                        'To generate new text based on a given prompt',
                        'To evaluate the quality of the generated text',
                        'To fine-tune the pre-trained language model for a specific task',
                        'To fetch relevant information from an external knowledge source based on the input query',
                        "To store and manage the model's learned knowledge",
                    ],
                    correct: 3,
                    explanation:
                        "Retrieval-Augmented Generation (RAG) is a two-step process. The 'retrieval' component's specific function is to search and pull relevant information from an external data source, which then informs the 'generation' component.",
                },
                {
                    category: 'Indexing & Data Structures',
                    question: 'What does it mean for a web crawler to be "scalable"?',
                    options: [
                        'It is designed to increase the crawl rate by adding more machines',
                        'It can accurately translate web pages into multiple languages',
                        'It is able to crawl only secure websites',
                        'It can analyze website content for sentiment',
                        'It can adapt to different internet protocols',
                    ],
                    correct: 0,
                    explanation:
                        "In computing, scalability refers to a system's ability to handle an increasing workload. For a web crawler, this means being architected to distribute the work, so adding more machines increases its capacity to crawl more of the web.",
                },
                {
                    category: 'Modern NLP & Neural IR',
                    question:
                        'Which component of an LSTM network is responsible for deciding what information to discard from the cell state?',
                    options: [
                        'Forget Gate',
                        'Hidden State',
                        'Cell State',
                        'Input Gate',
                        'Output Gate',
                    ],
                    correct: 0,
                    explanation:
                        "The 'forget gate' is a fundamental component of an LSTM cell. It uses a sigmoid function to decide which information from the previous cell state should be kept or discarded, enabling the network to manage its memory over long sequences.",
                },
                {
                    category: 'Indexing & Data Structures',
                    question:
                        'When performing a basic search using Boolean operators, field codes, truncation (*), and wildcard (?) symbols, which of the following best describes how a system handles implicit politeness in search operations?',
                    options: [
                        'The system automatically includes synonyms and plurals to refine searches without direct commands',
                        'The system requires users to specify field labels from drop-down lists for search refinement',
                        'The system uses explicit politeness commands to adapt searches to user intent',
                        'The system limits search results by date ranges and full-text availability through a results screen tab',
                        'The system employs advanced methods like word association to improve query relevance',
                    ],
                    correct: 2,
                    explanation:
                        "The term 'politeness' in web crawling refers to practices that avoid overloading a web server. The most direct way a system enforces this is by following explicit commands in a website's `robots.txt` file, which dictates what the crawler is and is not allowed to access.",
                },
                {
                    category: 'Indexing & Data Structures',
                    question:
                        'A crawler is designed to operate in a distributed environment. Which of the following is the most important consideration for maintaining data consistency across all crawler nodes?',
                    options: [
                        'Requiring all nodes to access the web through a single proxy server',
                        'Implementing a centralized URL frontier with proper synchronization mechanisms',
                        'Ensuring all nodes have identical hardware configurations',
                        'Ensuring all nodes use the same operating system',
                        'Distributing URLs randomly to each node',
                    ],
                    correct: 1,
                    explanation:
                        'In a distributed crawling system with multiple nodes, a centralized URL frontier is essential to prevent different nodes from crawling the same page simultaneously and to maintain a consistent, shared list of URLs that need to be visited. Synchronization ensures data integrity.',
                },
                {
                    category: 'Information Retrieval Evaluation',
                    question:
                        'In the context of search engine evaluation, what is a "test collection"?',
                    options: [
                        'A list of common spelling errors used to improve query correction',
                        'A tool used to monitor the performance of a search engine in real-time',
                        'A collection of users who are asked to test the search engine and provide feedback',
                        'A set of documents, queries, and relevance judgments used to evaluate search engines',
                        'A collection of algorithms used to rank search results',
                    ],
                    correct: 3,
                    explanation:
                        'A test collection is the standard benchmark for evaluating IR systems. It must contain a corpus of documents, a set of information needs expressed as queries, and relevance judgments that indicate which documents are relevant to each query.',
                },
                {
                    category: 'Indexing & Data Structures',
                    question:
                        'Which method does Google use to resolve URLs during crawling in the URL frontier?',
                    options: [
                        'Using relative paths to construct complete URLs',
                        'Ignoring broken or redirecting URLs',
                        'Appending full URLs directly from search results',
                        'Applying the Mercator scheme for URL resolution',
                        'Only processing URLs found in meta tags',
                    ],
                    correct: 3,
                    explanation:
                        "The Mercator crawler introduced a highly scalable and robust design for managing a URL frontier. This scheme includes methods for URL canonicalization, prioritization, and management, which influenced later large-scale crawlers like Google's.",
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question:
                        'When performing a basic search with a crawler, which of the following is TRUE?',
                    options: [
                        'Results are ordered by relevance first and then by chronological order',
                        'Search refinement options include limiting results to peer-reviewed journals only',
                        "Wildcard (?) symbols are not supported in the crawler's search function",
                        'Truncation (*) can be used to shorten search terms during a basic search',
                        'Field codes must be specified for each search to limit term usage',
                    ],
                    correct: 3,
                    explanation:
                        "Truncation, often denoted by an asterisk (*), is a standard feature in search systems that broadens a search by matching all terms that share a common root. For example, 'comput*' would match 'computer', 'computing', and 'computation'.",
                },
                {
                    category: 'Modern NLP & Neural IR',
                    question:
                        'Which of the following is a significant challenge associated with using Large Language Models (LLMs)?',
                    options: [
                        'LLMs are easy to train with small datasets',
                        'LLMs are limited to understanding only one language',
                        'LLMs require very little computational power to run',
                        'LLMs are unable to generate human-like text',
                        'LLMs often suffer from hallucinations and may generate factually incorrect information',
                    ],
                    correct: 4,
                    explanation:
                        "A widely recognized challenge with Large Language Models is their tendency to 'hallucinate,' meaning they generate text that sounds plausible but is factually incorrect or nonsensical. This happens because they predict text based on patterns, not true understanding.",
                },
                {
                    category: 'Indexing & Data Structures',
                    question:
                        'A web crawler needs to prioritize crawling pages that are most likely to provide new and valuable information to users. Which of the following URL prioritization strategies would be most effective?',
                    options: [
                        'Breadth-first crawling, starting from a set of seed URLs',
                        'Using a combination of PageRank, topic relevance, and freshness estimates to dynamically prioritize URLs in the frontier',
                        'Depth-first crawling, following links as deeply as possible within each site',
                        'Randomly selecting URLs from the frontier',
                        'Prioritizing URLs based on their length, with shorter URLs crawled first',
                    ],
                    correct: 1,
                    explanation:
                        'An effective web crawler needs to prioritize pages that are both important (high PageRank), relevant to specific topics, and new or recently updated (freshness). Combining these signals allows the crawler to use its resources efficiently to find the most valuable content.',
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question:
                        'Which of the following tools or methods allow for advanced search techniques such as using Boolean operators, field codes, truncation, and wildcards?',
                    options: [
                        'Truncation and wildcards only',
                        'Field codes only',
                        'Explicit politeness techniques',
                        'Boolean operators, field codes, truncation, and wildcards',
                        'Boolean operators only',
                    ],
                    correct: 3,
                    explanation:
                        'Advanced search techniques encompass a range of tools for precise querying. This includes Boolean operators (AND, OR, NOT) to combine terms, field codes to search specific parts of a document, truncation (*) to find word variations, and wildcards (?) to substitute characters.',
                },
                {
                    category: 'Indexing & Data Structures',
                    question: 'Which of these poses a challenge for web crawlers?',
                    options: [
                        'Consistent website design',
                        'Standardized HTML code',
                        'High server uptime',
                        'Site mirrors and duplicate pages',
                        'Low network latency',
                    ],
                    correct: 3,
                    explanation:
                        'Site mirrors and duplicate pages present a significant challenge for web crawlers. They contain identical or nearly identical content, forcing the crawler to expend resources identifying and filtering out this redundancy to avoid duplicate entries in the search index.',
                },
                {
                    category: 'Information Retrieval Evaluation',
                    question:
                        "A search engine's P@10 is 0.4. Which of the following interpretations is correct?",
                    options: [
                        'The search engine is performing poorly',
                        'The 4th relevant document appears at rank 10',
                        '40% of all documents in the collection are relevant to the query',
                        '40% of the top 10 documents retrieved are relevant to the query',
                        'The search engine retrieved 4 relevant documents',
                    ],
                    correct: 3,
                    explanation:
                        'Precision at K (P@K) is a metric that measures the proportion of retrieved documents in the top K results that are relevant. Therefore, P@10 of 0.4 means that out of the top 10 documents retrieved, 4 of them (or 40%) were relevant to the query.',
                },
                {
                    category: 'Indexing & Data Structures',
                    question:
                        'Which of the following scenarios requires the most careful consideration of both explicit and implicit politeness?',
                    options: [
                        'Crawling a website that explicitly allows unrestricted crawling',
                        'Crawling a government website during off-peak hours',
                        'Crawling a large university website with no robots.txt file',
                        'Crawling a website known to be a mirror of another site',
                        'Crawling a small personal blog with a clear robots.txt file',
                    ],
                    correct: 2,
                    explanation:
                        'Crawling a large university website with no `robots.txt` file requires the most care. The absence of an explicit politeness file (`robots.txt`) means the crawler must rely entirely on implicit politeness (e.g., limiting request rates) to avoid overwhelming the potentially vast and complex server infrastructure.',
                },
                {
                    category: 'Information Retrieval Evaluation',
                    question:
                        'Which of the following is the primary reason for evaluating search engines?',
                    options: [
                        "To assess the aesthetic appeal of the search engine's user interface",
                        'To measure how well the search engine satisfies user information needs',
                        "To count the number of lines of code in the search engine's software",
                        "To determine the cost-effectiveness of the search engine's infrastructure",
                        'To analyze the programming language used to develop the search engine',
                    ],
                    correct: 1,
                    explanation:
                        "The fundamental goal of evaluating a search engine is to determine how effectively it fulfills a user's need for information. Metrics like precision and recall are used to quantify this, moving beyond subjective assessments.",
                },
                {
                    category: 'Information Retrieval Evaluation',
                    question: 'What does Recall measure in the context of information retrieval?',
                    options: [
                        'The cost of retrieving documents',
                        'The proportion of relevant documents that are retrieved',
                        'The average rank of relevant documents',
                        'The proportion of retrieved documents that are relevant',
                        'The time it takes to retrieve documents',
                    ],
                    correct: 1,
                    explanation:
                        'Recall is a key metric in information retrieval that measures the completeness of the search results. It is calculated as the fraction of all relevant documents in the collection that were successfully retrieved by the system.',
                },
                {
                    category: 'Indexing & Data Structures',
                    question:
                        'A web crawler is encountering an increasing number of spam pages. Which combination of techniques would be most effective in identifying and avoiding these pages?',
                    options: [
                        'Requiring CAPTCHA completion for every page crawled',
                        'Relying solely on robots.txt directives and URL blacklists',
                        'Ignoring all pages with excessive use of keywords or advertisements',
                        'To Crawl only websites with high domain authority and established reputations',
                        'Using machine learning models trained on content features, link analysis, and user feedback, combined with adaptive crawling strategies',
                    ],
                    correct: 4,
                    explanation:
                        'Identifying and avoiding spam pages requires a sophisticated, multi-faceted approach. The most effective method combines content analysis, link analysis (e.g., reputation of linking domains), and user feedback signals, often using machine learning models to adapt and learn over time.',
                },
                {
                    category: 'Information Retrieval Evaluation',
                    question: 'What is the purpose of the F1-measure?',
                    options: [
                        'To determine the number of documents in a collection',
                        'To calculate the cost of indexing documents',
                        'To assess the user interface design of a search engine',
                        'To provide a single score that balances Precision and Recall',
                        'To measure the speed of document retrieval',
                    ],
                    correct: 3,
                    explanation:
                        "The F1-measure, or F-score, is the harmonic mean of precision and recall. Its purpose is to combine these two metrics into a single score, which is useful for evaluating a system's overall performance when there is a trade-off between retrieving all relevant results (recall) and retrieving only relevant results (precision).",
                },
                {
                    category: 'Indexing & Data Structures',
                    question:
                        'When Google crawls a webpage and caches its content, how does this affect the determination of page relevance for search queries?',
                    options: [
                        'The crawler does not use cached content for ranking purposes',
                        'The cached content is only for backup access',
                        'Relevance is based solely on the live page',
                        'Cached pages are never considered in relevance',
                        'The cached content is used to judge relevance',
                    ],
                    correct: 4,
                    explanation:
                        "Google's indexers use the cached version of a page to determine its content and relevance for ranking. This ensures that the ranking is based on the content Google actually saw and processed, rather than relying on the live page which could have changed or be unavailable.",
                },
                {
                    category: 'Indexing & Data Structures',
                    question: 'What is a "spider trap" designed to do?',
                    options: [
                        'Simplify website navigation',
                        'Improve website security',
                        'Optimize website loading speed',
                        'Cause a web crawler to make an infinite number of requests or crash',
                        'Enhance user experience',
                    ],
                    correct: 3,
                    explanation:
                        "A 'spider trap' is a technical feature on a website (often created unintentionally, like a calendar with infinite future dates) that causes a web crawler to enter a loop, making an endless number of requests for seemingly new but dynamically generated URLs.",
                },
                {
                    category: 'Indexing & Data Structures',
                    question:
                        'Which factor is least relevant when considering whether a page is malicious?',
                    options: [
                        "The page's attempts to collect personal information without consent",
                        'Whether the page redirects users to unexpected websites',
                        'Whether the page attempts to install software without user consent',
                        "The page's loading speed",
                        'The presence of excessive pop-up advertisements',
                    ],
                    correct: 3,
                    explanation:
                        "While collecting personal data, redirecting users, and installing software are all clear indicators of malicious intent, a page's loading speed is a performance metric. A page can be slow without being malicious, and vice versa.",
                },
                {
                    category: 'Indexing & Data Structures',
                    question:
                        'Which of the following is a characteristic of a well-designed web crawler?',
                    options: [
                        'It only crawls websites in a single language',
                        'It ignores robots.txt to ensure complete coverage',
                        'It is designed to run on multiple distributed machines',
                        'It prioritizes crawling websites with the fewest links',
                        'It operates on a single machine to avoid network issues',
                    ],
                    correct: 2,
                    explanation:
                        'A well-designed web crawler must be able to scale to the size of the web. This is achieved by designing it to run on multiple distributed machines, allowing the crawling workload to be parallelized and managed efficiently.',
                },
                {
                    category: 'Indexing & Data Structures',
                    question:
                        'When Google crawls a webpage and caches its content, how does this affect the determination of page relevance for search queries?',
                    options: [
                        'The crawler does not use cached content for ranking purposes',
                        'The cached content is only for backup access',
                        'Relevance is based solely on the live page',
                        'Cached pages are never considered in relevance',
                        'The cached content is used to judge relevance',
                    ],
                    correct: 4,
                    explanation:
                        "Google's ranking algorithms analyze the content of a webpage as it was captured and stored in its cache. This ensures consistency and allows for analysis even if the live page is temporarily down.",
                },
                {
                    category: 'Foundational IR Concepts',
                    question:
                        'How does PubMed handle search terms entered without a specific field label?',
                    options: [
                        'The system allows the imposition of limits by date, gender, age, language, or field label to refine search results',
                        'PubMed does not automatically map terms but directly searches for the exact phrase or combination entered in the query box',
                        'PubMed uses an automatic term mapping feature to search for terms using Medical Subject Headings (MeSH) and controlled vocabularies. When a term is entered without a field label, the system starts by searching in the MeSH table and then explores more specific categories',
                        'Term mapping can be bypassed if terms are entered as phrases, hyphenated, or using truncation symbols',
                        'PubMed displays results in batches of 20 citations, with sorting options for author, journal title, and publication date',
                    ],
                    correct: 2,
                    explanation:
                        "PubMed's Automatic Term Mapping (ATM) is a key feature that enhances search. When a term is entered without a field tag, PubMed first tries to map it to Medical Subject Headings (MeSH), a controlled vocabulary, to provide more accurate and comprehensive results.",
                },
                {
                    category: 'Indexing & Data Structures',
                    question: 'What is the purpose of the URL frontier in a web crawler?',
                    options: [
                        'To store the content of crawled web pages',
                        'To prevent the crawler from accessing certain websites',
                        'To display the crawled web pages to users',
                        'To manage and prioritize URLs to be crawled',
                        'To encrypt the data transmitted by the crawler',
                    ],
                    correct: 3,
                    explanation:
                        'The URL frontier is the core data structure for managing the crawling process. It stores the URLs that have been discovered and are waiting to be crawled, and it includes a prioritization mechanism to decide the order in which they should be fetched.',
                },
                {
                    category: 'Indexing & Data Structures',
                    question: 'What does "continuous operation" mean for a web crawler?',
                    options: [
                        'Crawling websites only once',
                        'Continuously fetching fresh copies of previously fetched pages',
                        'Crawling only the top-level domain of each website',
                        'Crawling websites in alphabetical order',
                        'Crawling the web only during daytime hours',
                    ],
                    correct: 1,
                    explanation:
                        'A web crawler operates continuously to keep its index current. This involves not just discovering new pages, but also periodically re-visiting and fetching fresh copies of previously crawled pages to update their content and check for changes.',
                },
                {
                    category: 'Indexing & Data Structures',
                    question:
                        'A web crawler encounters a URL that dynamically generates an infinite number of pages. Which of the following strategies would be most effective in preventing this spider trap from crashing the crawler?',
                    options: [
                        'Disabling JavaScript execution in the crawler',
                        "Increasing the crawler's memory allocation",
                        'Prioritizing the crawling of known high-quality websites',
                        'Implementing a URL length limit and a path depth limit',
                        'Ignoring all URLs with query parameters',
                    ],
                    correct: 3,
                    explanation:
                        'Spider traps can cause a crawler to request an infinite number of URLs. Implementing a limit on the maximum depth of a URL path (e.g., `site.com/a/b/c...`) and the total length of the URL string are effective heuristics to prevent the crawler from getting stuck.',
                },
                {
                    category: 'Indexing & Data Structures',
                    question: 'How does Google handle non-ASCII characters in domain names?',
                    options: [
                        'By converting them into ASCII using the Punycode system',
                        'By ignoring non-ASCII characters entirely',
                        'By truncating the domain name to fit ASCII standards',
                        'By appending a special prefix to indicate language',
                        'By translating them into Chinese characters',
                    ],
                    correct: 0,
                    explanation:
                        'Domain names must be represented in ASCII. The Punycode system is an encoding syntax used to translate Unicode characters (used in Internationalized Domain Names) into the limited ASCII character set allowed for domain name system (DNS) lookups.',
                },
                {
                    category: 'Modern NLP & Neural IR',
                    question:
                        'What is the primary reason for using multi-head attention in the Transformer architecture?',
                    options: [
                        'To prevent overfitting by regularizing the attention weights',
                        "To improve the model's ability to generalize to unseen data",
                        'To accelerate the training process by distributing the attention computation across multiple GPUs',
                        'To reduce the computational complexity of the self-attention mechanism',
                        'To enable the model to capture different types of relationships and dependencies in the input sequence',
                    ],
                    correct: 4,
                    explanation:
                        "Multi-head attention allows the Transformer model to focus on different parts of the input sequence simultaneously. Each 'head' can learn different types of relationships (e.g., syntactic, semantic), and their combined outputs provide a richer, more nuanced understanding of the dependencies in the text.",
                },
                {
                    category: 'Modern NLP & Neural IR',
                    question:
                        'Which of the following machine learning paradigms involves learning from labeled training data to make predictions?',
                    options: [
                        'Reinforcement Learning',
                        'Supervised Learning',
                        'Semi-Supervised Learning',
                        'Unsupervised Learning',
                        'Active Learning',
                    ],
                    correct: 1,
                    explanation:
                        'Supervised learning is a machine learning paradigm defined by its use of labeled training data. The model learns to map inputs to outputs by training on a dataset where the correct predictions (labels) are already known.',
                },
                {
                    category: 'Modern NLP & Neural IR',
                    question:
                        'Which component of the Transformer architecture is responsible for injecting information about the position of tokens in the input sequence?',
                    options: [
                        'Self-Attention Mechanism',
                        'Positional Encoding',
                        'Multi-Head Attention',
                        'Feed-Forward Neural Network',
                        'Layer Normalization',
                    ],
                    correct: 1,
                    explanation:
                        "The self-attention mechanism in the Transformer architecture is permutation-invariant, meaning it doesn't inherently know the order of tokens. Positional encoding injects information about the position of each token in the sequence, allowing the model to understand word order.",
                },
                {
                    category: 'Modern NLP & Neural IR',
                    question:
                        'Which of the following is a core component of the Transformer architecture?',
                    options: [
                        'Convolutional Neural Networks (CNNs)',
                        'Self-attention mechanism',
                        'Bag of Words (BoW) model',
                        'Recurrent Neural Networks (RNNs)',
                        'Support Vector Machines (SVMs)',
                    ],
                    correct: 1,
                    explanation:
                        'The self-attention mechanism is the fundamental innovation and a core component of the Transformer architecture. It allows the model to weigh the importance of all other words in the input sequence when processing a given word, capturing complex contextual relationships.',
                },
                {
                    category: 'Modern NLP & Neural IR',
                    question:
                        'What is the primary purpose of Retrieval-Augmented Generation (RAG) in the context of Large Language Models (LLMs)?',
                    options: [
                        'To enable LLMs to perform real-time translation of text',
                        'To reduce the computational cost of training LLMs',
                        'To allow LLMs to generate code in multiple programming languages',
                        'To improve the factual accuracy and reduce hallucinations in LLM-generated text',
                        'To enhance the creativity and artistic style of LLM-generated content',
                    ],
                    correct: 3,
                    explanation:
                        'The primary purpose of Retrieval-Augmented Generation (RAG) is to ground the responses of a Large Language Model (LLM) in factual, up-to-date information retrieved from an external knowledge base. This helps to reduce hallucinations and improve the factual accuracy of the generated text.',
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question:
                        'Level 3: Consider the query "A AND (B OR C)". Which processing order is generally most efficient, assuming postings lists are sorted by DocID?',
                    options: [
                        'Process "A AND C" first, then "OR" with B',
                        'Process A, B, and C independently, then combine',
                        'Process "A AND B" first, then "OR" with C',
                        "It doesn't matter; all orders are equivalent",
                        'Process "B OR C" first, then "AND" with A',
                    ],
                    correct: 4,
                    explanation:
                        "To process a query like 'A AND (B OR C)', the most efficient strategy is to resolve the operations that result in the smallest intermediate lists first. Processing the OR clause `(B OR C)` first, and then finding its intersection with `A`, minimizes the number of comparisons needed.",
                },
                {
                    category: 'Indexing & Data Structures',
                    question:
                        'What information is stored in a positional index (slide 7) that is NOT stored in a non-positional index?',
                    options: [
                        'A list of all terms in the dictionary',
                        'The document frequency of each term',
                        "The positions of each term's occurrences within each document",
                        'The total number of documents in the collection',
                        'The weight of each term in each document',
                    ],
                    correct: 2,
                    explanation:
                        'A non-positional index only stores which documents a term appears in. A positional index goes a step further by also storing the position (e.g., word offset) of each term within each document, which is essential for proximity and phrase queries.',
                },
                {
                    category: 'Indexing & Data Structures',
                    question:
                        'In the context of k-gram indexes for wildcard queries, what is the purpose of the post-filtering step?',
                    options: [
                        "To remove terms that don't actually match the original wildcard query",
                        'To sort the results by relevance',
                        'To calculate the Jaccard coefficient',
                        'To combine the results from multiple k-gram queries',
                        'To create the k-gram index itself',
                    ],
                    correct: 0,
                    explanation:
                        "Using a k-gram index for a wildcard query like `m*n` might retrieve terms like 'monument' (correct) but also 'management' (incorrect, as it contains 'm', 'o', 'n' but doesn't match). The post-filtering step is necessary to check the retrieved candidates against the original wildcard query to eliminate these false positives.",
                },
                {
                    category: 'Indexing & Data Structures',
                    question:
                        'Why might a term-document incidence matrix be impractical for large document collections?',
                    options: [
                        'It would be too dense, requiring excessive memory',
                        'It cannot handle phrase queries',
                        'It cannot be used for ranking',
                        'It would be extremely sparse, wasting a lot of space',
                        'It cannot handle Boolean queries',
                    ],
                    correct: 3,
                    explanation:
                        'In a large document collection, any given document contains only a tiny fraction of the total possible terms (the vocabulary). This means a term-document incidence matrix (with terms as rows and documents as columns) would be overwhelmingly filled with zeros, making it extremely sparse and an inefficient use of storage.',
                },
                {
                    category: 'Indexing & Data Structures',
                    question:
                        "In a term-document incidence matrix, what does a '1' in cell (i, j) represent?",
                    options: [
                        'Term i appears in document j',
                        'Term i and document j are semantically related',
                        'Document j is the i-th document in the collection',
                        'Document j contains term i',
                        'Term i is the j-th most frequent term in the collection',
                    ],
                    correct: 0,
                    explanation:
                        "A term-document incidence matrix is a binary matrix used to represent which terms appear in which documents. A '1' at the intersection of row `i` (representing term `i`) and column `j` (representing document `j`) signifies that term `i` is present in document `j`.",
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question:
                        'What is a "proximity operator" in the context of information retrieval?',
                    options: [
                        'An operator that ranks documents based on their relevance to a query',
                        'An operator that specifies that two terms must occur close to each other in a document',
                        'An operator that automatically corrects spelling errors in a query',
                        'An operator that finds documents related to a specific topic',
                        'An operator that translates a query into different languages',
                    ],
                    correct: 1,
                    explanation:
                        'A proximity operator is a specialized search operator that constrains results to documents where specified terms appear within a certain distance of each other. This is more restrictive than a simple AND query and is useful for finding more precise relationships between terms.',
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question: "What does the symbol '*' typically represent in a wildcard query?",
                    options: [
                        'The end of a term',
                        'Any single character',
                        'A specific character defined elsewhere',
                        'The beginning of a term',
                        'Any (possibly empty) string of characters',
                    ],
                    correct: 4,
                    explanation:
                        'The asterisk (*) is the most common symbol used for truncation in wildcard queries. It represents any sequence of zero or more characters, allowing users to search for variations of a word root (e.g., `comput*`).',
                },
                {
                    category: 'Indexing & Data Structures',
                    question: 'What is the purpose of next word index?',
                    options: [
                        'To store the sentiment of the next sentence in the document',
                        'To store the geographical location of the next document in the collection',
                        'To expand the query with synonyms of the original query terms',
                        'To store the citation information of the next article in the collection',
                        'To store the next word that follows each term in a document',
                    ],
                    correct: 4,
                    explanation:
                        'A next word index is designed to support query suggestions or auto-completion. For each term in the vocabulary, it stores a list of words that commonly follow it, allowing the system to predict what the user might type next.',
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question:
                        'What is a potential drawback of the Extended Boolean Model compared to ranked retrieval models?',
                    options: [
                        "It doesn't handle phrase queries",
                        "It doesn't provide a ranking of results based on relevance",
                        "It can't handle proximity operators",
                        'It only works with small collections',
                        "It's more complex to implement",
                    ],
                    correct: 1,
                    explanation:
                        "The Extended Boolean Model incorporates aspects of ranked retrieval (like term weighting) into a Boolean framework. However, a key drawback compared to pure ranked models (like the vector space model) is that it doesn't naturally produce a fully ranked list of results based on relevance scores.",
                },
                {
                    category: 'Modern NLP & Neural IR',
                    question:
                        'Which of the following is a key difference between BERT and GPT in terms of their architecture and training?',
                    options: [
                        'GPT is pre-trained on a smaller dataset compared to BERT',
                        'BERT uses a decoder-only architecture, while GPT uses an encoder-only architecture',
                        'BERT uses bidirectional context understanding, while GPT uses unidirectional context',
                        'BERT is primarily used for text generation, while GPT is used for text understanding',
                        'GPT relies on masked language modeling, while BERT relies on next sentence prediction',
                    ],
                    correct: 2,
                    explanation:
                        'A key architectural difference is that BERT (Bidirectional Encoder Representations from Transformers) is designed to understand context from both left-to-right and right-to-left (bidirectional), making it powerful for understanding tasks. GPT (Generative Pre-trained Transformer) is unidirectional, processing text in one direction, which makes it well-suited for text generation.',
                },
                {
                    category: 'Modern NLP & Neural IR',
                    question:
                        "What is the primary objective of Masked Language Modeling (MLM) in BERT's pre-training?",
                    options: [
                        'Predict the next sentence in a sequence',
                        'Generate new sentences similar to the input',
                        'Predict randomly masked words in a sentence',
                        'Classify the sentiment of a sentence',
                        'Translate the sentence into another language',
                    ],
                    correct: 2,
                    explanation:
                        "Masked Language Modeling (MLM) is a pre-training objective where some percentage of words in the input text are randomly masked (e.g., replaced with a `[MASK]` token). The model's task is to predict these masked words based on the surrounding unmasked words, forcing it to learn a deep bidirectional understanding of language.",
                },
                {
                    category: 'Modern NLP & Neural IR',
                    question:
                        'Which of the following word embedding techniques leverages global corpus statistics to create word representations?',
                    options: ['Word2Vec', 'ELMo', 'GloVe', 'BERT', 'FastText'],
                    correct: 2,
                    explanation:
                        'GloVe (Global Vectors for Word Representation) is distinct from models like Word2Vec because it explicitly leverages global corpus statistics. It constructs a word-word co-occurrence matrix from the entire corpus and then factorizes this matrix to produce word embeddings.',
                },
                {
                    category: 'Modern NLP & Neural IR',
                    question: 'What is the primary goal of Word2Vec?',
                    options: [
                        'To convert words into dense vectors that capture semantic relationships',
                        'To perform sentiment analysis on customer reviews',
                        'To identify the grammatical structure of sentences',
                        'To eliminate stop words from a text corpus',
                        'To translate text from one language to another',
                    ],
                    correct: 0,
                    explanation:
                        'The primary goal of Word2Vec (and other similar techniques) is to learn distributed representations of words, known as word embeddings. These are dense vectors where words with similar meanings have similar vector representations, capturing semantic relationships.',
                },
                {
                    category: 'Modern NLP & Neural IR',
                    question:
                        'What is the purpose of the Masked Language Modeling (MLM) objective in BERT?',
                    options: [
                        'To predict randomly masked words in a sentence, forcing the model to understand bidirectional context',
                        'To classify the sentiment of a given text',
                        'To predict the next sentence in a sequence',
                        'To translate text from one language to another',
                        'To generate new text based on a prompt',
                    ],
                    correct: 0,
                    explanation:
                        "The Masked Language Modeling (MLM) objective is crucial for BERT. By forcing the model to predict a masked word using both the words that come before it and the words that come after it, it learns a deep, bidirectional representation of context, which is BERT's key innovation.",
                },
                {
                    category: 'Modern NLP & Neural IR',
                    question:
                        'What is the "scaling hypothesis" in the context of Large Language Models (LLMs)?',
                    options: [
                        'Increasing the batch size dramatically improves performance',
                        'Decreasing the learning rate dramatically improves performance',
                        'Increasing the amount of regularization dramatically improves performance',
                        'Decreasing the model size dramatically improves performance',
                        'Increasing model size and training data dramatically improves performance',
                    ],
                    correct: 4,
                    explanation:
                        'The scaling hypothesis in the context of LLMs is the empirical finding that performance on many tasks improves dramatically and predictably as you increase the model size (number of parameters), the amount of training data, and the compute used for training.',
                },
                {
                    category: 'Modern NLP & Neural IR',
                    question:
                        'What is the primary reason that Large Language Models (LLMs) are often paired with vector databases?',
                    options: [
                        'To allow the LLM to perform mathematical calculations',
                        'To enable the LLM to access and retrieve relevant information from a large corpus of knowledge',
                        'To reduce the size of the LLM',
                        "To improve the LLM's ability to generate code",
                        'To provide the LLM with a persistent memory of past interactions',
                    ],
                    correct: 1,
                    explanation:
                        'LLMs are often paired with vector databases for efficient Retrieval-Augmented Generation (RAG). The vector database stores embeddings of a large corpus of knowledge, allowing the LLM to perform a fast similarity search to find and retrieve the most relevant information to augment its response.',
                },
                {
                    category: 'Modern NLP & Neural IR',
                    question:
                        'Which of the following is NOT a key component of the Transformer architecture?',
                    options: [
                        'Self-Attention Mechanism',
                        'Positional Encoding',
                        'Multi-Head Attention',
                        'Feed-Forward Neural Network',
                        'Recurrent Layers',
                    ],
                    correct: 4,
                    explanation:
                        'The Transformer architecture is built on self-attention mechanisms, positional encodings, multi-head attention, and feed-forward networks. Recurrent Layers are the defining feature of Recurrent Neural Networks (RNNs) and are explicitly replaced by the self-attention mechanism in Transformers.',
                },
                {
                    category: 'Modern NLP & Neural IR',
                    question:
                        'Which of the following techniques is used to break down words into subwords to handle out-of-vocabulary words in BERT?',
                    options: [
                        'GloVe',
                        'TF-IDF',
                        'WordPiece tokenization',
                        'Word2Vec',
                        'Bag of Words',
                    ],
                    correct: 2,
                    explanation:
                        'BERT uses a tokenization technique called WordPiece. To handle a large vocabulary and out-of-vocabulary words, WordPiece breaks words down into common subword units. This allows the model to represent any word as a sequence of these known subwords.',
                },
                {
                    category: 'Modern NLP & Neural IR',
                    question:
                        'In Retrieval-Augmented Generation (RAG), what is the role of the retrieval component?',
                    options: [
                        'To evaluate the quality of the generated output',
                        'To fine-tune the language model for a specific task',
                        'To generate new text based on the input prompt',
                        'To compress the input prompt for faster processing',
                        'To extract relevant information from external knowledge sources',
                    ],
                    correct: 4,
                    explanation:
                        "In Retrieval-Augmented Generation (RAG), the retrieval component's role is to act as a knowledge retriever. It searches an external source to find relevant text snippets that are then passed to the language model to inform and ground the final generated text.",
                },
                {
                    category: 'Modern NLP & Neural IR',
                    question:
                        'Which of the following best describes the function of the "forget gate" in an LSTM (Long Short-Term Memory) network?',
                    options: [
                        'It adds new information to the hidden state',
                        'It decides what information to discard from the cell state',
                        'It regulates the information passed to the next time step or the output layer',
                        'It determines what new information to store in the cell state',
                        'It combines the input gate and output gate functionalities',
                    ],
                    correct: 1,
                    explanation:
                        "The forget gate in an LSTM is the mechanism that controls the flow of information from one time step to the next. It specifically determines which pieces of information in the cell state are no longer relevant and should be discarded or 'forgotten'.",
                },
                {
                    category: 'Modern NLP & Neural IR',
                    question:
                        'What is the primary purpose of prompt engineering in the context of Large Language Models (LLMs)?',
                    options: [
                        'To craft inputs that elicit desired behaviors from LLMs',
                        'To optimize the hardware infrastructure for LLMs',
                        'To fine-tune LLMs for specific downstream tasks',
                        'To reduce the computational cost of running LLMs',
                        'To encrypt sensitive data used to train LLMs',
                    ],
                    correct: 0,
                    explanation:
                        "Prompt engineering is the art and science of designing effective inputs (prompts) for LLMs. The goal is to carefully craft the prompt in a way that guides the model to produce the desired output, whether it's a specific format, style, or type of information.",
                },
                {
                    category: 'Modern NLP & Neural IR',
                    question: 'In Word2Vec, what is the main goal of the Skip-gram model?',
                    options: [
                        'To cluster similar documents together',
                        'To predict the target word from its surrounding context',
                        'To predict the sentiment of a given text',
                        'To predict context words from a target word',
                        'To predict the next sentence in a sequence',
                    ],
                    correct: 3,
                    explanation:
                        'The Skip-gram model is one of the two main architectures of Word2Vec. Its specific objective is to take a single target word as input and train the model to accurately predict the words that appear in its surrounding context window.',
                },
                {
                    category: 'Modern NLP & Neural IR',
                    question:
                        'In the context of Transformer architecture, what is the role of positional encoding?',
                    options: [
                        'To inject information about the position of tokens in the sequence',
                        'To normalize the input data',
                        'To capture semantic relationships between words',
                        'To mask certain tokens from the attention mechanism',
                        'To reduce the dimensionality of input embeddings',
                    ],
                    correct: 0,
                    explanation:
                        'Since the self-attention mechanism in Transformers processes all tokens in parallel and does not have an inherent sense of sequence, positional encodings are added to the input embeddings to inject information about the position of each token.',
                },
                {
                    category: 'Modern NLP & Neural IR',
                    question:
                        'Which model architecture is primarily decoder-based and focused on text generation?',
                    options: ['GloVe', 'RNN', 'BERT', 'GPT', 'Word2Vec'],
                    correct: 3,
                    explanation:
                        'GPT (Generative Pre-trained Transformer) models are built using a decoder-only architecture. This structure is inherently auto-regressive, meaning it generates text one token at a time based on the preceding tokens, making it highly effective for text generation tasks.',
                },
                {
                    category: 'Modern NLP & Neural IR',
                    question:
                        'What key innovation allowed Transformers to overcome the limitations of RNNs in capturing long-range dependencies?',
                    options: [
                        'Convolutional layers',
                        'Attention Mechanism',
                        'Pooling layers',
                        'Activation functions',
                        'Backpropagation',
                    ],
                    correct: 1,
                    explanation:
                        'The key innovation of the Transformer architecture is the self-attention mechanism. This mechanism allows the model to weigh the importance of all words in the input when processing any given word, enabling it to capture long-range dependencies in the text far more effectively than the sequential processing of RNNs.',
                },
                {
                    category: 'Modern NLP & Neural IR',
                    question:
                        'Which of the following is a key advantage of Transformers over Recurrent Neural Networks (RNNs)?',
                    options: [
                        'Transformers can process entire sequences in parallel, allowing for faster computation',
                        'RNNs utilize the self-attention mechanism',
                        'Transformers have difficulty capturing long-range dependencies',
                        'Transformers process data sequentially, while RNNs process data in parallel',
                        'RNNs are better at capturing global context',
                    ],
                    correct: 0,
                    explanation:
                        'Unlike RNNs, which must process sequential data one token at a time, the self-attention mechanism in Transformers allows them to process all tokens in a sequence simultaneously. This parallelization leads to significantly faster computation and training times on modern hardware like GPUs.',
                },
                {
                    category: 'Modern NLP & Neural IR',
                    question:
                        'What is the main advantage of using LSTMs over basic RNNs for processing sequential data?',
                    options: [
                        'LSTMs require less computational power than RNNs',
                        'LSTMs can process data in parallel, while RNNs must process sequentially',
                        'LSTMs do not need to be trained on large datasets',
                        'LSTMs mitigate the vanishing gradient problem, allowing them to capture long-range dependencies',
                        'LSTMs are simpler to implement',
                    ],
                    correct: 3,
                    explanation:
                        'Basic RNNs suffer from the vanishing gradient problem, where gradients can become extremely small during backpropagation, making it difficult to learn dependencies between distant items in a sequence. LSTMs introduce gating mechanisms (like the forget gate) that control the flow of information, mitigating this problem and allowing them to capture long-range dependencies.',
                },
                {
                    category: 'Modern NLP & Neural IR',
                    question: 'In the context of BERT, what is Masked Language Modeling (MLM)?',
                    options: [
                        'A pre-training objective where the model predicts randomly masked words in a sentence',
                        'A type of data augmentation used to increase the size of the training dataset',
                        'A technique for generating new sentences from a given input',
                        'A fine-tuning technique for improving model performance on specific tasks',
                        'A method for predicting the next sentence in a sequence',
                    ],
                    correct: 0,
                    explanation:
                        'Masked Language Modeling (MLM) is the self-supervised pre-training task used by BERT. It involves masking some words in a sentence and training the model to predict them based on the surrounding bidirectional context.',
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question:
                        'What is a primary limitation of the Bag of Words (BoW) approach in NLP?',
                    options: [
                        'It reduces dimensionality effectively',
                        'It loses word order and treats text as an unordered collection of words',
                        'It captures semantic relationships between words',
                        'It effectively handles long-range dependencies',
                        'It preserves the grammatical structure of sentences',
                    ],
                    correct: 1,
                    explanation:
                        'The Bag of Words (BoW) model represents text simply as a multiset (or bag) of its words, completely disregarding grammar and word order. While simple and efficient, this loss of sequential information is its primary limitation, as it cannot capture context or meaning that depends on word order.',
                },
                {
                    category: 'Modern NLP & Neural IR',
                    question:
                        'Which pre-training objective in BERT helps the model understand relationships between sentences?',
                    options: [
                        'Position Embeddings',
                        'Transfer Learning',
                        'WordPiece Tokenization',
                        'Masked Language Modeling (MLM)',
                        'Next Sentence Prediction (NSP)',
                    ],
                    correct: 4,
                    explanation:
                        'In addition to Masked Language Modeling (MLM), BERT is also pre-trained on a Next Sentence Prediction (NSP) task. The model is given two sentences, A and B, and must predict whether B is the actual sentence that follows A in the original text. This helps the model learn relationships between sentences.',
                },
                {
                    category: 'Text Processing & Normalization',
                    question: 'When processing text, why might diacritics be removed?',
                    options: [
                        'To preserve the case sensitivity of proper nouns',
                        'To improve search engine rankings',
                        'Because they have minimal impact on query performance',
                        'To handle acronyms correctly',
                        'To standardize all words to lowercase',
                    ],
                    correct: 2,
                    explanation:
                        "Diacritics (accent marks) can create variations of the same word (e.g., 'ÿ®ÿ≥ŸÖ ÿßŸÑŸÑŸá ÿßŸÑÿ±ÿ≠ŸÖŸÜ ÿßŸÑÿ±ÿ≠ŸäŸÖ' vs. 'ÿ®Ÿêÿ≥ŸíŸÖŸê ÿßŸÑŸÑŸëŸéŸáŸê ÿßŸÑÿ±ŸëŸéÿ≠ŸíŸÖŸéŸ∞ŸÜŸê ÿßŸÑÿ±ŸëŸéÿ≠ŸêŸäŸÖŸê'). Removing them during text processing is a normalization step that helps treat these variations as the same term, which generally has a minimal negative impact on performance while improving matching.",
                },
                {
                    category: 'Text Processing & Normalization',
                    question:
                        'How does stemming generally affect retrieval performance in English?',
                    options: [
                        'It consistently improves recall but may harm precision',
                        'It consistently harms both precision and recall',
                        'It consistently improves precision but may harm recall',
                        'It consistently improves both precision and recall',
                        'It has no effect on either precision or recall',
                    ],
                    correct: 0,
                    explanation:
                        "Stemming reduces different forms of a word to a common root (e.g., 'retrieving', 'retrieved' -> 'retriev'). This increases recall because a search for 'retrieve' will now match documents containing any of its variants. However, it can sometimes conflate distinct words (e.g., 'university' and 'universe' to 'univers'), which can harm precision.",
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question:
                        'Level 5: A user submits a complex query with many terms, some with high document frequencies and some with low. What is a good strategy for query processing?',
                    options: [
                        'Process terms in order of decreasing document frequency',
                        'Process terms in order of increasing document frequency',
                        'Process all terms simultaneously',
                        'It depends on the specific retrieval model being used',
                        'Process terms randomly',
                    ],
                    correct: 1,
                    explanation:
                        'When processing a query with multiple AND clauses, it is most efficient to start with the term that has the lowest document frequency. The size of the resulting document set from the first intersection will be, at most, the size of the smallest input set, minimizing the work for subsequent intersections.',
                },
                {
                    category: 'Text Processing & Normalization',
                    question:
                        'Which of the following BEST describes how the spelling correction problem can be approached using heuristics?',
                    options: [
                        'Inverted Index',
                        'Term-Document Incidence Matrix',
                        'Probabilistic Retrieval',
                        'Boolean Retrieval',
                        'Vector Space Model',
                    ],
                    correct: 3,
                    explanation:
                        'The spelling-correction task can be handled heuristically by generating a set of candidate corrections (e.g. all terms within a given edit-distance of the misspelling) and then issuing a Boolean OR query over those candidates against the index. In other words, you treat each possible correction as a term in a Boolean Retrieval query to quickly filter down to only those dictionary entries that actually occur in the collection.',
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question: 'In the Boolean retrieval model, how is a document viewed?',
                    options: [
                        'A collection of sentences',
                        'A set of words',
                        'A hierarchical structure',
                        'A sequence of characters',
                        'A graph of terms',
                    ],
                    correct: 1,
                    explanation:
                        'The Boolean retrieval model is the simplest information retrieval model. It views each document as an unstructured collection of words, represented as a set. Queries are Boolean expressions of terms, and a document is a match if it satisfies the query based on set theory.',
                },
                {
                    category: 'Indexing & Data Structures',
                    question: 'What is an advantage of using a biword index?',
                    options: [
                        'It is more accurate than a positional index',
                        'It eliminates the need for stemming',
                        'It can handle phrase queries directly',
                        'It allows for efficient processing of single-word queries',
                        'It reduces the size of the vocabulary',
                    ],
                    correct: 2,
                    explanation:
                        "A biword index stores pairs of consecutive terms as single entries in its dictionary. This allows it to directly handle phrase queries for two-word phrases (e.g., 'information retrieval') by looking up the single biword 'information_retrieval' instead of intersecting postings lists.",
                },
                {
                    category: 'Indexing & Data Structures',
                    question: 'What is a "posting" in the context of an inverted index?',
                    options: [
                        'An entry in a postings list, recording that a term appeared in a document',
                        'A web page that contains advertisements',
                        "A statistical measure of a term's importance",
                        "A user's query submitted to a search engine",
                        'A document that is highly relevant to a query',
                    ],
                    correct: 0,
                    explanation:
                        "An inverted index consists of a dictionary and postings lists. A 'posting' is an individual entry in a postings list. At its simplest, each posting contains a document ID, recording the fact that the term associated with that postings list appeared in that specific document.",
                },
                {
                    category: 'Text Processing & Normalization',
                    question:
                        'Which of the following techniques is MOST directly related to handling spelling variations in queries?',
                    options: [
                        'Lemmatization',
                        'Soundex',
                        'n-gram indexing',
                        'Stemming',
                        'All of the above',
                    ],
                    correct: 4,
                    explanation:
                        'All the listed techniques are methods for handling variations in word forms or spelling. Lemmatization and stemming reduce words to a base form. Soundex groups phonetically similar words. N-gram indexing finds words with similar character subsequences. All are relevant to handling spelling variations.',
                },
                {
                    category: 'Text Processing & Normalization',
                    question:
                        'Why is it generally not advisable to treat an entire book as a single document when performing searches?',
                    options: [
                        'Makes searching faster',
                        'Increases document size',
                        'Reduces storage requirements',
                        'Improves relevance by focusing on smaller sections',
                        'Enhances encryption security',
                    ],
                    correct: 3,
                    explanation:
                        'If an entire book is treated as a single document, a query for a term that appears only in one chapter will cause the entire book to be returned. By breaking the book into smaller sections (like chapters or paragraphs), a search can return a more focused and relevant part of the book, thus improving relevance.',
                },
                {
                    category: 'Foundational IR Concepts',
                    question:
                        'Level 2: In Boolean retrieval, what is the difference between an "information need" and a "query"?',
                    options: [
                        'There is no difference',
                        'The query is broader, while the information need is more specific',
                        'The information need is expressed in natural language, while the query is in Boolean logic',
                        'The information need is broader, while the query is the specific search string',
                        'The information need is processed by the system, while the query is not',
                    ],
                    correct: 3,
                    explanation:
                        "An 'information need' is the user's underlying abstract goal or topic of interest (e.g., 'I need to know about bicycle safety'). A 'query' is the concrete string of characters the user types into the search box to try to satisfy that need (e.g., 'bicycle helmets'). The information need is broader and more conceptual than the specific query.",
                },
                {
                    category: 'Text Processing & Normalization',
                    question:
                        'When processing text to determine the vocabulary of terms, what is a primary goal of lemmatization ?',
                    options: [
                        'To convert words into their phonetic equivalents using Soundex',
                        'To remove all derivational affixes from a word, achieving proper lemmatization',
                        'To identify and tag terms in multiple languages within a document collection',
                        'To collapse derivationally related words into a single base form',
                        'To reduce inflectional endings to return the base lemma of a word',
                    ],
                    correct: 4,
                    explanation:
                        "Lemmatization is a linguistic process of normalization. Its primary goal is to analyze a word and reduce it to its canonical dictionary form, known as its lemma. This involves correctly handling inflectional endings (like '-s', '-ing', '-ed').",
                },
                {
                    category: 'Indexing & Data Structures',
                    question:
                        'Why is logarithmic merging more efficient than repeatedly merging a small auxiliary index into a large main index for dynamic indexing?',
                    options: [
                        'Logarithmic merging distributes the indexing load across multiple machines',
                        'Logarithmic merging avoids sorting the main index',
                        'Logarithmic merging reduces the number of disk seeks',
                        'Logarithmic merging allows for parallel processing',
                        'Logarithmic merging ensures that each posting is only merged a logarithmic number of times',
                    ],
                    correct: 4,
                    explanation:
                        'When new documents are added to a dynamic index, they are put in a small in-memory auxiliary index. Repeatedly merging this small index into a very large main index is inefficient. Logarithmic merging is more efficient because it merges indexes of similar sizes, ensuring that any given posting is moved only a logarithmic number of times as the index grows.',
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question:
                        'In query processing, what does "merging" postings lists typically involve for an AND query?',
                    options: [
                        'Sorting the lists alphabetically',
                        'Finding the intersection of the lists',
                        'Concatenating the lists',
                        'Finding the union of the lists',
                        'Calculating the difference between the lists',
                    ],
                    correct: 1,
                    explanation:
                        "In a Boolean retrieval system, an 'AND' query requires that all specified terms are present in a document. To process this, the system retrieves the postings lists for each term and then performs a merge operation to find the intersection of these lists the set of documents that appear in all of them.",
                },
                {
                    category: 'Indexing & Data Structures',
                    question:
                        'In the Blocked Sort-Based Indexing (BSBI) algorithm, what is the main reason for dividing the document collection into blocks?',
                    options: [
                        'To reduce the number of disk seeks during sorting',
                        'To allow for easier updates to the index',
                        'To fit intermediate data into main memory',
                        'To simplify the merging process',
                        'To enable parallel processing on multiple machines',
                    ],
                    correct: 2,
                    explanation:
                        'The Blocked Sort-Based Indexing (BSBI) algorithm is designed to build an index for a document collection that is too large to fit in main memory. It works by dividing the collection into blocks that can fit into memory, creating a mini-index for each block, and then merging these mini-indexes into the final index.',
                },
                {
                    category: 'Text Processing & Normalization',
                    question: 'In information retrieval, what does "edit distance" measure?',
                    options: [
                        'The semantic similarity between two words',
                        'The difference in length between two strings',
                        'The minimum number of character-level operations (insert, delete, substitute) to transform one string into another',
                        'The number of documents containing both words',
                        'The frequency difference between two words',
                    ],
                    correct: 2,
                    explanation:
                        'Edit distance, specifically Levenshtein distance, is a standard way to measure the similarity between two strings. It is defined as the minimum number of single-character edits (insertions, deletions, or substitutions) required to change one string into the other.',
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question:
                        'In Boolean Retrieval, what does the query "cat AND dog NOT mouse" retrieve?',
                    options: [
                        'Documents that contain "cat" and "dog" but not "mouse"',
                        'Documents that contain "cat", "dog", or "mouse"',
                        'Documents that contain both "cat" and "dog"',
                        'Documents that contain "cat" or "dog" but not "mouse"',
                        'Documents that contain "cat" and "dog" and may or may not contain "mouse"',
                    ],
                    correct: 0,
                    explanation:
                        "In Boolean logic, the 'AND' operator finds the intersection of sets, while the 'NOT' operator finds the complement. Therefore, the query 'cat AND dog NOT mouse' retrieves all documents that are in the set for 'cat' AND in the set for 'dog', but then removes any documents that are also in the set for 'mouse'.",
                },
                {
                    category: 'Text Processing & Normalization',
                    question:
                        'Which of the following best describes the process of tokenization in Information Retrieval?',
                    options: [
                        'Creating equivalence classes of words, such as treating',
                        'Breaking down a character sequence into individual semantic units',
                        'Reducing words to their root form, such as',
                        'Grouping related documents together based on their content',
                        'Identifying the language of a document',
                    ],
                    correct: 1,
                    explanation:
                        'Tokenization is the initial step in the indexing pipeline. It involves parsing a stream of characters (a document) and breaking it down into a sequence of individual processing units called tokens, which are typically words, numbers, or symbols.',
                },
                {
                    category: 'Text Processing & Normalization',
                    question:
                        'Which of the following is NOT typically considered a linguistic module operation in the indexing pipeline?',
                    options: [
                        'Skip pointer creation',
                        'Stemming',
                        'Lemmatization',
                        'Stop word removal',
                        'Tokenization',
                    ],
                    correct: 0,
                    explanation:
                        'Stemming, lemmatization, stop word removal, and tokenization are all linguistic processing steps applied to the text content. Skip pointer creation is a structural optimization applied to the postings lists of the index itself to speed up query processing, not a linguistic operation on the text.',
                },
                {
                    category: 'Indexing & Data Structures',
                    question:
                        'The algorithm described in Figure 2.12 is designed to find within-k word proximity searches by using positional indexes. How does this approach impact the required postings storage?',
                    options: [
                        'They minimize storage requirements while supporting proximity searches',
                        'They do not affect the required postings storage',
                        'Positional indexes significantly increase required postings storage',
                        'They reduce the required postings storage',
                        'They allow for efficient computation of within-k proximity searches but may trade off storage space',
                    ],
                    correct: 2,
                    explanation:
                        'Positional indexes must store the position of every occurrence of every term in every document. This is a significant amount of extra information compared to a standard inverted index (which only stores document IDs) and thus substantially increases the required storage space for the postings file.',
                },
                {
                    category: 'Indexing & Data Structures',
                    question: 'Skip pointers in postings lists are primarily used to:',
                    options: [
                        'Improve the efficiency of OR queries',
                        'Handle phrase queries',
                        'Improve the efficiency of AND queries',
                        'Reduce the size of the postings lists',
                        'Make the index dynamic',
                    ],
                    correct: 2,
                    explanation:
                        'Skip pointers are shortcuts embedded within long postings lists. During the intersection of two lists (an AND query), if a document ID in one list is far ahead, a skip pointer allows the algorithm to jump forward in the other list, skipping over many documents that cannot possibly be in the intersection, thus improving efficiency.',
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question:
                        'What symbol is commonly used to represent a wildcard in a wildcard query?',
                    options: ['*', '$', '%', 'o', '?'],
                    correct: 0,
                    explanation:
                        'The asterisk (*) is the most widely recognized and commonly used character to represent a wildcard for truncation in search queries. It matches any sequence of zero or more characters at the position where it is placed.',
                },
                {
                    category: 'Text Processing & Normalization',
                    question:
                        'A user frequently searches for information related to "automobile repair", but sometimes uses "car repair" or "vehicle repair". Which combination of IR techniques would BEST address this user\'s needs?',
                    options: [
                        'Case folding and n-gram indexing',
                        'Stemming and stop word removal',
                        'Tokenization and stop word removal',
                        'Skip pointers and stemming',
                        'Lemmatization and thesaurus-based query expansion',
                    ],
                    correct: 4,
                    explanation:
                        "The user is using different but semantically related terms. Lemmatization would normalize 'automobile' and 'automobiles'. A thesaurus-based query expansion would recognize that 'car', 'vehicle', and 'automobile' are synonyms and could be searched together to improve recall.",
                },
                {
                    category: 'Text Processing & Normalization',
                    question: 'What is the purpose of a stop list in Information Retrieval?',
                    options: [
                        'To store frequently occurring terms that are likely to be relevant to user queries',
                        'To exclude common words like "the", "a", "is" from indexing',
                        'To store a list of all terms in the document collection',
                        'To maintain a list of synonyms for query expansion',
                        'To store a list of all stemmed words',
                    ],
                    correct: 1,
                    explanation:
                        "A stop list contains extremely common words (like 'the', 'a', 'is', 'in') that occur so frequently they carry little semantic weight for determining document relevance. The purpose of using a stop list is to exclude these words from the index to save space and processing time.",
                },
                {
                    category: 'Indexing & Data Structures',
                    question:
                        'Which of the following is NOT a typical step in the process of constructing an inverted index?',
                    options: [
                        'Merging postings lists from different blocks (if using a blocked approach)',
                        'Parsing documents to extract terms',
                        'Calculating the semantic similarity between terms',
                        'Creating postings lists for each term',
                        'Sorting terms alphabetically',
                    ],
                    correct: 2,
                    explanation:
                        "Constructing an inverted index involves parsing documents, creating postings, and merging them. Calculating the semantic similarity between terms is not part of the index construction process itself; it's a feature of retrieval models (like the vector space model) that use the index.",
                },
                {
                    category: 'Indexing & Data Structures',
                    question:
                        'How does a permuterm index handle a wildcard query like fi*mo*er?',
                    options: [
                        'It computes the edit distance to all terms',
                        'It only works for trailing wildcards',
                        'It directly searches for the entire pattern',
                        'It uses a k-gram index to find matching terms',
                        'It breaks the query into smaller subqueries based on the * position, using rotations',
                    ],
                    correct: 4,
                    explanation:
                        'A permuterm index handles wildcard queries by creating rotated versions of each term and storing them in an index like a B-tree. A query like `fi*mo*er` can be broken down into subqueries like `er$fi*` and `mo*er$fi`, which are then processed. This method uses rotations to handle wildcards anywhere in a term.',
                },
                {
                    category: 'Text Processing & Normalization',
                    question:
                        'What is the primary purpose of the tokenization process in Information Retrieval?',
                    options: [
                        'To identify the language of a document',
                        'To determine the document unit for indexing',
                        'To chop a character sequence into pieces called tokens',
                        'To perform word segmentation',
                        'To convert the byte sequence of a document into a linear sequence of characters',
                    ],
                    correct: 2,
                    explanation:
                        'Tokenization is the foundational step of indexing where a sequence of characters from a document is analyzed and segmented into individual pieces, called tokens (usually words or numbers), that form the basis for the index.',
                },
                {
                    category: 'Indexing & Data Structures',
                    question:
                        'In the context of Information Retrieval, what is an "inverted index"?',
                    options: [
                        'A list of documents sorted by their relevance to a query',
                        'A compressed representation of the document collection',
                        'A list of all unique terms in the collection',
                        'A matrix showing the frequency of each term in each document',
                        'A data structure that maps terms to the documents they appear in',
                    ],
                    correct: 4,
                    explanation:
                        'An inverted index is the core data structure in modern information retrieval. Its primary function is to create a mapping from terms (the vocabulary) to the locations (the documents) in which they appear, allowing for fast retrieval.',
                },
                {
                    category: 'Indexing & Data Structures',
                    question:
                        'What is the primary advantage of using a B-tree over a hashtable for implementing a dictionary in an IR system?',
                    options: [
                        'Lower memory consumption',
                        'Simpler implementation',
                        'Faster lookup time',
                        'Ability to handle prefix searches (wildcard queries)',
                        'Easier handling of term variations',
                    ],
                    correct: 3,
                    explanation:
                        "The main advantage of using a B-tree over a hash table for the dictionary of an IR system is its ability to efficiently handle prefix searches, which are necessary for wildcard queries (e.g., finding all terms starting with 'auto*'). Hash tables are not well-suited for this kind of range or prefix search.",
                },
                {
                    category: 'Indexing & Data Structures',
                    question:
                        'In a positional index, what information is stored in each posting, in addition to the docID?',
                    options: [
                        'A skip pointer',
                        'The term frequency',
                        'The stemmed form of the term',
                        'The document length',
                        'A list of positions (word offsets) where the term occurs in the document',
                    ],
                    correct: 4,
                    explanation:
                        'A positional index enhances a standard inverted index by storing not just the document ID (docID) in each posting, but also a list of the positions (e.g., word offsets from the beginning of the document) where the term occurs within that document.',
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question:
                        'In the Boolean Retrieval model (slide 7), how is a document represented?',
                    options: [
                        'As a graph of relationships',
                        'As a sequence of characters',
                        'As a weighted vector',
                        'As a set of words',
                        'As a probabilistic distribution',
                    ],
                    correct: 3,
                    explanation:
                        'The classic Boolean retrieval model represents a document in the simplest way possible: as an unordered set of the words (or terms) that it contains. It does not consider term frequency, order, or any other weights.',
                },
                {
                    category: 'Text Processing & Normalization',
                    question:
                        'In how many cases would a query of "o\u2019neill" and "capital" match?',
                    options: ['5', '3', '4', '2', '1'],
                    correct: 4,
                    explanation:
                        "The question asks for a match for 'o'neill' AND 'capital'. This requires analyzing how 'o'neill' might be tokenized. Common tokenizations could be 'o'neill', 'oneill', or 'neill'. Assuming 'o'neill' is the standard tokenization, only a query for that specific token would match. The question is ambiguous, but the provided answer '1' implies that only one specific tokenization of the name is being indexed and queried for.",
                },
                {
                    category: 'Text Processing & Normalization',
                    question:
                        'What does edit distance allow in spelling correction techniques, as described in the context?',
                    options: [
                        'Measuring the number of differing characters between two strings',
                        'A combination of insertions, deletions, and replacements to find the minimum steps between two strings',
                        'The minimum number of insertions or deletions needed to transform one string into another',
                        'Correcting a single query term at a time',
                        'The number of replacements required to transform one string into another',
                    ],
                    correct: 1,
                    explanation:
                        'Edit distance, particularly Levenshtein distance, defines the difference between two strings as the minimum number of single-character edits needed to transform one into the other. These edits are a combination of inserting a character, deleting a character, and replacing one character with another.',
                },
                {
                    category: 'Text Processing & Normalization',
                    question:
                        'Which of the following scenarios would be BEST addressed by using a Soundex algorithm as part of the query processing pipeline?',
                    options: [
                        "Expanding a query for 'car' to include 'automobile'",
                        "Correcting a query for 'acomodate' to 'accommodate'",
                        "Correcting a query for 'colour' to 'color'",
                        "Correcting a query for 'Britny Spers' to 'Britney Spears'",
                        "Handling a wildcard query like 'tele*one'",
                    ],
                    correct: 3,
                    explanation:
                        "The Soundex algorithm is a phonetic algorithm that indexes words by their sound as pronounced in English. It is particularly effective for names, as it can correct misspellings that are phonetically similar, such as 'Britny Spers' and 'Britney Spears'.",
                },
                {
                    category: 'Indexing & Data Structures',
                    question:
                        'What is the primary goal of an Inverted Index in Information Retrieval?',
                    options: [
                        'To allow for efficient calculation of the term-document incidence matrix',
                        'To enable grepping through text for efficient search',
                        'To map terms to the documents where they occur',
                        'To store the term-document incidence matrix in a compressed format',
                        'To store the complete text of each document for efficient retrieval',
                    ],
                    correct: 2,
                    explanation:
                        'The primary goal of an inverted index is to provide an efficient data structure that maps each term in the vocabulary to a list of documents in which that term appears, enabling fast lookups for query terms.',
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question: 'A leading wildcard query is of the form ?',
                    options: ['mo*n', 'm*n', 'mon**', 'mon*', '*mon'],
                    correct: 4,
                    explanation:
                        "A wildcard query is classified by the position of the wildcard symbol (*). A 'leading' wildcard query is one where the wildcard is at the beginning of the term, such as '*mon', which would match any term ending in 'mon'.",
                },
                {
                    category: 'Text Processing & Normalization',
                    question: 'What is the purpose of query expansion?',
                    options: [
                        'To automatically correct spelling errors in the query',
                        'To add related terms to the query, potentially improving recall',
                        'To remove stop words from the query',
                        'To make the query shorter',
                        'To restrict the query to a specific document collection',
                    ],
                    correct: 1,
                    explanation:
                        'Query expansion is a technique to improve the recall of a search system. It works by automatically or semi-automatically adding related terms (such as synonyms or morphological variants) to the original user query, thereby broadening the search to find relevant documents that might not contain the exact initial terms.',
                },
                {
                    category: 'Indexing & Data Structures',
                    question: 'What is the primary purpose of an inverted index?',
                    options: [
                        'To rank documents based on their relevance to a query',
                        'To map terms to the documents in which they appear',
                        'To provide a graphical user interface for search',
                        'To store the original documents in a compressed format',
                        'To perform stemming and lemmatization of terms',
                    ],
                    correct: 1,
                    explanation:
                        'An inverted index is the fundamental data structure in information retrieval. Its primary role is to create a mapping from each unique term in a vocabulary to a list of documents (and often positions within those documents) where the term appears. This allows for very fast lookups of which documents contain a given query term, avoiding a slow scan of the entire collection.',
                },
                {
                    category: 'Indexing & Data Structures',
                    question:
                        'Which statement about the effectiveness of search engine indexing methods is supported by the given context?',
                    options: [
                        "Tomlinson's work (2003) suggested that lemmatizers performed worse than stemming methods for most languages",
                        'Skip pointers, as discussed in Moffat and Zobel (1996), are no longer considered effective for modern search engines due to performance issues',
                        'Using compound splitting improved search performance for all languages discussed',
                        'Indexing based on character four-grams (n-grams) provided significant improvements across multiple languages, including English and Spanish',
                        'The Bar-Ilan and Gutman study found that commercial web search engines were hampered by a lack of language-specific processing in 2003',
                    ],
                    correct: 3,
                    explanation:
                        'N-gram indexing involves breaking words into sequences of N characters. This approach is language-agnostic and robust against spelling errors and morphological variations. The provided answer states that this method led to significant improvements for languages like English and Spanish, which is a plausible outcome because it avoids the need for complex, language-specific stemming rules while still capturing word similarities.',
                },
                {
                    category: 'Text Processing & Normalization',
                    question: 'What is a "stop word" in the context of IR?',
                    options: [
                        'A stemmed word',
                        'A common word (e.g., "the", "a", "is") that is often removed during preprocessing',
                        'A rare word',
                        'A word that appears in the query',
                        'A misspelled word',
                    ],
                    correct: 1,
                    explanation:
                        "Stop words are words that occur with very high frequency in a language, such as 'the', 'a', 'in', and 'of'. Because they are so common, they are generally not useful for distinguishing between documents based on topic. Therefore, they are often removed from both queries and documents during preprocessing to save storage space and reduce processing time.",
                },
                {
                    category: 'Indexing & Data Structures',
                    question:
                        'What is the primary advantage of using skip pointers in postings lists?',
                    options: [
                        'They allow for faster merging of postings lists during query processing',
                        'They reduce the size of the index',
                        'They improve the accuracy of stemming',
                        'They help in identifying stop words',
                        'They eliminate the need for stemming',
                    ],
                    correct: 0,
                    explanation:
                        'When processing a Boolean AND query, the system must find the intersection of the postings lists for the query terms. Skip pointers are shortcuts embedded within a postings list that allow the merge algorithm to jump over large portions of another list that do not contain any matching document IDs. This significantly reduces the number of comparisons needed and speeds up the intersection process.',
                },
                {
                    category: 'Text Processing & Normalization',
                    question: 'What is the main difference between stemming and lemmatization?',
                    options: [
                        'Stemming is language-dependent, while lemmatization is not',
                        'Lemmatization always produces a valid word, while stemming may not',
                        'Stemming is only for English, while lemmatization works for all languages',
                        'Stemming is more computationally expensive than lemmatization',
                        'Lemmatization uses a dictionary, while stemming uses simple rules',
                    ],
                    correct: 4,
                    explanation:
                        "Lemmatization uses a dictionary and morphological analysis to reduce a word to its base or dictionary form (its 'lemma'), which is always a grammatically correct word (e.g., 'studying' -> 'study'). Stemming, in contrast, uses heuristic rules to chop off word endings (e.g., 'studying' -> 'studi'), which is faster but may result in a non-word stem.",
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question:
                        'Based on the exercise on slide 14, which of the following strategies would be MOST efficient for processing the query (tangerine OR trees) AND (marmalade OR skies) AND (kaleidoscope OR eyes)?',
                    options: [
                        'Process the OR operations with the largest combined postings list sizes first',
                        'Process (tangerine OR trees) first, as it has the smallest individual term',
                        'Process the AND operations first, then the OR operations',
                        'Process the OR operations with the smallest combined postings list sizes first',
                        "It doesn't matter what order the operations are processed in",
                    ],
                    correct: 3,
                    explanation:
                        'The goal of query optimization is to minimize the amount of work done, which usually means minimizing the size of the intermediate postings lists created during processing. For a query with multiple AND clauses, it is most efficient to calculate the results of the OR sub-queries first and then intersect the smallest resulting lists, as this will reduce the number of comparisons needed in the final steps.',
                },
                {
                    category: 'Indexing & Data Structures',
                    question: 'What type of query is a positional index essential for?',
                    options: [
                        'Queries with stop words',
                        'Boolean queries with AND',
                        'Stemmed queries',
                        'Phrase queries (e.g., "Stanford University")',
                        'Boolean queries with OR',
                    ],
                    correct: 3,
                    explanation:
                        "A phrase query, like 'Stanford University', requires not only that the terms 'Stanford' and 'University' are present in a document, but also that they appear in that specific order and adjacent to one another. A positional index is essential for this because it stores the word positions (offsets) for each term in a document, allowing the system to verify both order and adjacency.",
                },
                {
                    category: 'Text Processing & Normalization',
                    question:
                        'A query for "flights to London" would likely benefit most from which of the following IR techniques?',
                    options: [
                        'Case folding',
                        'Keeping stop words',
                        'Stop word removal',
                        'Lemmatization',
                        'Aggressive stemming',
                    ],
                    correct: 1,
                    explanation:
                        "The query 'flights to London' has a specific meaning where 'to' defines the direction of the flights. If stop words were removed, the query would become 'flights London', which could incorrectly match documents that simply contain both words but aren't about flights to London. In this case, the stop word is crucial for preserving the query's intent.",
                },
                {
                    category: 'Text Processing & Normalization',
                    question:
                        "In soundex algorithm, the letters 'C', 'G', 'J', 'K', 'Q', 'S', 'X', 'Z' are changed to what digit?",
                    options: ['1', '5', '3', '0', '2'],
                    correct: 4,
                    explanation:
                        "The Soundex algorithm is a phonetic algorithm that indexes words by their sound. It groups consonants with similar sounds together under the same digit. The letters C, G, J, K, Q, S, X, and Z are all considered to have a similar guttural or hissing sound and are mapped to the code '2'.",
                },
                {
                    category: 'Text Processing & Normalization',
                    question:
                        'What is the primary challenge in tokenization when dealing with languages like Chinese and Japanese?',
                    options: [
                        'The lack of punctuation marks',
                        'The absence of spaces between words',
                        'The large number of characters in the alphabets',
                        'The use of right-to-left writing',
                        'The presence of multiple alphabets intermingled',
                    ],
                    correct: 1,
                    explanation:
                        'Languages such as English use spaces to clearly separate words, making the process of tokenization (splitting text into words) relatively simple. Languages like Chinese and Japanese do not have explicit word delimiters, meaning a continuous string of characters must be segmented into words using complex algorithms. This ambiguity is a primary challenge.',
                },
                {
                    category: 'Indexing & Data Structures',
                    question:
                        'A document collection contains many technical reports with complex part numbers, error codes, and version numbers (e.g., "B-52", "324a3df234cb23e"). Which indexing approach would be MOST helpful for retrieving documents based on these specific identifiers?',
                    options: [
                        'Lemmatization',
                        'Stop word removal',
                        'N-gram indexing',
                        'Stemming',
                        'Case folding',
                    ],
                    correct: 2,
                    explanation:
                        "Complex identifiers like part numbers and error codes are not traditional words and would be poorly handled by standard tokenizers or stemmers. N-gram indexing, which breaks text into overlapping sequences of characters (e.g., 'B-5', '-52'), is robust to this structure. It allows for partial and exact matching of these unique strings, making it the most suitable approach.",
                },
                {
                    category: 'Text Processing & Normalization',
                    question: 'What is a "stop word" in Information Retrieval?',
                    options: [
                        'A word that appears frequently in a document',
                        'A word that is misspelled',
                        'A word that is grammatically incorrect',
                        'A very common word deemed unhelpful for retrieval, often excluded from the vocabulary',
                        'A word that is used in a phrase query',
                    ],
                    correct: 3,
                    explanation:
                        "This is the standard definition of a stop word in Information Retrieval. They are very common words (e.g., 'is', 'an', 'the') that provide little semantic value for identifying a document's topic. They are often removed during preprocessing to enhance efficiency and, in many cases, relevance.",
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question:
                        'Which of the following documents would NOT appear in the search results for the query "Brutus and Caesar and not Calpurnia"?',
                    options: [
                        'Antony and Cleopatra',
                        'Romeo and Juliet',
                        'The Tempest',
                        'Hamlet',
                        'Julius Caesar',
                    ],
                    correct: 3,
                    explanation:
                        "The query is 'Brutus AND Caesar AND NOT Calpurnia'. To appear in the results, a document must contain both 'Brutus' and 'Caesar'. The play 'Hamlet' does not contain both of these characters, so it would not satisfy the 'Brutus AND Caesar' part of the query and would therefore not be retrieved.",
                },
                {
                    category: 'Text Processing & Normalization',
                    question:
                        'What is an effective strategy used by some Boolean retrieval systems like Westlaw and Lexis-Nexis to improve query generalization?',
                    options: [
                        'Using compound splitter modules for German compound nouns',
                        'Handling language-specific reduced definite articles, such as the apostrophe use in French (e.g., l\u2019ensemble)',
                        'Performing character-based indexing regardless of word boundaries',
                        'Encouraging users to enter hyphens in queries to allow generalization of forms like "multi-word" to "multi word"',
                        'Combining all of the above strategies for optimal indexing',
                    ],
                    correct: 4,
                    explanation:
                        'Modern, robust retrieval systems need to handle the linguistic nuances of many different languages. The strategies listed (compound splitting for German, handling French articles, etc.) are all valid techniques for improving search quality. An effective system would ideally combine these and other strategies to provide comprehensive query generalization.',
                },
                {
                    category: 'Text Processing & Normalization',
                    question:
                        'An IR system is designed to handle queries in multiple languages. Which of the following indexing strategies would be MOST robust and adaptable?',
                    options: [
                        'Using language-specific tokenization and normalization modules',
                        'Ignoring all accents and diacritics',
                        'Using a single, universal stemming algorithm',
                        'Treating all words as case-insensitive',
                        'Relying solely on n-gram indexing',
                    ],
                    correct: 0,
                    explanation:
                        'Languages have unique rules for grammar, word formation (morphology), and word separation (tokenization). A single universal algorithm for normalization (like a single stemmer) will perform poorly on many languages. The most robust and effective approach is to first identify the language of the text and then apply normalization rules and tokenizers specifically designed for that language.',
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question:
                        'What is the primary basis for ranked retrieval in information retrieval systems?',
                    options: [
                        'Boolean logic',
                        'Index size',
                        'Term frequency',
                        'Scoring functions',
                        'Number of documents in the collection',
                    ],
                    correct: 3,
                    explanation:
                        'Unlike Boolean retrieval, which returns an undifferentiated set of matching documents, ranked retrieval models aim to order the results by relevance. This is achieved by using scoring functions (such as TF-IDF or BM25) to assign a numerical score to each document based on how well it matches the query. The final results are then presented to the user in descending order of this score.',
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question:
                        'Which of the following is a characteristic of a "sparse" vector in the context of the vector space model?',
                    options: [
                        'It represents a common term',
                        'It is not used in information retrieval',
                        'It contains mostly non-zero values',
                        'It contains mostly zero values',
                        'It represents a short document',
                    ],
                    correct: 3,
                    explanation:
                        "In the vector space model, each document is represented by a vector whose dimensionality is equal to the number of unique terms in the entire collection's vocabulary. Any single document contains only a small fraction of this vocabulary. Thus, its vector will have non-zero values only for the terms it contains and zero for all others, resulting in a sparse vector.",
                },
                {
                    category: 'Information Retrieval Evaluation',
                    question:
                        'Consider a scenario where two different information retrieval systems are being evaluated on the same dataset. System A achieves a higher precision at k for low values of k (e.g., k=10) compared to System B. However, System B achieves a higher MAP score. What inferences can be drawn about the performance characteristics of these two systems, and how would you advise a user who needs to choose between them?',
                    options: [
                        'More information is needed to make a determination',
                        'The two systems are equivalent, and the choice between them is arbitrary',
                        'System B is better overall because MAP is a more reliable metric than precision at k',
                        'System A is better overall because precision at k is a more reliable metric than MAP',
                        'System A is better for tasks where only the top few results matter, while System B is better for tasks where finding as many relevant documents as possible is important',
                    ],
                    correct: 4,
                    explanation:
                        'Precision@k (P@10) measures relevance only within the top K results, which is important for users who want a good answer quickly on the first page. Mean Average Precision (MAP) evaluates ranking quality across all recall levels, rewarding systems that find many relevant documents and rank them highly. Therefore, System A is better for top-heavy relevance, while System B is better for exhaustive searches.',
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question:
                        'What is the purpose of using logarithmic frequency weighting in term frequency calculation?',
                    options: [
                        'To reduce the impact of very frequent terms',
                        'To convert term frequencies to binary values',
                        'To increase the impact of very frequent terms',
                        'To normalize term frequencies across different documents',
                        'To eliminate the need for inverse document frequency (IDF)',
                    ],
                    correct: 0,
                    explanation:
                        "The relevance of a term doesn't grow proportionally with its frequency; a word appearing 20 times is not 10 times more relevant than a word appearing twice. Using the logarithm of the term frequency (tf) dampens the weight of very frequent terms. This ensures that their presence contributes to the score, but without letting them completely dominate other, rarer terms in the document.",
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question:
                        'Which of the following factors is LEAST likely to affect the relevance ranking of documents in a typical information retrieval system?',
                    options: [
                        'Inverse document frequency',
                        'The color of the text in the document',
                        'The length of the document',
                        'Term frequency',
                        'The presence of query terms in the document title',
                    ],
                    correct: 1,
                    explanation:
                        'Information retrieval systems determine relevance based on the textual content of a document and its relationship to the query. Factors like term frequency (TF), inverse document frequency (IDF), document length, and term proximity are all crucial. The visual formatting of the text, such as its color, has no bearing on the semantic content and is therefore not used in relevance calculations.',
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question:
                        'In the vector space model, how are documents and queries represented?',
                    options: [
                        'As vectors in a high-dimensional space',
                        'As linked lists of terms',
                        'As probability distributions over words',
                        'As binary matrices',
                        'As sets of keywords',
                    ],
                    correct: 0,
                    explanation:
                        'This is the foundational concept of the vector space model. Documents and queries are transformed into numerical vectors. Each dimension of the vector space corresponds to a unique term in the vocabulary. This geometric representation allows for the calculation of similarity (e.g., via the cosine of the angle between the vectors) to rank documents.',
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question:
                        'How do Champion Lists relate to Index Elimination? Can they be used together? Options:',
                    options: [
                        'Champion Lists are implemented using a Term-Document Incidence Matrix to track document relevance',
                        'Index Elimination is used without Champion Lists, relying solely on removing lower quality documents',
                        'Champion Lists can be used with Index Elimination to improve search results by focusing on high-quality documents',
                        'Index Elimination focuses on removing documents below a certain score without considering Champion Lists',
                        'The use of Champion Lists requires the Inverted Index for effective implementation',
                    ],
                    correct: 2,
                    explanation:
                        'Index Elimination is a technique to speed up retrieval by reducing the set of documents that need to be fully scored. Champion Lists are pre-computed lists of the highest-quality documents for a given term. By combining these, a system can choose to only score documents that appear in the champion lists of the query terms, effectively eliminating most documents from consideration and focusing only on high-quality candidates.',
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question:
                        'What is the main purpose of using tf-idf weighting in information retrieval?',
                    options: [
                        'To reduce the computational complexity of query processing',
                        'To give equal weight to all terms in a document',
                        'To assign higher weights to terms that are frequent in a specific document but rare in the overall collection',
                        'To prioritize documents with the most terms',
                        'To rank documents based on their length',
                    ],
                    correct: 2,
                    explanation:
                        'TF-IDF weighting is designed to identify terms that are highly characteristic of a document. It does this by balancing two factors: term frequency (TF), which is high for terms used often in that document, and inverse document frequency (IDF), which is high for terms that are rare in the overall collection. The product of these two gives the highest weight to terms that are both locally frequent and globally rare.',
                },
                {
                    category: 'Indexing & Data Structures',
                    question:
                        'Which of the following is true about inverted indexes in parametric and zone indexes?',
                    options: [
                        'Inverted indexes are used to store documents by term occurrences',
                        'Parametric indexes are not used for field-based queries',
                        'Zone indexes build an inverted index on specific document sections, allowing efficient querying within those sections',
                        'All inverted indexes are designed solely for metadata fields',
                        'Parametric indexes are used for metadata fields like author or year',
                    ],
                    correct: 2,
                    explanation:
                        "A document can be divided into 'zones' like 'title', 'author', and 'body'. A zone index is a specialized inverted index created for the content within a specific zone. This allows a user to perform a search for a term within that specific section of a document (e.g., author:\"Smith\"), which is a very efficient and powerful feature.",
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question:
                        'Which of the following statements correctly describes cosine similarity?',
                    options: [
                        'After normalization, all documents have identical vectors regardless of their original length and content',
                        'Cosine similarity is affected by the lengths of the document and query vectors before normalization',
                        'Cosine similarity is calculated as the sum of the query weights and document weights',
                        'The cosine similarity formula subtracts the sum of squares from the dot product to get the final score',
                        'When vectors are normalized by their L2 norm, cosine similarity becomes equivalent to the dot product of the unit vectors',
                    ],
                    correct: 4,
                    explanation:
                        'The formula for cosine similarity involves dividing the dot product of two vectors by the product of their magnitudes (L2 norms). If the vectors are first normalized to unit length (meaning their magnitude becomes 1), the denominator of the formula becomes 1. Thus, the cosine similarity calculation simplifies to just the dot product of the two unit vectors.',
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question:
                        'Which of the following is a key limitation of using the Jaccard coefficient for scoring in information retrieval?',
                    options: [
                        'It cannot handle Boolean queries',
                        'It does not consider the frequency of terms within a document',
                        'It requires documents to be of equal length',
                        'It only works for short queries',
                        'It is computationally expensive to calculate for large document collections',
                    ],
                    correct: 1,
                    explanation:
                        'The Jaccard coefficient measures the similarity between two sets based on the ratio of the size of their intersection to the size of their union. It operates on the presence or absence of elements (terms) only. It does not account for how many times a term appears within the document, which is a critical piece of information for more advanced relevance scoring models like TF-IDF.',
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question:
                        'In the context of information retrieval, which of the following scenarios would MOST benefit from incorporating term proximity into the scoring function?',
                    options: [
                        'A query where all documents in the collection contain the query terms',
                        'A query consisting of a single keyword',
                        "A query where the order of words is irrelevant to the user's information need",
                        'A query targeting documents with a high term frequency for individual keywords',
                        'A query where the exact phrase match is more important than individual term occurrences',
                    ],
                    correct: 4,
                    explanation:
                        "Term proximity is a scoring factor that gives higher weight to documents where the query terms appear close to each other. This is most valuable when the user is searching for a concept that is best represented as a phrase or by a close relationship between words (e.g., 'New York City' or 'effects of climate change'). When an exact phrase match is important, proximity scoring is essential.",
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question:
                        'Which of the following is the most accurate definition of tf-idf weighting?',
                    options: [
                        'A scheme that normalizes document length',
                        'A scheme that only considers term frequency within a document',
                        'A scheme that combines term frequency and inverse document frequency to weigh terms',
                        'A scheme that only considers the inverse document frequency of a term',
                        'A scheme that only considers the length of the document',
                    ],
                    correct: 2,
                    explanation:
                        'This is the definition of the TF-IDF (Term Frequency-Inverse Document Frequency) weighting scheme. It is a statistical measure used to evaluate how important a word is to a document in a collection. It works by multiplying the term frequency (local importance) by the inverse document frequency (global importance).',
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question:
                        'In the context of tf-idf weighting, which of the following statements best describes its effect on ranking documents for a query with multiple terms?',
                    options: [
                        'tf-idf only considers the most frequent term in the query',
                        'tf-idf prioritizes documents that contain stop words',
                        'tf-idf gives equal weight to all terms in the query, regardless of their frequency',
                        'tf-idf gives higher weight to terms that are frequent in a document but rare in the overall collection',
                        'tf-idf only considers the presence or absence of query terms in a document',
                    ],
                    correct: 3,
                    explanation:
                        "The TF-IDF score for a term is high if the term appears frequently in a particular document (high TF) but is rare in the overall collection (high IDF). When ranking a document for a multi-term query, the document's total score is a sum of the TF-IDF weights of the query terms it contains. This naturally gives preference to documents that are good matches for these discriminative terms.",
                },
                {
                    category: 'Foundational IR Concepts',
                    question:
                        'In the context of information retrieval, what is a "free text query"?',
                    options: [
                        'A query consisting of one or more words in a human language',
                        'A query that requires precise Boolean logic',
                        'A query that uses a specific query language with operators and expressions',
                        'A query that returns only a small subset of relevant documents',
                        'A query that is always interpreted literally without any stemming or lemmatization',
                    ],
                    correct: 0,
                    explanation:
                        "A free text query is what a typical user enters into a search engine: one or more keywords representing an information need, without any special query syntax like Boolean operators (AND, OR) or field specifiers. The system is responsible for interpreting this 'free text' to find relevant results.",
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question:
                        'What is "term frequency" (TF) in the context of information retrieval?',
                    options: [
                        'The number of documents in the collection that contain a specific term',
                        'The number of times a term appears in a specific document',
                        'The length of the term in characters',
                        'The number of times a term appears in the entire document collection',
                        'The inverse of the number of documents containing a specific term',
                    ],
                    correct: 1,
                    explanation:
                        "Term Frequency (TF) is a simple count of how many times a given term appears within a single document. It serves as a measure of a term's local importance; the more a term is used in a document, the more likely it is that the document is 'about' that term.",
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question:
                        'Which of the following is the most accurate definition of "term frequency (tf)"?',
                    options: [
                        'The number of times a term appears in the entire document collection',
                        'The length of the term',
                        'The number of documents in which a term appears',
                        'The number of times a term appears in a specific document',
                        'The inverse of the number of documents in which a term appears',
                    ],
                    correct: 3,
                    explanation:
                        "This is a direct definition. Term Frequency, or 'tf', is the raw count of the occurrences of a term within a given document. It is a core component in many scoring models, often logarithmically scaled.",
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question:
                        'Which of the following best describes the trade-off between accuracy and computational efficiency when retrieving information?',
                    options: [
                        'Exact Retrieval',
                        'Inverted Index',
                        'Probabilistic Retrieval',
                        'Boolean Retrieval',
                        'Approximate Retrieval',
                    ],
                    correct: 4,
                    explanation:
                        "Exact retrieval methods aim to find every single matching document, which can be computationally expensive. Approximate retrieval methods sacrifice a degree of accuracy or completeness to gain significant improvements in speed and efficiency. They are designed to find the 'top-K' most relevant results quickly, which is often sufficient for users.",
                },
                {
                    category: 'Information Retrieval Evaluation',
                    question:
                        'Consider two search engines: Engine A returns highly relevant results but misses some relevant documents, while Engine B returns most relevant documents but includes some irrelevant ones. Which of the following metrics would best differentiate the performance of these two engines, considering a user wants to find all relevant documents?',
                    options: [
                        'F1-score',
                        'Recall@K',
                        'Mean Reciprocal Rank (MRR)',
                        'Precision at K (P@K)',
                        'Mean Average Precision (MAP)',
                    ],
                    correct: 1,
                    explanation:
                        "Recall is the fraction of relevant documents that are successfully retrieved. If a user's goal is to find all relevant documents (e.g., in a legal discovery case), then recall is the most critical metric. Engine A misses some relevant documents (low recall), while Engine B returns most of them (high recall). Therefore, recall-based metrics like Recall@K would best differentiate their performance for this user's needs.",
                },
                {
                    category: 'Text Processing & Normalization',
                    question: 'Which of the following is a potential drawback of query expansion?',
                    options: [
                        'It requires a large amount of computational resources',
                        'It reduces the size of the index',
                        'It can decrease the recall of the system',
                        'It can introduce irrelevant terms into the query, decreasing precision',
                        'It can only be used with the Boolean retrieval model',
                    ],
                    correct: 3,
                    explanation:
                        "Query expansion works by adding synonyms or related terms to the original query to broaden the search. While this can increase recall (finding more relevant documents), it risks 'query drift'. The added terms might be polysemous (have multiple meanings) and cause the system to retrieve documents that are irrelevant to the user's original intent, thus lowering precision.",
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question: 'What role do static quality scores play in search engine ranking?',
                    options: [
                        'They replace query-dependent scores entirely',
                        'They are used to eliminate stop words',
                        'They are combined with query-dependent scores to influence the final ranking',
                        'They are only used for Boolean queries',
                        'They are used to normalize term frequencies',
                    ],
                    correct: 2,
                    explanation:
                        "A document's final ranking is usually determined by two main factors: its relevance to the specific query (a dynamic, query-dependent score like TF-IDF) and its overall authority or quality, independent of the query (a static quality score like PageRank). Combining these two scores ensures that the results are not only topically relevant but also from trustworthy sources.",
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question:
                        'Which of the following methods, when used in index elimination, reduces the candidate set by focusing only on documents containing many high-idf query terms?',
                    options: [
                        'Term-Document Incidence Matrix',
                        'Inverted Index',
                        'Boolean Retrieval',
                        'Vector Space Model',
                        'High-idf query terms only',
                    ],
                    correct: 4,
                    explanation:
                        'This is a specific index elimination strategy. Terms with a high Inverse Document Frequency (IDF) are rare and thus highly discriminative. By restricting the initial candidate set of documents to only those that contain these rare, important query terms, the system can drastically reduce the number of documents that need to be fully scored, thereby speeding up the process.',
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question:
                        'In the context of term frequency-idf (tf-idf) weighting schemes used in information retrieval, why is cos(SaS,PaP) greater than cos(SaS,WH)?',
                    options: [
                        'TF-IDF assigns higher weights to terms that appear more frequently in documents',
                        'TF-IDF weights documents based on their occurrence across the corpus',
                        'Cosine similarity measures the angle between vectors representing text content',
                        'Collection refers to the total number of documents in the dataset',
                        "The query is represented as a vector where each term's weight is its frequency in the query text",
                    ],
                    correct: 0,
                    explanation:
                        "The term 'tf-idf' is a combination of term frequency and inverse document frequency. The provided answer correctly identifies that a key part of this scheme is assigning higher weights to terms that appear more frequently in a document (the 'TF' component).",
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question:
                        'Why is cosine similarity preferred over Euclidean distance in vector space models for information retrieval?',
                    options: [
                        'To calculate the similarity between sparse vectors',
                        'It provides a scalar value for similarity comparison',
                        'To measure the angle between vectors',
                        'Because it handles documents of varying lengths effectively',
                        'To determine document relevance based on proximity',
                    ],
                    correct: 3,
                    explanation:
                        "Euclidean distance measures the straight-line distance between two points (vectors) and is highly sensitive to their magnitude. A long document containing a query term once might be 'further' away than a short document containing it once. Cosine similarity measures the angle between vectors, which is independent of their length. This makes it a much better measure for comparing documents of varying lengths on a topical basis.",
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question: 'Why is query-term proximity important in web search?',
                    options: [
                        'It ensures that all query terms are present in the document',
                        'It eliminates stop words',
                        'It improves the accuracy of stemming',
                        'It reduces the computational cost of scoring',
                        "It indicates that the document is focused on the user's query intent",
                    ],
                    correct: 4,
                    explanation:
                        "When a user's query terms appear close to each other in a document, it is a strong signal that the document is specifically about the relationship expressed in the query. For example, a document with the phrase 'machine learning applications' is more likely to be focused on the user's intent than a document where 'machine' appears on page 1 and 'learning' and 'applications' appear on page 20.",
                },
                {
                    category: 'Indexing & Data Structures',
                    question: 'What is a tiered index in information retrieval?',
                    options: [
                        'An index that stores terms based on their length',
                        'An index that stores terms in different languages',
                        'An index that stores documents in different geographical locations',
                        'An index that stores postings entries with different term frequency thresholds',
                        'An index that stores documents based on their file types',
                    ],
                    correct: 3,
                    explanation:
                        "A tiered index is an optimization that splits the postings list for a term into multiple 'tiers' based on document quality or importance (which can be estimated by metrics like term frequency or static PageRank). During query processing, the system can search the highest tier first to find good results quickly, and only proceed to lower tiers if not enough high-quality results are found.",
                },
                {
                    category: 'Indexing & Data Structures',
                    question:
                        'Which component of a search system is responsible for generating snippets of text that accompany each document in the results list?',
                    options: [
                        'Scoring module',
                        'Ranking module',
                        'Document cache',
                        'Indexer',
                        'Query parser',
                    ],
                    correct: 2,
                    explanation:
                        'Search result snippets are the short text extracts displayed to the user. To generate these quickly at query time without re-fetching the entire document, search engines maintain a cache of the documents. The snippet generation component uses this cache to find passages that contain the query terms and construct the summary.',
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question:
                        'What is the main characteristic of the Bag of Words model in information retrieval?',
                    options: [
                        'It uses stemming and lemmatization as the only preprocessing steps',
                        'It only considers the first 10 words of a document',
                        'It represents a document as an unordered set of words and their frequencies',
                        'It ignores the frequency of words in a document',
                        'It considers the order and structure of words in a document',
                    ],
                    correct: 2,
                    explanation:
                        "This is the fundamental definition of the Bag of Words (BoW) model. It simplifies text representation by ignoring grammar and word order, treating a document as an unordered multiset (or 'bag') of its words. The only information it retains is which words are present and their counts (frequencies).",
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question: 'What is the primary goal of cluster pruning?',
                    options: [
                        'To reduce the size of the index',
                        'To improve stemming efficiency',
                        'To reduce the number of documents for which cosine scores are computed',
                        'To eliminate duplicate documents',
                        'To improve the accuracy of cosine similarity calculations',
                    ],
                    correct: 2,
                    explanation:
                        'Cluster pruning is a ranked retrieval optimization technique. It works by grouping the entire document collection into clusters. At query time, instead of comparing the query to every document, the system first identifies the most relevant cluster(s) and then only computes the expensive cosine similarity scores for the documents within that small subset, drastically reducing computation.',
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question: 'What does term frequency (tf) represent?',
                    options: [
                        'The number of times a term appears in a specific document',
                        'The inverse of the number of documents in which a term appears',
                        'The number of documents in which a term appears',
                        'The total number of terms in the collection',
                        'The average length of documents in the collection',
                    ],
                    correct: 0,
                    explanation:
                        "This is the definition of term frequency (tf). It represents the number of times a specific term occurs within a single document and is used as a measure of that term's importance within that local context.",
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question:
                        'How does a scoring function in information retrieval take into account the quality of document matches?',
                    options: [
                        'Jaccard coefficient',
                        'Log-frequency weighting',
                        'Term frequency (tf)',
                        'Bag of words model',
                        'Collection vs. Document frequency',
                    ],
                    correct: 4,
                    explanation:
                        "This phrasing describes the core idea of TF-IDF. A document's relevance is determined not just by how often a term appears within it (term frequency, or TF), but also by how rare that term is across the entire collection (inverse document frequency, or IDF). Comparing the term's frequency in the document versus the collection gives a measure of its importance.",
                },
                {
                    category: 'Text Processing & Normalization',
                    question:
                        'Which of the following techniques is most likely to improve the recall of an information retrieval system without significantly sacrificing precision?',
                    options: [
                        'Increasing the weight of common terms in the tf-idf calculation',
                        'Implementing query expansion with carefully selected synonyms',
                        'Reducing the length of the document collection',
                        'Removing all stop words from the index and queries',
                        'Using a very aggressive stemming algorithm',
                    ],
                    correct: 1,
                    explanation:
                        'Recall measures the ability to find all relevant documents, while precision measures the ability to avoid irrelevant ones. Query expansion (adding terms) inherently increases recall but often hurts precision. The key is to be selective. Using carefully chosen synonyms expands the query in a controlled way, increasing the chance of finding relevant documents using different terminology, without adding excessive noise.',
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question: 'What is impact ordering in the context of information retrieval?',
                    options: [
                        'Ordering query terms by decreasing order of IDF',
                        'Ordering documents by their publication date',
                        'Ordering documents by their static quality scores',
                        'Ordering documents in the postings list of a term by decreasing order of tf- idf',
                        'Ordering documents randomly',
                    ],
                    correct: 3,
                    explanation:
                        'Impact ordering is a query processing optimization where postings lists are not sorted by document ID, but by a measure of impact or importance (such as a pre-calculated TF-IDF score or a static quality score). This ensures that when the system processes the list, it encounters the highest-impact documents first, enabling faster retrieval of top results.',
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question:
                        'Which of the following is NOT a typical step in processing a free text query in an information retrieval system?',
                    options: [
                        'Expanding the query with synonyms',
                        'Translating the query into machine code',
                        'Parsing the query into individual terms',
                        'Stemming or lemmatizing the query terms',
                        'Removing stop words from the query',
                    ],
                    correct: 1,
                    explanation:
                        "Information retrieval systems process queries by performing linguistic analysis (tokenization, stemming, etc.) and matching them against a text-based index. They do not compile the user's query into executable machine code in the way that a compiler processes source code for a programming language like C++ or Java.",
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question:
                        "When two documents d and d' are appended to each other, what happens when they are length-normalized?",
                    options: [
                        "The shorter document's weight is adjusted to match the longer one",
                        'The longer document will have a higher weight',
                        'Their weights will be adjusted so they have the same weight as each other',
                        'They will always have the highest possible cosine similarity',
                        'It depends on the term frequency',
                    ],
                    correct: 3,
                    explanation:
                        "If document d' is created by appending d to itself, its term frequency vector will be exactly 2 times the vector of d. When a vector is length-normalized, it is divided by its magnitude. The magnitude of 2*v is 2 times the magnitude of v. Therefore, the normalized vectors for d and d' will be identical. The cosine similarity of two identical vectors is 1, which is the highest possible value.",
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question:
                        "Consider two terms, 'apple' and 'the'. 'Apple' appears in 100 documents, and 'the' appears in 10,000 documents in a collection of 100,000 documents. According to inverse document frequency (idf), which term will have a higher idf weight and why?",
                    options: [
                        'The idf values cannot be compared without knowing the term frequencies (tf)',
                        'Both terms will have the same idf because idf only depends on the total number of documents',
                        "'The' will have a higher idf because it appears in more documents",
                        'Neither term will have a significant idf because both are common words',
                        "'Apple' will have a higher idf because it appears in fewer documents",
                    ],
                    correct: 4,
                    explanation:
                        "Inverse Document Frequency (IDF) is calculated as log(N/df), where N is the total number of documents and df is the number of documents the term appears in. A term that appears in fewer documents (like 'apple') has a lower df, which results in a higher IDF weight. A very common term (like 'the') has a high df and thus a very low IDF weight.",
                },
                {
                    category: 'Information Retrieval Evaluation',
                    question: 'What is the "precision" of an information retrieval system?',
                    options: [
                        'The proportion of relevant documents that are retrieved',
                        'The speed at which the system retrieves documents',
                        'The proportion of retrieved documents that are relevant',
                        'The amount of storage space used by the index',
                        'The number of documents in the collection',
                    ],
                    correct: 2,
                    explanation:
                        "Precision is a key evaluation metric in information retrieval. It is defined as the fraction of retrieved documents that are actually relevant to the user's query. A high precision score means that the system is returning accurate results and not a lot of irrelevant junk.",
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question:
                        'Which data structure is typically used to efficiently select the K top documents based on their cosine similarity scores when performing ranked retrieval?',
                    options: ['Sort', 'Heap', 'Merge Sort', 'TF-IDF', 'Bag of Words'],
                    correct: 1,
                    explanation:
                        "To find the top K documents from a large collection, a min-heap of size K is the most efficient data structure. The system computes a score for each document and compares it to the smallest score in the heap. If it's larger, the smallest element is removed and the new one is added. This process maintains the top K results with a time complexity far better than sorting the entire collection.",
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question:
                        "What is the main characteristic of the 'bag of words' model in information retrieval?",
                    options: [
                        'It assigns different weights to different words based on their semantic meaning',
                        'It only considers the most frequent words in a document',
                        'It treats documents as unordered collections of words, disregarding grammar and word order',
                        'It considers the order and position of words in a document',
                        'It uses a thesaurus to expand the query with synonyms',
                    ],
                    correct: 2,
                    explanation:
                        "This is the defining characteristic of the 'bag of words' model. It intentionally discards all information about the order of words and grammatical structure, simplifying the document to an unordered collection (a multiset) of words. This makes it easy to represent as a vector but loses significant semantic meaning.",
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question:
                        'What does "document frequency" (DF) represent in information retrieval?',
                    options: [
                        'The number of times a specific term appears in a given document',
                        'The number of documents in the collection that contain a specific term',
                        'The frequency of the most common term in the collection',
                        'The average length of documents in the collection',
                        'The total number of terms in the document collection',
                    ],
                    correct: 1,
                    explanation:
                        "Document Frequency (DF) is a statistic that counts the number of documents in an entire collection that contain a specific term at least once. It's a measure of the term's overall commonness and is the basis for calculating Inverse Document Frequency (IDF).",
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question: 'Why is log-frequency weighting often used in information retrieval?',
                    options: [
                        'To normalize document lengths',
                        'To eliminate stop words',
                        'To reduce the impact of very frequent terms',
                        'To improve query processing speed',
                        'To give more weight to rare terms',
                    ],
                    correct: 2,
                    explanation:
                        'Raw term frequency (tf) gives undue weight to terms that appear many times. Log-frequency weighting (e.g., 1 + log(tf)) is used to dampen this effect. It ensures that the score contribution from a term increases as its frequency increases, but at a diminishing rate, preventing very frequent terms from disproportionately influencing the final document score.',
                },
                {
                    category: 'Foundational IR Concepts',
                    question:
                        'Which of the following is NOT a typical scale of operation for IR systems?',
                    options: [
                        'Personal information retrieval',
                        'Enterprise search',
                        'Quantum computing search',
                        'Web search',
                        'Domain-specific search',
                    ],
                    correct: 2,
                    explanation:
                        "Web search, enterprise search, and personal information retrieval are all established applications and scales of IR systems. While quantum computing is a field of research, and quantum algorithms for search exist (e.g., Grover's algorithm), 'Quantum computing search' is not a typical, practical scale of operation for current information retrieval systems.",
                },
                {
                    category: 'Text Processing & Normalization',
                    question: 'What is the Soundex algorithm primarily used for?',
                    options: [
                        'To translate words into different languages',
                        'To equivalence class or expand terms with phonetic equivalents',
                        'To generate synonyms for words',
                        'To stem words to their root form',
                        'To correct spelling errors in words',
                    ],
                    correct: 1,
                    explanation:
                        "The Soundex algorithm creates a phonetic code for a word, primarily a name. Its purpose is to group words that sound alike but may be spelled differently (e.g., 'Smith' and 'Smythe') into the same phonetic equivalence class. This is useful for searching in contexts where exact spelling is uncertain.",
                },
                {
                    category: 'Foundational IR Concepts',
                    question:
                        'Who among the following was NOT one of the main authors of the book?',
                    options: [
                        'Pavel Berkhin',
                        'Ghulam Raza',
                        'Thomas Zeume',
                        'Andrew Turner',
                        'James Allan',
                    ],
                    correct: 0,
                    explanation: 'Pavel Berkhin is not a main author.',
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question:
                        'Which of the following best describes why extended Boolean retrieval models were used in commercial systems before the early 1990s?',
                    options: [
                        'They relied solely on basic Boolean operations (AND, OR, NOT) without additional features',
                        'They were primarily used after the arrival of the World Wide Web in the early 1990s',
                        'They were the only search method available until the invention of ranked retrieval models',
                        'They provided precise control over search results using operators like proximity (/s, /p, /k)',
                        'They allowed users to type free text queries without any operators',
                    ],
                    correct: 3,
                    explanation:
                        "Early commercial retrieval systems, often used by expert searchers like lawyers and paralegals, prioritized control and predictability. Extended Boolean models provided this by allowing users to construct very precise queries using proximity operators (e.g., find 'contract' within the same sentence as 'breach'). This level of control was more valuable to them than the automatically ranked results of other models.",
                },
                {
                    category: 'Foundational IR Concepts',
                    question:
                        'According to the text, which statement is true regarding structured and unstructured data?',
                    options: [
                        'Almost no data are truly unstructured if you consider inherent structures like language or markup',
                        'Information retrieval primarily deals with structured data',
                        'Structured data refers exclusively to relational databases',
                        'All text data is considered structured',
                        'Unstructured data has no inherent structure at all',
                    ],
                    correct: 0,
                    explanation:
                        "This statement highlights a key concept in IR. While text data is called 'unstructured' because it lacks a rigid database schema, it is not devoid of structure. It contains inherent linguistic structure (grammar, syntax) and often formatting structure (document titles, paragraphs, HTML tags). Effective IR systems are designed to recognize and leverage this implicit structure.",
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question:
                        'Which of the following retrieval models is most closely associated with the concept of "relevance feedback"?',
                    options: [
                        'Vector Space Model with Rocchio Algorithm',
                        'Boolean Retrieval Model',
                        'Inferred Model',
                        'Language Model',
                        'Probabilistic Retrieval Model',
                    ],
                    correct: 0,
                    explanation:
                        "Relevance feedback is the process of refining a query based on a user's judgments of initial results. The Rocchio Algorithm is the classic and most famous method for implementing relevance feedback within the Vector Space Model. It modifies the query vector by moving it toward the centroid of relevant documents and away from the centroid of irrelevant documents.",
                },
                {
                    category: 'Text Processing & Normalization',
                    question:
                        'Why might an IR system choose not to remove stop words, despite their high frequency?',
                    options: [
                        'Stop words are needed for stemming algorithms',
                        'Stop words are always short and easy to process',
                        'Stop words are essential for phrase queries',
                        'Stop words are crucial for language identification',
                        'Stop words can improve the precision of retrieval',
                    ],
                    correct: 2,
                    explanation:
                        "While stop words are often removed to improve efficiency, they are critical for understanding the meaning of certain queries, especially phrase queries. A query like 'to be or not to be' would become nonsensical if stop words were removed. Therefore, to correctly process such phrases, a system must retain stop words in its index and processing pipeline.",
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question:
                        'Level 5: In a situation where all query terms have very high document frequencies (i.e., appear in many documents), which retrieval model might struggle the most, and why?',
                    options: [
                        'Vector Space Model, because cosine similarity would be low',
                        'Probabilistic Retrieval Model, because it would be difficult to estimate probabilities',
                        'Boolean Retrieval, because it would return too many documents',
                        'All models would perform equally well',
                        'No model would struggle',
                    ],
                    correct: 2,
                    explanation:
                        "In a Boolean retrieval model, a query with common terms connected by 'AND' (e.g., 'the AND of AND a') would match any document containing all those words. Since these are high-frequency words, the result set would be enormous, providing little value to the user. This is often called the 'firehose' problem. Ranked retrieval models handle this better by assigning very low weights (via IDF) to common terms.",
                },
                {
                    category: 'Indexing & Data Structures',
                    question: 'What is the purpose of skip pointers in postings lists?',
                    options: [
                        'To reduce the storage space required for postings lists',
                        'To speed up the intersection of postings lists',
                        'To provide a graphical representation of the document collection',
                        'To automatically correct spelling errors in queries',
                        'To improve the ranking of search results',
                    ],
                    correct: 1,
                    explanation:
                        'This is the primary purpose of skip pointers. During the intersection of postings lists for an AND query, skip pointers allow the processing algorithm to jump over large blocks of non-matching document IDs, avoiding a linear, one-by-one comparison. This greatly accelerates the intersection (merge) operation, especially for long postings lists.',
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question: 'Which of the following statements about Boolean queries is true?',
                    options: [
                        'Experts recommend Boolean queries for high-recall searches',
                        'Using "and" in Boolean queries increases precision but may lower recall',
                        'Free text queries are less effective than Boolean queries',
                        'The extended Boolean model is better at handling spelling mistakes',
                        'Boolean queries are always more effective than free-text queries',
                    ],
                    correct: 1,
                    explanation:
                        "Adding an 'AND' clause to a query makes it more restrictive, as a document must now satisfy an additional condition. This typically reduces the number of retrieved documents, which tends to increase precision (the proportion of retrieved documents that are relevant). However, it may also cause the system to miss relevant documents that don't meet the new, stricter criteria, thus lowering recall.",
                },
                {
                    category: 'Foundational IR Concepts',
                    question: 'What is the primary function of Information Retrieval (IR)?',
                    options: [
                        'Locating structured data in databases',
                        'Organizing emails in folders',
                        'Creating new documents',
                        'Managing file systems',
                        'Finding material (usually documents)',
                    ],
                    correct: 4,
                    explanation:
                        "This is the broad, encompassing definition of Information Retrieval (IR). Its primary function is to address a user's information need by finding relevant material (which is usually text documents, but can be other media) from within a large collection.",
                },
                {
                    category: 'Indexing & Data Structures',
                    question:
                        'What is the advantage of using a B-tree over a binary search tree for dictionary lookup, especially when the dictionary is disk-resident?',
                    options: [
                        "B-trees don't require an ordered character set",
                        'B-trees are always faster',
                        'B-trees require less memory',
                        'B-trees are easier to implement',
                        'B-trees can collapse multiple levels, reducing disk accesses',
                    ],
                    correct: 4,
                    explanation:
                        'Disk seeks are a major performance bottleneck. B-trees are optimized for disk-based storage by being short and wide. They have a high branching factor (many children per node), which dramatically reduces the height of the tree compared to a binary tree. A shorter tree requires fewer node traversals to find an item, which means fewer slow disk accesses.',
                },
                {
                    category: 'Indexing & Data Structures',
                    question:
                        'You have the wildcard query "appl*". Using a 3-gram index, which Boolean query would you construct to find potential matching terms?',
                    options: [
                        '$ap AND ppl',
                        'ap AND pl',
                        '$ap AND apl',
                        '$ap AND app AND ppl',
                        'app AND ppl AND pl$',
                    ],
                    correct: 3,
                    explanation:
                        'To handle a wildcard query like appl* using a 3-gram index, the query is first augmented to $appl, where $ is a special start-of-word character. The 3-grams are then extracted: $ap, app, and ppl. A Boolean query $ap AND app AND ppl is then run against the 3-gram index to find all candidate terms that start with appl. (PDF Answer: apANDaplANDpl)',
                },
                {
                    category: 'Indexing & Data Structures',
                    question: 'What is a "positional index" used for?',
                    options: [
                        'To store the citation information of documents',
                        'To store the geographical location of documents',
                        'To store the sentiment of documents (positive, negative, neutral)',
                        'To store the ranking of documents based on their relevance to a query',
                        'To store the position of each term within a document',
                    ],
                    correct: 4,
                    explanation:
                        'A positional index is an enhancement to a standard inverted index. In addition to storing the document IDs in its postings lists, it also stores the position (e.g., word offset from the beginning of the document) of each occurrence of the term. This positional information is essential for handling phrase and proximity queries.',
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question:
                        'Which retrieval model represents documents and queries as vectors in a high-dimensional space?',
                    options: [
                        'Vector Space Model',
                        'Probabilistic Retrieval',
                        'Inverted Index',
                        'Language Model',
                        'Boolean Retrieval',
                    ],
                    correct: 0,
                    explanation:
                        'This is the defining feature of the Vector Space Model. It represents all documents and queries as vectors in a multi-dimensional space, where each dimension corresponds to a unique term from the vocabulary. This allows retrieval to be treated as a geometric problem of finding the document vectors closest to the query vector.',
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question:
                        'Level 2: What is the purpose of the "merge" operation in processing an "AND" query using Inverted Indexes?',
                    options: [
                        'To remove duplicate document IDs',
                        'To sort the postings lists by document ID',
                        'To calculate term frequencies',
                        'To combine the postings lists of multiple terms',
                        'To find the intersection of document sets containing the query terms',
                    ],
                    correct: 4,
                    explanation:
                        "For a Boolean 'AND' query, a document is only a match if it contains all the query terms. In an inverted index system, this means finding the document IDs that are present in the postings lists of all the query terms. The 'merge' algorithm is the process that efficiently traverses these sorted lists to find their common elements, i.e., their intersection.",
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question: 'In the Boolean retrieval model, how are documents viewed?',
                    options: [
                        'As a vector of term frequencies',
                        'As a probability distribution of terms',
                        'As a graph of interconnected words',
                        'As a ranked list of terms',
                        'As a set of terms',
                    ],
                    correct: 4,
                    explanation:
                        "The basic Boolean retrieval model operates on the simple presence or absence of terms. It does not consider term frequency, position, or any other advanced feature. Therefore, from the model's perspective, a document is simply a set of the words it contains.",
                },
                {
                    category: 'Text Processing & Normalization',
                    question:
                        'What is a key advantage of using asymmetric expansion in query processing?',
                    options: [
                        'It enables',
                        'It prevents unintended matches by allowing asymmetrical term expansions',
                        'It ensures that all terms are treated identically during indexing',
                        'It requires less processing time at query execution',
                        'It allows for more efficient storage by reducing the number of postings lists',
                    ],
                    correct: 1,
                    explanation:
                        "Symmetric expansion (e.g., making 'car' and 'automobile' equivalent) can lead to unintended results. Asymmetric expansion provides more control. For instance, a query for 'Windows' (the operating system) should not retrieve documents about physical windows. An asymmetric rule can ensure that a query for 'Windows' only matches related computer terms, preventing unintended matches and improving precision.",
                },
                {
                    category: 'Text Processing & Normalization',
                    question:
                        'Which of the following is NOT a typical edit operation used to calculate edit distance?',
                    options: [
                        'Deletion',
                        'Substitution',
                        'Insertion',
                        'Transposition',
                        'Duplication',
                    ],
                    correct: 4,
                    explanation:
                        'Edit distance (specifically, Levenshtein distance) is a measure of the difference between two strings, calculated as the minimum number of single-character edits required to change one string into the other. The standard primitive edit operations are insertion, deletion, and substitution. Duplication of a character is not one of these standard operations.',
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question: 'What is the main idea behind Probabilistic Retrieval models?',
                    options: [
                        'Ranking documents based on their estimated probability of relevance to the query',
                        'Representing documents and queries as vectors',
                        'Retrieving documents that contain all query terms',
                        'Using Boolean logic to retrieve documents',
                        'Using term frequencies to rank documents',
                    ],
                    correct: 0,
                    explanation:
                        'This is the core idea of the probabilistic approach to IR, formalized by the Probability Ranking Principle (PRP). The model attempts to estimate the probability that a document d is relevant to a query q, P(R=1|d,q). It then ranks documents in descending order of this probability, which is considered the optimal ranking for a user.',
                },
                {
                    category: 'Indexing & Data Structures',
                    question:
                        'How could an IR system combine the use of a positional index and stop words? What is the potential problem, and how could it be handled? Options:',
                    options: [
                        'Use positional indexes and apply stop word removal before indexing. The potential issue is inefficiency due to multiple passes. To handle this, optimize the process so stop words are marked during indexing',
                        'Integrate positional data with stop word lists for retrieval efficiency. However, without context, this approach may be limited. Enhance it by using context- aware algorithms',
                        'Leverage both methods for efficient indexing. The problem is dual processing overhead. Use parallel processing to improve performance',
                        "Combine the two techniques by analyzing each word's position across documents. The problem is that this might require complex analysis. A solution would be to preprocess documents to mark stop words clearly",
                        'Merge positional and stop word data into a single index. This could cause bloat and retrieval problems. Implement efficient storage and querying techniques to mitigate this',
                    ],
                    correct: 3,
                    explanation:
                        "Handling phrase queries requires knowing the positions of words, which necessitates a positional index. However, many phrase queries involve stop words (e.g., 'to be or not to be'). A simple solution would be to not index stop words, but this would make it impossible to answer the phrase query. A more sophisticated system must therefore find a way to combine positional indexing with the handling of stop words, for instance by marking them during preprocessing so they can be handled specially.",
                },
                {
                    category: 'Indexing & Data Structures',
                    question:
                        'In an inverted index, what is used for vocabulary lookup to determine if a query term exists and retrieve its postings?',
                    options: [
                        'A binary search tree',
                        'A dictionary data structure using hash tables',
                        'An inverted list',
                        'A term-document incidence matrix',
                        'A wildcard query',
                    ],
                    correct: 0,
                    explanation:
                        'To quickly find a term within a large vocabulary (the dictionary of the inverted index), an efficient data structure is needed. Both hash tables and tree-based structures like B-trees or binary search trees are used. A key advantage of a binary search tree (or B-tree) is that it maintains the terms in sorted order, which naturally supports efficient prefix searches for wildcard queries (e.g., info*).',
                },
                {
                    category: 'Text Processing & Normalization',
                    question: 'What is "case-folding" in the context of text processing?',
                    options: [
                        'Replacing all numbers with words',
                        'Converting all text to lowercase',
                        'Translating the text into a different language',
                        'Removing all punctuation from the text',
                        'Converting all text to uppercase',
                    ],
                    correct: 1,
                    explanation:
                        "Case-folding is a standard text normalization technique. It involves converting all characters in the text to a single case, typically lowercase. This ensures that words that differ only in their capitalization (e.g., 'Search', 'search', 'SEARCH') are treated as the same term, which is usually the desired behavior in retrieval systems.",
                },
                {
                    category: 'Text Processing & Normalization',
                    question:
                        'In determining the correct tokenization for names like "O\u2019Neill", which option represents the most appropriate way to tokenize this name?',
                    options: ['o\u2019 neill', 'neill', 'o\u2019neill', 'o neill', 'oneill'],
                    correct: 2,
                    explanation:
                        "For a name like 'O'Neill', the apostrophe is a meaningful part of the name. Simply splitting on non-alphanumeric characters would result in 'o' and 'neill'. The most appropriate tokenization is to keep the name as a single token. This token would then typically be case-folded to 'o'neill' for indexing.",
                },
                {
                    category: 'Foundational IR Concepts',
                    question: 'In the context of IR, what does "unstructured data" refer to?',
                    options: [
                        'Data easily processed by computers',
                        'Data organized in tables',
                        'Data with explicit markup',
                        'Data without clear, semantically overt structure',
                        'Data in relational databases',
                    ],
                    correct: 3,
                    explanation:
                        "In the context of information systems, 'structured data' refers to data that resides in a fixed field within a record or file, such as data in a relational database. 'Unstructured data' refers to data that does not have a pre-defined data model or is not organized in a pre-defined manner. Free-form text is the most common example, as it has linguistic structure but lacks an explicit, semantic schema.",
                },
                {
                    category: 'Modern NLP & Neural IR',
                    question:
                        'What is the primary purpose of Retrieval Augmented Generation (RAG) in the context of Large Language Models (LLMs)?',
                    options: [
                        'To reduce the computational cost of training LLMs',
                        'To enable LLMs to perform sentiment analysis',
                        'To fine-tune LLMs for specific tasks',
                        "To improve the factual accuracy and reduce hallucinations by grounding the LLM's responses in external knowledge",
                        'To translate text between multiple languages',
                    ],
                    correct: 3,
                    explanation:
                        "Large Language Models (LLMs) can sometimes generate text that is plausible but factually incorrect, a phenomenon known as 'hallucination'. Retrieval-Augmented Generation (RAG) addresses this by first retrieving relevant information from a reliable external knowledge source and then providing this information to the LLM as context for its response. This 'grounds' the generation in facts, improving accuracy.",
                },
                {
                    category: 'Modern NLP & Neural IR',
                    question:
                        'Which component of the Transformer architecture allows each element in a sequence to interact with every other element, capturing complex contextual relationships?',
                    options: [
                        'Pooling Layer',
                        'Recurrent Layer',
                        'Convolutional Layer',
                        'Embedding Layer',
                        'Self-Attention Mechanism',
                    ],
                    correct: 4,
                    explanation:
                        'This is the key innovation of the Transformer architecture. In contrast to RNNs which process sequences token-by-token, the self-attention mechanism allows every token in the input sequence to directly interact with and weigh the importance of every other token in the sequence. This enables the model to capture complex, long-range dependencies in a single, parallelizable step.',
                },
                {
                    category: 'Modern NLP & Neural IR',
                    question:
                        'In the context of prompt engineering for Large Language Models (LLMs), what is "few-shot prompting"?',
                    options: [
                        'Providing the LLM with a few examples of the desired input-output behavior in the prompt itself',
                        'Training a separate model to generate prompts for the LLM',
                        'Prompting the LLM with very short and concise instructions',
                        'Providing the LLM with a large amount of training data before prompting',
                        'Prompting the LLM with Questions that have only a few possible answers',
                    ],
                    correct: 0,
                    explanation:
                        "Few-shot prompting is a technique for guiding an LLM where the user provides a small number of examples ('shots') of the task they want performed directly within the prompt. The model then uses these input-output pairs as a template to generate the correct output for a new, unseen input. This allows the model to perform tasks it wasn't explicitly trained on, just by seeing a few examples.",
                },
                {
                    category: 'Modern NLP & Neural IR',
                    question:
                        'What is the main advantage of using Retrieval-Augmented Generation (RAG) with Large Language Models (LLMs)?',
                    options: [
                        'It eliminates the need for fine-tuning LLMs on specific tasks',
                        'It reduces the computational cost of training LLMs',
                        'It allows LLMs to access and incorporate external knowledge sources',
                        "It improves the model's ability to perform mathematical calculations",
                        'It simplifies the process of prompt engineering',
                    ],
                    correct: 2,
                    explanation:
                        'The primary advantage of Retrieval-Augmented Generation (RAG) is that it connects a Large Language Model (LLM) to an external, up-to-date, or specialized knowledge source. This allows the LLM to generate responses based on information that was not part of its original, static training data, thereby increasing its factual accuracy and relevance.',
                },
                {
                    category: 'Indexing & Data Structures',
                    question:
                        'What is the primary purpose of an inverted index in information retrieval?',
                    options: [
                        'Reduce storage space by removing zeros',
                        'Calculate precision and recall',
                        'Track the positions of terms within documents',
                        'Map terms to the documents they appear in',
                        'Store the entire term-document matrix',
                    ],
                    correct: 3,
                    explanation:
                        'This is the core function of an inverted index. It is a data structure that maps content, such as words or numbers, to their locations in a database file, or in a document or a set of documents. It is the most popular data structure used in information retrieval systems, used on a large scale for web search engines.',
                },
                {
                    category: 'Text Processing & Normalization',
                    question: 'What is the purpose of tokenization in information retrieval?',
                    options: [
                        'To rank documents based on their relevance to a query',
                        'To translate a document into a different language',
                        'To remove stop words from a document',
                        'To divide a stream of text into individual words or units',
                        'To convert all text to lowercase',
                    ],
                    correct: 3,
                    explanation:
                        'Tokenization is the foundational step of the Natural Language Processing (NLP) pipeline. It is the process of breaking down a stream of raw text into its constituent parts, called tokens. These tokens can be words, numbers, punctuation marks, or other meaningful elements, which then serve as the input for subsequent processing steps like stemming or indexing.',
                },
                {
                    category: 'Modern NLP & Neural IR',
                    question:
                        'What is a major challenge that LSTMs (Long Short-Term Memory networks) are designed to address in Recurrent Neural Networks (RNNs)?',
                    options: [
                        'The vanishing gradient problem, which hinders learning long-range dependencies',
                        'The high computational cost of training',
                        'The difficulty in parallelizing computations',
                        'The lack of memory capacity for storing past information',
                        'The inability to process sequential data',
                    ],
                    correct: 0,
                    explanation:
                        'The vanishing gradient problem is a major obstacle in training standard Recurrent Neural Networks (RNNs). Because gradients are propagated multiplicatively over time, they can become exponentially small for long sequences. This prevents the network from learning long-range dependencies between distant elements in the sequence. LSTMs were specifically invented to mitigate this problem using their gating mechanism.',
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question:
                        'In the context of Natural Language Processing (NLP), what is a primary limitation of the Bag of Words (BoW) approach?',
                    options: [
                        'It requires significant computational resources',
                        'It is not applicable to text data',
                        'It fails to consider word frequency',
                        'It loses word order and grammatical structure',
                        'It cannot handle large vocabularies',
                    ],
                    correct: 3,
                    explanation:
                        'The Bag of Words (BoW) model represents text as an unordered collection of its words, completely disregarding the original word order and any grammatical relationships between them. This loss of sequential information is a primary limitation, as the meaning of text is often highly dependent on the order of its words.',
                },
                {
                    category: 'Text Processing & Normalization',
                    question: 'Level 3: How does stemming improve retrieval effectiveness?',
                    options: [
                        'By reducing words to their root form, increasing matches',
                        'By correcting spelling errors',
                        'By removing common words',
                        'By expanding the query with related terms',
                        'By identifying synonyms',
                    ],
                    correct: 0,
                    explanation:
                        "Stemming reduces different forms of a word (e.g., 'retrieving', 'retrieved', 'retrieval') to a common root ('retriev'). This allows a query for one form to match documents containing another. By increasing the number of ways a query can match documents, stemming increases the number of relevant documents found, thereby improving retrieval effectiveness, primarily through higher recall.",
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question:
                        'When generating multiple queries for ranked retrieval based on user input, such as expanding "rising interest rates" into three separate phrase and vector space queries, what is the appropriate way to aggregate the scores from each component?',
                    options: [
                        'Sum of the maximum scores from each query',
                        'Sum of all scores, regardless of their origin',
                        'Use cosine similarity alone as a combined score',
                        'Average of all individual scores',
                        'Take the highest score among all queries',
                    ],
                    correct: 4,
                    explanation:
                        "When a user's information need is expanded into multiple distinct sub-queries (e.g., a phrase query and a vector space query), a document should be considered a good match if it scores highly on any of those interpretations. The most common way to aggregate the scores is to assign the document the maximum score it achieved across all the sub-queries, reflecting its best possible match to the user's intent.",
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question:
                        'What is the key difference between document frequency (df) and collection frequency (cf)?',
                    options: [
                        'Document frequency is the number of documents in which a term appears, while collection frequency is the total number of times a term appears in the entire collection',
                        'Document frequency is calculated using logarithms, while collection frequency is not',
                        'Document frequency is the number of times a term appears in a specific document, while collection frequency is the total number of documents in the collection',
                        'Document frequency and collection frequency are the same thing',
                        'Document frequency is used for stop words, while collection frequency is used for rare words',
                    ],
                    correct: 0,
                    explanation:
                        "These two metrics measure term frequency at different granularities. Document Frequency (df) is the number of documents in a collection that contain a term at least once. Collection Frequency (cf) is the total number of times a term appears across all documents in the collection. For example, if 'cat' appears 3 times in one document and once in another, its df is 2, but its cf is 4.",
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question:
                        'What does the term frequency (tf) represent in the context of information retrieval?',
                    options: [
                        'The inverse of the document frequency',
                        'The number of unique terms in the collection',
                        'The total number of documents in the collection',
                        'The number of times a term appears in a specific document',
                        'The length of the document',
                    ],
                    correct: 3,
                    explanation:
                        "This is the definition of term frequency (tf), a core concept in information retrieval. It measures the frequency of a term within a specific document, serving as a signal of that term's importance to that particular document's content.",
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question:
                        'What is the key difference between collection frequency and document frequency?',
                    options: [
                        'Collection frequency only considers terms in the query, while document frequency considers terms in the documents',
                        'There is no difference between collection frequency and document frequency',
                        'Collection frequency counts the number of documents containing a term, while document frequency counts term occurrences in a single document',
                        'Collection frequency is used for ranking, while document frequency is used for stop word removal',
                        'Collection frequency counts term occurrences across the entire corpus, while document frequency counts the number of documents containing the term',
                    ],
                    correct: 4,
                    explanation:
                        "Collection frequency (cf) is the total count of a term's occurrences across the entire collection of documents. Document frequency (df) is the count of how many distinct documents contain that term at least once. cf will always be greater than or equal to df.",
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question:
                        'What is the main idea behind champion lists (or top docs) in information retrieval?',
                    options: [
                        'To cluster documents based on their similarity',
                        'To store all documents in a ranked order',
                        'To eliminate documents with low IDF scores',
                        'To precompute the top r documents for each term based on their weights',
                        'To perform Boolean retrieval more efficiently',
                    ],
                    correct: 3,
                    explanation:
                        "Champion lists (or top docs) are an efficiency mechanism for ranked retrieval. For each term in the vocabulary, the system pre-computes and stores a list of the top 'r' documents that are most relevant to that term (based on a static score like PageRank). At query time, the system can then restrict its expensive scoring calculations to only the documents that appear in these pre-computed lists, greatly speeding up retrieval.",
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question:
                        'In the context of tf-idf weighting, what does the "idf" component represent?',
                    options: [
                        'The number of documents in the collection that contain a specific term',
                        'The length of a document normalized by the number of terms it contains',
                        'The inverse of the frequency of a term across the entire collection of documents',
                        'The total number of terms in the collection',
                        'The frequency of a term within a specific document',
                    ],
                    correct: 2,
                    explanation:
                        "The 'idf' in 'tf-idf' stands for Inverse Document Frequency. It is a measure of how much information a word provides, i.e., whether it is common or rare across all documents. It is calculated as the logarithm of the total number of documents divided by the number of documents containing the term. This gives a higher weight to rarer terms.",
                },
                {
                    category: 'Foundational IR Concepts',
                    question:
                        'In the context of information retrieval, what does a "free text query" refer to?',
                    options: [
                        'A query that is free of charge',
                        'A query that requires the user to specify the exact location of the desired information',
                        'A query that contains only stop words',
                        'A query that is limited to a specific number of characters',
                        'A query written in a natural language, without specific syntax requirements',
                    ],
                    correct: 4,
                    explanation:
                        'A free text query is the typical way a user interacts with a modern search engine. It consists of one or more words entered in a natural language format, without the need for the user to learn or apply a specific, structured query syntax like Boolean operators (AND, OR, NOT).',
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question:
                        'In the Vector Space Model, how are documents and queries represented?',
                    options: [
                        'As sequences of characters',
                        'As vectors in a high-dimensional space',
                        'As nodes in a graph',
                        'As sets of terms',
                        'As probability distributions',
                    ],
                    correct: 1,
                    explanation:
                        'This is the central concept of the Vector Space Model. Documents and queries are treated as vectors of term weights in a high-dimensional space where each dimension corresponds to a term in the vocabulary. Their similarity is then calculated geometrically, typically using the cosine of the angle between their vectors.',
                },
                {
                    category: 'Text Processing & Normalization',
                    question:
                        'Which of the following statements about terms and tokens in IR systems is correct?',
                    options: [
                        'All tokens are considered terms in an IR system',
                        'Only exact matches of tokens can be considered terms',
                        'Terms are the raw tokens before any processing occurs',
                        'Types and terms are the same concept in IR systems',
                        'Tokens must undergo normalization to become terms',
                    ],
                    correct: 4,
                    explanation:
                        "The information retrieval pipeline distinguishes between tokens and terms. 'Tokens' are the raw character sequences extracted from the text during the initial tokenization step (e.g., 'Running', 'U.S.A.'). 'Terms' are the final, standardized forms of these tokens that are entered into the index's dictionary after undergoing normalization processes like case-folding and stemming (e.g., 'run', 'usa').",
                },
                {
                    category: 'Indexing & Data Structures',
                    question:
                        'What data structure is commonly used to efficiently find documents containing specific terms?',
                    options: [
                        'Skip List',
                        'Term-Document Incidence Matrix',
                        'Forward Index',
                        'Adjacency Matrix',
                        'Inverted Index',
                    ],
                    correct: 4,
                    explanation:
                        'The Inverted Index is the central data structure for efficient information retrieval. It provides a direct mapping from any term to a postings list, which contains the IDs of all documents that contain that term. This allows the system to find all documents containing a specific term without having to scan the entire collection.',
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question:
                        'During a postings scan for a query q and a term t, when computing the Jaccard coefficient, which structure is utilized to efficiently retrieve the number of k-grams in t?',
                    options: [
                        'Inverted Index',
                        'Probabilistic Retrieval',
                        'Term-Document Incidence Matrix',
                        'Boolean Retrieval',
                        'Vector Space Model',
                    ],
                    correct: 2,
                    explanation:
                        "The inverted index is the ideal choice for efficiently retrieving the number of k-grams in a term during a postings scan because its dictionary stores term metadata, such as the k-gram count, allowing quick access without additional computation. This directly supports computing the Jaccard coefficient, as the postings scan can rapidly process the term's postings list using this readily available information to evaluate similarity between terms.",
                },
                {
                    category: 'Indexing & Data Structures',
                    question:
                        'According to the text, what is a common heuristic for placing skip pointers in a postings list of length L?',
                    options: [
                        'Use L/2 evenly-spaced skip pointers',
                        'Use log(L) evenly-spaced skip pointers',
                        'Place a skip pointer after every term',
                        'Place a skip pointer at every 10th entry',
                        'Use \u221aL evenly-spaced skip pointers',
                    ],
                    correct: 4,
                    explanation:
                        'This is a standard and well-established heuristic for optimizing the intersection of postings lists. The use of \u221aL skip pointers provides a good theoretical balance between the overhead of storing and traversing the pointers themselves and the gains achieved by skipping sections of the list. It aims to make the total number of operations proportional to the length of the lists.',
                },
                {
                    category: 'Indexing & Data Structures',
                    question:
                        'In a B-tree used for dictionary search, what is the primary advantage of having multiple children per internal node (compared to a binary tree)?',
                    options: [
                        'It simplifies the rebalancing process',
                        'It eliminates the need for a separate postings list',
                        'It allows for faster insertion of new terms',
                        'It makes it easier to handle wildcard queries',
                        'It reduces the height of the tree, improving search efficiency',
                    ],
                    correct: 4,
                    explanation:
                        "B-trees are optimized for storage systems where accessing data is slow compared to processing it (like a hard disk). By having many children per node (a high branching factor), the height of the tree is significantly reduced. Since each level traversal can correspond to a slow disk seek, minimizing the tree's height is the most critical factor for improving search performance.",
                },
                {
                    category: 'Text Processing & Normalization',
                    question:
                        'Which of the following is TRUE about STOP WORDS in information retrieval?',
                    options: [
                        'Stop lists are used to improve search results by reducing unnecessary postings',
                        'Stop words include terms like "the" and "and", which are often removed during indexing',
                        "Stop words are always excluded from a vocabulary because they don't help with document retrieval",
                        'Phrase searches like "President of the United States" are more affected by stop words than keyword searches',
                        'Most common English words are usually considered stop words',
                    ],
                    correct: 3,
                    explanation:
                        "Stop words are common words like 'of' and 'the'. If an information retrieval system removes these words during indexing, it loses the positional information between the remaining words. This makes it impossible to verify an exact phrase match for a query containing those stop words, thus phrase searches are more severely affected than simple keyword searches.",
                },
                {
                    category: 'Text Processing & Normalization',
                    question:
                        'Why is it important to apply the same normalization rules to both indexed text and query terms?',
                    options: [
                        'To make stemming more effective',
                        'To ensure that relevant documents are retrieved regardless of minor variations in word form',
                        'To reduce the size of the inverted index',
                        'To eliminate the need for stop word removal',
                        'To speed up the tokenization process',
                    ],
                    correct: 1,
                    explanation:
                        "Normalization (which includes techniques like stemming and case-folding) is the process of converting text to a canonical form. For retrieval to work correctly, the same normalization rules must be applied to the document text during indexing and to the user's query at search time. This ensures that a query term like 'running' can match the indexed term 'run', because both are converted to the same canonical form.",
                },
                {
                    category: 'Text Processing & Normalization',
                    question: 'The Porter Stemmer is an example of:',
                    options: [
                        'A stop word list',
                        'A lemmatization algorithm',
                        'A stemming algorithm',
                        'A phrase index',
                        'A language identification algorithm',
                    ],
                    correct: 2,
                    explanation:
                        'The Porter Stemmer is a foundational and widely-used algorithm for stemming English words. It uses a set of heuristic rules to iteratively chop off common suffixes from words to reduce them to a common base form. It is a classic example of a stemming algorithm.',
                },
                {
                    category: 'Indexing & Data Structures',
                    question:
                        'What is the primary data structure used for vocabulary lookup in an inverted index?',
                    options: [
                        'Array',
                        'Binary Search Tree',
                        'Hash Table',
                        'Linked List',
                        'Dictionary',
                    ],
                    correct: 4,
                    explanation:
                        'An inverted index consists of two main components. The "dictionary" (also called vocabulary or lexicon) is the data structure that stores the list of all unique terms. For each term in the dictionary, there is a pointer to its corresponding "postings list", which contains the IDs of the documents where the term appears.',
                },
                {
                    category: 'Text Processing & Normalization',
                    question:
                        'What problem does language identification solve in Information Retrieval?',
                    options: [
                        'Translating documents',
                        'Determining the author of a document',
                        'Identifying the language of a document or part of a document',
                        'Finding documents about languages',
                        'Detecting spam documents',
                    ],
                    correct: 2,
                    explanation:
                        'This is the definition of language identification. Many subsequent processing steps, such as stemming and stop word removal, are language-specific. Therefore, it is crucial to first identify the language of a document (or even parts of it) to apply the correct set of linguistic tools.',
                },
                {
                    category: 'Indexing & Data Structures',
                    question:
                        'What is the purpose of the "Reduce" phase in the MapReduce framework, in the context of index construction?',
                    options: [
                        'To assign tasks to worker machines',
                        'To parse the documents and extract terms',
                        'To sort the (term, docID) pairs by term',
                        'To distribute the document collection across multiple machines',
                        'To merge the intermediate postings lists generated by the Map phase into final postings lists',
                    ],
                    correct: 4,
                    explanation:
                        'In the MapReduce paradigm for index construction, the "Map" phase emits (term, docID) pairs. The framework then sorts and groups these pairs by term. The "Reduce" function\'s task is to process one term at a time, taking its entire list of associated docIDs and merging them into a single, complete postings list for that term.',
                },
                {
                    category: 'Text Processing & Normalization',
                    question:
                        'Which of the following best describes the relationship between "terms" and "tokens" in Information Retrieval?',
                    options: [
                        'Tokens are instances of character sequences, while terms are normalized word types',
                        'Terms and tokens are synonymous',
                        'Tokens are used for indexing, while terms are used for query processing',
                        'Terms are raw, unprocessed words, while tokens are normalized words',
                        'Terms are always single words, while tokens can be phrases',
                    ],
                    correct: 0,
                    explanation:
                        "This explanation correctly defines the two concepts. Tokenization is the initial step of breaking text into a sequence of characters, called tokens (e.g., 'Running'). Normalization is the subsequent step of converting these tokens into a canonical form, called terms (e.g., 'run'), which are then added to the index's dictionary.",
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question:
                        'Which of the following is a disadvantage of the Boolean retrieval model?',
                    options: [
                        'It is difficult to implement',
                        'It is computationally expensive',
                        'It does not support phrase queries',
                        'It does not allow for ranked retrieval',
                        'It requires a large amount of storage space',
                    ],
                    correct: 3,
                    explanation:
                        "The classic Boolean retrieval model works on the principles of set theory. A document either matches the query's logical criteria (AND, OR, NOT) or it does not. This results in an unordered set of matching documents, with no mechanism to determine which documents are 'more' relevant than others.",
                },
                {
                    category: 'Text Processing & Normalization',
                    question:
                        'Which of the following strategies is suggested for handling words without diacritics in search queries?',
                    options: [
                        'Require users to input diacritics for accurate results',
                        'Use machine learning models exclusively for diacritic normalization',
                        'Ignore the presence of accents entirely, as they are irrelevant',
                        'Only consider case-folding without addressing diacritics',
                        'Equate words regardless of diacritics to improve search accuracy',
                    ],
                    correct: 4,
                    explanation:
                        "Users are often inconsistent when typing diacritics (e.g., accents in 'ÿ®Ÿêÿ≥ŸíŸÖŸê ÿßŸÑŸÑŸëŸéŸáŸê ÿßŸÑÿ±ŸëŸéÿ≠ŸíŸÖŸéŸ∞ŸÜŸê ÿßŸÑÿ±ŸëŸéÿ≠ŸêŸäŸÖŸê'). To ensure that a search for 'resume' finds documents containing 'ÿ®ÿ≥ŸÖ ÿßŸÑŸÑŸá ÿßŸÑÿ±ÿ≠ŸÖŸÜ ÿßŸÑÿ±ÿ≠ŸäŸÖ' and vice versa, a common normalization step is to remove diacritics from both the query and the indexed terms, thus treating them as equivalent. This improves recall and overall search accuracy.",
                },
                {
                    category: 'Indexing & Data Structures',
                    question:
                        'What is the primary reason for breaking an inverted index into tiers of decreasing importance?',
                    options: [
                        'To ensure that documents with higher field values are retrieved before others',
                        'To prevent reliance on a single point of failure in document retrieval',
                        'To ensure that all documents are equally represented across all tiers',
                        'To improve query performance by prioritizing more important documents first',
                        'To allow for easier manipulation and updating of the index',
                    ],
                    correct: 3,
                    explanation:
                        'A tiered index is an optimization strategy where the index is partitioned into tiers based on document importance (e.g., PageRank). By searching the highest-importance, smallest tier first, the system can quickly return high-quality results to the user, significantly improving query performance and perceived speed.',
                },
                {
                    category: 'Text Processing & Normalization',
                    question: 'What is the purpose of "lemmatization" in information retrieval?',
                    options: [
                        'To identify the language of a document',
                        'To reduce words to their dictionary form (lemma)',
                        'To expand the query with synonyms and related terms',
                        'To remove all punctuation from the text',
                        'To count the number of words in a document',
                    ],
                    correct: 1,
                    explanation:
                        "This is the definition of lemmatization. Unlike stemming, which uses heuristics to chop off suffixes, lemmatization uses morphological analysis and a dictionary to reduce words to their actual dictionary headword, known as the lemma. For example, the lemma for 'was' is 'be'.",
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question:
                        "How does a query parser expand a user's free text query if it is of rising interest?",
                    options: [
                        'By splitting the query into subqueries based on term frequency',
                        'By running the query as a phrase query',
                        'By checking how many documents contain both phrases',
                        'By using vector space models to retrieve relevant documents',
                        'None of the above',
                    ],
                    correct: 0,
                    explanation:
                        "When a user enters a free text query like 'rising interest rates', a common strategy for a ranked retrieval system is to parse it into multiple subqueries: a phrase query for the full string and individual term queries. The scores from these subqueries are then combined. Query optimization often involves processing the postings lists for the rarest terms first (those with the lowest document frequency) to reduce the size of intermediate results.",
                },
                {
                    category: 'Information Retrieval Evaluation',
                    question:
                        'Why is Mean Average Precision (MAP) considered a good evaluation measure?',
                    options: [
                        'It gives more weight to the first few relevant documents retrieved',
                        'It only considers the top-ranked documents',
                        'It is easy to calculate and interpret',
                        'It is not affected by the number of relevant documents in the collection',
                        'It provides a single-figure measure of quality across recall levels',
                    ],
                    correct: 4,
                    explanation:
                        'Mean Average Precision (MAP) is a popular evaluation metric because it summarizes the performance of a ranked retrieval system with a single number. It is sensitive to the entire ranked list, and it averages performance over many different queries. It rewards systems that not only retrieve relevant documents but also rank them highly, thus reflecting performance over different recall levels.',
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question:
                        'What is the purpose of "length normalization" in the vector space model?',
                    options: [
                        'To ensure that all documents have the same number of terms',
                        'D) To reduce the impact of document length on similarity scores',
                        'To give more weight to longer documents',
                        'E) To remove stop words from the document collection',
                        'To adjust term frequencies based on document length',
                    ],
                    correct: 1,
                    explanation:
                        'Longer documents naturally tend to have higher term frequencies and a greater variety of terms, which can cause them to have higher similarity scores to a query by chance. Length normalization, most commonly done as part of calculating cosine similarity, scales the document vectors so that they all have a unit length. This removes the length bias, allowing a short, focused document to be ranked as highly as a long one.',
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question:
                        'What is the primary purpose of using log-frequency weighting in information retrieval?',
                    options: [
                        'To increase the impact of rare terms',
                        'To completely ignore frequent terms',
                        'To reduce the impact of very frequent terms',
                        'To normalize document length',
                        'To give more weight to frequent terms',
                    ],
                    correct: 2,
                    explanation:
                        'The relevance of a term does not increase linearly with its frequency. The difference between one and two occurrences of a term is more significant than the difference between 10 and 11. Log-frequency weighting dampens the raw term frequency score, preventing terms that appear many times from having a disproportionately high impact on the final document score.',
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question:
                        'Why is Euclidean distance a poor choice for measuring document similarity in the vector space model?',
                    options: [
                        'It is not sensitive to differences in term frequency',
                        'It only works for binary vectors',
                        'It is highly affected by document length',
                        'It cannot handle high-dimensional data',
                        'It is computationally expensive to calculate',
                    ],
                    correct: 2,
                    explanation:
                        'Euclidean distance measures the straight-line distance between the tips of two vectors. This measure is highly sensitive to the length (magnitude) of the vectors. In text retrieval, vector length often corresponds to document length. Cosine similarity, which measures the angle between vectors, is insensitive to length and is thus preferred for comparing documents of varying lengths.',
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question:
                        "When two documents d and d' (where d' is d appended to itself) are length-normalized, what happens to their cosine similarity?",
                    options: [
                        'Their cosine similarity remains the same as before normalization',
                        'Their cosine similarity becomes 1',
                        'The cosine similarity decreases because one document is longer than the other',
                        'Both are now identical vectors, so their cosine similarity is 1',
                        'One of them becomes a unit vector but not the other',
                    ],
                    correct: 3,
                    explanation:
                        "The document d' is simply document d repeated. In the vector space model, its vector will point in the exact same direction as d's vector, but it will have a larger magnitude. Length normalization scales a vector to a length of 1 but preserves its direction. Since both vectors have the same direction, their normalized versions will be identical. The cosine similarity between two identical vectors is 1.",
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question: 'What is the main characteristic of the Bag of Words model?',
                    options: [
                        'It only considers the first 100 words of a document',
                        'It represents a document as an ordered list of words',
                        'It assigns different weights to different words based on their position',
                        'It ignores the ordering of words in a document',
                        'It considers the order of words in a document',
                    ],
                    correct: 3,
                    explanation:
                        "The Bag of Words model is a simplifying representation used in natural language processing and information retrieval. It represents a piece of text as an unordered collection (a multiset or 'bag') of its words, completely disregarding grammar and the original word order, but keeping track of frequency.",
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question:
                        'Which of the following statements about the Jaccard coefficient is FALSE?',
                    options: [
                        'It measures the overlap of two sets',
                        'It can be used to score the similarity between a query and a document',
                        'It always assigns a value between 0 and 1',
                        'It does not require the sets being compared to be the same size',
                        'It considers term frequency within a document',
                    ],
                    correct: 4,
                    explanation:
                        'The statement is false because the standard Jaccard coefficient operates on sets. It measures the size of the intersection divided by the size of their union. This only accounts for the presence or absence of a term, not how many times it occurs (its frequency). There are weighted versions of the Jaccard index, but the standard one does not consider term frequency.',
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question: 'How can a vector space index be used to answer Boolean queries?',
                    options: [
                        'By converting the Boolean query into a phrase query',
                        'By assigning a non-zero weight to a term in the document vector if the term occurs in the document',
                        'Vector space indices cannot be used for Boolean queries',
                        'By ignoring term weights and only considering term presence',
                        'By using a separate Boolean index',
                    ],
                    correct: 1,
                    explanation:
                        "While a vector space index is designed for ranked retrieval, it can be adapted to handle Boolean queries. If a term exists in a document, its corresponding component in the document's vector will have a non-zero weight. Therefore, a Boolean query like 'A AND B' can be processed by finding all document vectors where the components for both A and B are non-zero.",
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question:
                        'Which method is typically used in information retrieval to rank documents based on their similarity to a query?',
                    options: [
                        'Jaccard coefficient',
                        'Cosine of the angle between vectors',
                        'Term frequency (tf)',
                        'Euclidean distance',
                        'Bag of Words model',
                    ],
                    correct: 1,
                    explanation:
                        'In the vector space model, both documents and queries are represented as vectors in a high-dimensional space where each dimension corresponds to a term. The standard method to measure the similarity between the query vector and a document vector is to calculate the cosine of the angle between them. A smaller angle results in a higher cosine value, indicating greater similarity.',
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question: 'In the context of term frequency (tf), what does tft,d represent?',
                    options: [
                        'The number of times that term t occurs in document d',
                        'The total number of terms in document d',
                        'The average length of documents containing term t',
                        'The inverse document frequency of term t',
                        'The number of documents in the collection that contain term t',
                    ],
                    correct: 0,
                    explanation:
                        'This is the standard definition of term frequency, often denoted as `tf_{t,d}`. It is a simple count of how many times a specific term `t` appears within a given document `d`. It serves as a component in more complex weighting schemes like tf-idf.',
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question:
                        'What is a key challenge in designing an effective relevance ranking system?',
                    options: [
                        'Minimizing the storage space required for the index',
                        'Eliminating the need for user feedback',
                        'Balancing precision and recall to provide a good overall user experience',
                        'Ensuring that the system retrieves all documents in the collection',
                        'Maximizing the speed of the retrieval system, even at the expense of accuracy',
                    ],
                    correct: 2,
                    explanation:
                        'Precision (fraction of retrieved documents that are relevant) and recall (fraction of all relevant documents that are retrieved) are the two primary measures of retrieval effectiveness. They are often in opposition; improving one can hurt the other. A central challenge in IR system design is to tune the ranking algorithm to find an appropriate balance between them that satisfies the needs of the target users.',
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question:
                        'The Question asks about the impact of term frequency on weight in Information Retrieval. When a term is very frequent in a collection, what happens to its weight?',
                    options: [
                        'The weight is 1 + log10(df)',
                        "The weight decreases because it's less informative",
                        'The weight increases because it becomes more common',
                        'The weight is always the same for all terms',
                        'The weight depends on the base of the logarithm used',
                    ],
                    correct: 1,
                    explanation:
                        "This explanation refers to the concept of Inverse Document Frequency (IDF). Terms that are frequent across the entire collection (like 'the', 'is', 'a') are not good discriminators between documents. The IDF component of weighting schemes like tf-idf assigns a lower weight to these common terms, making them less influential in the final score because they are considered less informative.",
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question:
                        'Which of the following is a key property of the Jaccard coefficient?',
                    options: [
                        'It always assigns a number between 0 and 1',
                        'It gives higher weight to rare terms',
                        'It is not sensitive to document length',
                        'It ranges between -1 and 1',
                        'It considers term frequency',
                    ],
                    correct: 0,
                    explanation:
                        'The Jaccard coefficient is mathematically defined as the size of the intersection of two sets divided by the size of their union. Since the intersection is a subset of the union, and sizes are non-negative, the resulting value must lie in the range [0, 1]. A value of 0 means the sets have no elements in common, and a value of 1 means the sets are identical.',
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question: 'Which statement about efficient cosine ranking is correct?',
                    options: [
                        'Efficient cosine ranking allows us to avoid computing all N cosines and instead select the top K documents using methods like heaps or sampling',
                        'Term frequency (tf) is the primary computational bottleneck in scoring',
                        'Document frequency determines whether a term appears in a collection, affecting log-frequency weighting',
                        "To find the top K documents in a collection 'nearest' to a query, we must compute the cosine similarity for every document",
                        'Jaccard coefficient is used as a replacement for cosine similarity in certain applications',
                    ],
                    correct: 0,
                    explanation:
                        'For a large collection, computing the cosine similarity score between the query and every single document is too slow. Efficient ranking techniques (or inexact top-K retrieval) are used to find the K best-scoring documents without performing all N computations. Methods like using heaps, champion lists, or tiered indexes are employed to achieve this efficiency.',
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question: 'What is the purpose of log-frequency weighting?',
                    options: [
                        'To eliminate stop words',
                        'To reduce the impact of very frequent terms',
                        'To improve query processing speed',
                        'To normalize the length of documents',
                        'To increase the weight of rare terms',
                    ],
                    correct: 1,
                    explanation:
                        'The relevance score does not typically increase linearly with the raw term frequency. A term appearing 10 times is not 10 times as relevant as a term appearing once. Log-frequency weighting dampens the term frequency score, reducing the impact of very frequent terms and leading to a more balanced relevance calculation.',
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question:
                        'Which of the following is NOT a typical component of a ranked retrieval system?',
                    options: [
                        'Returning a list of documents in no particular order',
                        'Indexing the document collection',
                        'Formulating a query in a specific query language',
                        'Ranking documents based on relevance to the query',
                        'Using term frequencies to determine document relevance',
                    ],
                    correct: 0,
                    explanation:
                        'A core feature and goal of a ranked retrieval system is to order the retrieved documents based on their computed relevance to the query. Returning an unordered list is the behavior of a simple Boolean system, and is antithetical to the concept of ranked retrieval.',
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question:
                        'What is a limitation of using a vector space model for phrase queries?',
                    options: [
                        'The vector space model cannot assign weights to terms',
                        'The relative order of terms in a document is lost in the vector representation',
                        'The vector space model is too computationally expensive for phrase queries',
                        'The vector space model requires stemming, which reduces accuracy for phrase queries',
                        'The vector space model cannot handle multiple terms in a query',
                    ],
                    correct: 1,
                    explanation:
                        'The vector space model is based on the bag-of-words assumption, where a document is treated as an unordered collection of its terms. This means that all information about the original order and proximity of words is lost. Because phrase queries depend on the exact order of words, the basic vector space model cannot process them directly.',
                },
                {
                    category: 'Information Retrieval Evaluation',
                    question:
                        'In the context of information retrieval, what is the "recall" of a system?',
                    options: [
                        'The number of documents in the collection',
                        'The amount of storage space used by the index',
                        'The speed at which the system retrieves documents',
                        'The proportion of retrieved documents that are relevant',
                        'The proportion of relevant documents that are retrieved',
                    ],
                    correct: 4,
                    explanation:
                        'This is the standard definition of recall in information retrieval. It measures how effective a system is at finding all the relevant documents in a collection for a given query. It is calculated as the number of retrieved relevant documents divided by the total number of existing relevant documents.',
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question:
                        'Which of the following statements best describes the vector space model?',
                    options: [
                        'Documents are represented as sets of keywords',
                        'Documents and queries are represented as vectors in a high-dimensional space',
                        'Documents are categorized into predefined topics',
                        'Documents are ranked based on the number of shared words with the query',
                        'Documents are represented as binary vectors',
                    ],
                    correct: 1,
                    explanation:
                        'This is the fundamental concept of the vector space model. Documents and queries are transformed into numerical vectors. Each dimension of the vector space corresponds to a unique term from the vocabulary, allowing for the calculation of similarity (e.g., cosine similarity) between them.',
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question: 'Which of the following best describes Ranked Retrieval?',
                    options: [
                        'A model that returns an ordering of documents in a collection based on their relevance to a free-text query, without using Boolean operators',
                        'A concept where document relevance is determined by matching terms with index words from the collection vocabulary',
                        'A method where systems return document rankings based on query expressions using Boolean logic',
                        'A process that calculates the likelihood of documents being relevant based on their occurrence in the text',
                        'A technique involving term frequency analysis to determine document weights for scoring',
                    ],
                    correct: 0,
                    explanation:
                        'Ranked retrieval models are designed to overcome the rigidity of the Boolean model. They accept free-text queries (natural language keywords) and, instead of returning a simple match/no-match set, they compute a relevance score for documents and return an ordered list ranked by that score.',
                },
                {
                    category: 'Indexing & Data Structures',
                    question:
                        'Which of the following is the first step in basic crawler operation?',
                    options: [
                        'Beginning with known seed URLs',
                        'Securing the network',
                        'Designing the user interface',
                        'Analyzing website traffic',
                        'Creating a database schema',
                    ],
                    correct: 0,
                    explanation:
                        'A web crawler must have a starting point to begin exploring the web. This initial list of URLs, known as the seed set, is the first thing the crawler processes. It fetches these pages, extracts new links from them, and adds those links to its queue (URL frontier) to continue the crawling process.',
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question:
                        'When performing a basic search using Boolean operators, field codes, truncation (*), and wildcard (?) symbols, how are these symbols used to handle word boundaries in a phrase search?',
                    options: [
                        'Truncation (*) is used to limit the search to exact phrases by enclosing them in quotes',
                        'The asterisk (*) symbol is used to indicate the exclusion of certain terms during a search',
                        'Phrase searching requires using both truncation (*) and wildcard (?) symbols simultaneously',
                        'Wildcard (?) can be used to replace any character within a word when searching for partial matches',
                        'Truncation (*) and wildcard (?) symbols are used interchangeably without considering the need for exact phrases',
                    ],
                    correct: 3,
                    explanation:
                        "In query syntax, the wildcard symbol '?' is commonly used to stand in for a single character. This allows users to search for variations of a word, for instance, a query like 'organi?ation' would match both 'organisation' (British spelling) and 'organization' (American spelling). This is a form of partial matching.",
                },
                {
                    category: 'Foundational IR Concepts',
                    question:
                        "When a search term is entered without a field label in PubMed, what happens if automatic term mapping fails?",
                    options: [
                        'The search term is added to the query box',
                        'The search is canceled and redisplayed',
                        'The entire phrase is treated as a single search term',
                        'The search returns no results',
                        'The term is parsed into individual words for separate mapping',
                    ],
                    correct: 4,
                    explanation:
                        "PubMed uses a sophisticated process called Automatic Term Mapping (ATM). If ATM fails to map a multi-word phrase to a known concept (like a MeSH term), it doesn't give up. Instead, it breaks the phrase into its constituent parts and attempts to map them individually or search for them as separate keywords.",
                },
                {
                    category: 'Indexing & Data Structures',
                    question:
                        "What does Google's crawler do when encountering cached content from potentially malicious pages?",
                    options: [
                        'Alerts users about the possibility of malicious content',
                        'Does not distinguish between safe and unsafe cached content',
                        'Uses cached content to determine relevance',
                        "Deletes cached content if it's deemed unsafe",
                        'Automatically ignores cached content',
                    ],
                    correct: 4,
                    explanation:
                        "This answer is likely incorrect as stated. A more accurate statement is that Google's crawler would identify the page as malicious and would not index it or would heavily penalize it in rankings. It would avoid caching and serving content from a page it deems malicious. The idea of ignoring cached content is part of this broader avoidance strategy.",
                },
                {
                    category: 'Information Retrieval Evaluation',
                    question:
                        'You are comparing two search engines, A and B, using MAP. Engine A has a MAP of 0.8, while Engine B has a MAP of 0.6. However, for a specific query, Engine B returns a perfectly relevant result at rank 1, while Engine A returns its first relevant result at rank 3. Which engine is better for this specific query from a user perspective, and why?',
                    options: [
                        'Engine A, because it has a higher overall MAP score',
                        'Engine A, because MAP considers both precision and recall',
                        'Engine B, because it provides a perfectly relevant result at the top',
                        'It cannot be determined without knowing the total number of relevant documents',
                        'The two engines perform equally well for this query',
                    ],
                    correct: 2,
                    explanation:
                        "MAP (Mean Average Precision) is a measure of average performance across many queries. While Engine A is better on average, for a single specific query, the user's immediate experience is what matters. Engine B returned a perfectly relevant result at rank 1, which is the ideal user outcome for that search. Therefore, for that specific query, Engine B performed better.",
                },
                {
                    category: 'Foundational IR Concepts',
                    question:
                        "Which of the following features is included as part of PubMed's explicit politeness?",
                    options: [
                        'Query box entry for search terms',
                        'Inverted index preview of search results',
                        'History of previous searches',
                        'Limits by date, gender, age, language, or field label',
                        'Clipboard manipulation',
                    ],
                    correct: 3,
                    explanation:
                        "Explicit politeness means upfront controls to narrow your search, and PubMed's limit options (date, age, language, field, etc.) are exactly those user-tunable constraints. (PDF Answer: Query box entry for search terms)",
                },
                {
                    category: 'Foundational IR Concepts',
                    question: 'How does EBSCO Host manage its URL frontier?',
                    options: [
                        'It employs both adjacency searching and exact phrase matching for URL storage',
                        'It stores cached pages for faster access during repeated searches',
                        'It uses expanders to refine search results without requiring quotation marks',
                        'It dynamically generates URLs based on search terms and contexts',
                        'It segments URLs into hierarchical levels using a scheme similar to the Mercator system',
                    ],
                    correct: 4,
                    explanation:
                        'The Mercator system is a well-known, sophisticated architecture for a distributed web crawler. It provides a method for partitioning the entire URL space so that different crawling machines can work in parallel on different segments of the web without duplicating effort. This is essential for large-scale crawling operations.',
                },
                {
                    category: 'Information Retrieval Evaluation',
                    question:
                        'Which of the following scenarios would MRR be the MOST appropriate evaluation metric?',
                    options: [
                        'Evaluating a search engine designed to retrieve all relevant research papers for a given topic',
                        'Evaluating a search engine designed to provide a diverse set of results for a broad query',
                        'Evaluating a search engine designed to rank products based on customer reviews',
                        'Evaluating a search engine designed to find the homepage of a specific company, given its name',
                        'Evaluating a search engine designed to provide a comprehensive overview of a historical event',
                    ],
                    correct: 3,
                    explanation:
                        "Mean Reciprocal Rank (MRR) is an evaluation metric designed specifically for tasks where a user is looking for a single correct answer and its rank is of primary importance. Navigational queries, like finding a company's homepage, are a classic example. MRR measures, on average, how close to the top of the results the first correct answer is.",
                },
                {
                    category: 'Indexing & Data Structures',
                    question: 'What is the primary function of a web crawler?',
                    options: [
                        'To create social media posts',
                        'To systematically discover, retrieve, and index web content',
                        'To manage network infrastructure',
                        'To display web pages to users',
                        'To design website layouts',
                    ],
                    correct: 1,
                    explanation:
                        "This is the definition of a web crawler's function. Its job is to navigate the web in a methodical and automated way, starting from a set of seed URLs, to discover new pages, retrieve their content, and pass that content to an indexer so it can be made searchable.",
                },
                {
                    category: 'Foundational IR Concepts',
                    question:
                        'Does EBSCO Host save search results for users by default, or does it require a user to sign in to retain saved searches?',
                    options: [
                        'Searches are stored permanently once a user signs up',
                        'It uses quotation marks for exact phrase searches without needing limiters',
                        "It saves all searches in the user's session unless they log out",
                        'It does not save searches automatically and requires a login for any saved results',
                        'It allows users to export or share search results via email',
                    ],
                    correct: 2,
                    explanation:
                        'Many web applications, including search databases like EBSCO Host, use sessions to maintain temporary user data, such as the history of searches performed. This history is typically available for the duration of the session but is cleared once the user logs out or closes the browser, unless they have an account and explicitly save the searches.',
                },
                {
                    category: 'Foundational IR Concepts',
                    question:
                        'Which three refinement options are available through the tab on the results screen?',
                    options: [
                        'Date ranges, subject categories, and appearance in forums',
                        'Date ranges, peer-reviewed journals, and full-text availability',
                        'Date ranges, appearance in blogs, and availability in books',
                        'Date ranges, citation indexes, and availability in PDFs',
                        'Date ranges, appearance in news articles, and availability in conference papers',
                    ],
                    correct: 1,
                    explanation:
                        'These three options are standard and critical filtering tools (also known as facets) in academic search engines. Researchers and students frequently need to refine their search results by the date of publication, limit the results to only scholarly (peer-reviewed) sources, and filter for articles where the full text is immediately accessible.',
                },
                {
                    category: 'Indexing & Data Structures',
                    question: 'What does it mean for a web crawler to be "extensible"?',
                    options: [
                        'It can only crawl a fixed number of web pages',
                        'It can adapt to new data formats and protocols',
                        'It can only crawl websites with specific domain extensions',
                        'It can automatically generate website content',
                        'It can operate without any human intervention',
                    ],
                    correct: 1,
                    explanation:
                        'In software engineering, extensibility refers to a design principle where a system can be easily enhanced with new functionality. For a web crawler, which must operate on the ever-changing internet, this means being able to easily add modules to handle new network protocols (beyond HTTP/S), new document formats (beyond HTML), and new ways of discovering links.',
                },
                {
                    category: 'Foundational IR Concepts',
                    question:
                        "When performing an advanced search on EBSCO's platform, which of the following options allows users to refine their search by limiting results based on specific criteria such as date ranges or full-text availability?",
                    options: [
                        'Advanced search with field labels and Boolean operators',
                        'Truncation and wildcards without Boolean operators',
                        'Field codes only',
                        'A dedicated refinement tab that includes date ranges and peer-reviewed journals',
                        'Automatic synonym inclusion without user intervention',
                    ],
                    correct: 3,
                    explanation:
                        "This is a common user interface design pattern in advanced search systems. To help users manage large sets of results, a dedicated pane or tab provides a variety of filters or 'facets' such as filtering by date, source type (like peer-reviewed journals), subject, or author that can be applied to narrow down the results.",
                },
                {
                    category: 'Information Retrieval Evaluation',
                    question:
                        'Consider two search engines. Engine X has a higher MAP score than Engine Y on a given test collection. What can we generally infer from this?',
                    options: [
                        'Engine X is guaranteed to return more relevant documents than Engine Y for every query',
                        'Engine X is likely to provide better overall ranking performance than Engine Y across a range of queries',
                        'Engine X is faster than Engine Y',
                        'Engine X is always better than Engine Y for all possible queries',
                        'Engine X is more user-friendly than Engine Y',
                    ],
                    correct: 1,
                    explanation:
                        'Mean Average Precision (MAP) is a single-value metric that evaluates the quality of a ranked list across multiple queries. A higher MAP score indicates better performance on average. It does not guarantee better performance on every single query, but it is a strong indicator of the overall quality of the ranking algorithm.',
                },
                {
                    category: 'Foundational IR Concepts',
                    question:
                        'When a search term is entered into PubMed without specifying a field label, how does the system handle the search?',
                    options: [
                        'Looks only for MeSH (Medical Subject Headings) terms',
                        'Does not consider any field labels or controlled vocabularies',
                        'Searches for matches in the MeSH table, then explodes more specific subheadings if available',
                        'Searches first in the Journals table, then in the Phrase List',
                        'Searches all fields and uses AND operations after failing to translate the term',
                    ],
                    correct: 2,
                    explanation:
                        "This describes a key part of PubMed's Automatic Term Mapping (ATM) functionality. When a user's query matches a Medical Subject Heading (MeSH), the system can be set to 'explode' that term, which means it automatically includes all the more specific terms that fall under that heading in the MeSH hierarchy, thus broadening the search to be more comprehensive.",
                },
                {
                    category: 'Information Retrieval Evaluation',
                    question:
                        'Which of the following metrics is used to evaluate unranked retrieval?',
                    options: [
                        'Click-Through Rate (CTR)',
                        'Precision@K (P@K)',
                        'Mean Average Precision (MAP)',
                        'Mean Reciprocal Rank (MRR)',
                        'Precision',
                    ],
                    correct: 4,
                    explanation:
                        'Unranked retrieval, such as from a simple Boolean model, returns a set of documents where every item is considered equally matching. There is no order. Metrics that rely on rank (like MAP, MRR, P@K) are therefore not applicable. Precision, the fraction of retrieved documents that are relevant, can be calculated on the entire set and is a primary metric for this type of retrieval.',
                },
                {
                    category: 'Information Retrieval Evaluation',
                    question:
                        'What is a key characteristic of relevance in the context of search engine evaluation?',
                    options: [
                        'Relevance is solely determined by the number of keywords in a document that match the query',
                        'Relevance is only important for academic search engines, not for commercial ones',
                        'Relevance is subjective and can vary from person to person',
                        'Relevance is static and does not change over time',
                        'Relevance is purely objective and can be determined by algorithms alone',
                    ],
                    correct: 2,
                    explanation:
                        'This is a fundamental truth in information retrieval. What a user considers relevant to their query depends on their specific need, intent, and background knowledge. Two different people can issue the same query but have different ideas about what constitutes a good answer. This makes evaluating IR systems challenging and is why evaluation relies on relevance judgments collected from human assessors.',
                },
                {
                    category: 'Foundational IR Concepts',
                    question:
                        'what PubMed does after a search term is entered without a field labe?',
                    options: [
                        'Explodes the term using MeSH hierarchy',
                        'Searches only in the Author Index',
                        'Uses an automatic mapping feature. From the context, PubMed uses an automatic term mapping feature that includes exploding the term via MeSH hierarchy if applicable. Therefore, the Correct Answer is:',
                        'Displays results in batches of 20',
                        'Translates the term into other languages',
                    ],
                    correct: 0,
                    explanation:
                        "This is a restatement of the concept from Q250. 'Exploding' a term is a feature of PubMed's search that leverages the hierarchical structure of Medical Subject Headings (MeSH). When a search term is mapped to a MeSH term, exploding it automatically includes all narrower, more specific concepts in the search, leading to a more comprehensive recall.",
                },
                {
                    category: 'Indexing & Data Structures',
                    question:
                        'A crawler is designed to fetch pages of higher quality first. Which of the following metrics would be most effective in estimating page quality before fetching?',
                    options: [
                        'The number of images on the page',
                        'The number of inbound links to the page from known reputable sites',
                        'The number of outbound links on the page',
                        "The length of the page's HTML code",
                        "The page's server response time",
                    ],
                    correct: 1,
                    explanation:
                        "To prioritize which pages to crawl from a massive queue (the URL frontier), a crawler needs to estimate a page's quality before downloading it. Since the page's content is unknown, it must rely on link-based metrics. The number of inbound links, especially from pages that are already known to be high-quality or reputable, is a very strong signal of importance, as captured by algorithms like PageRank.",
                },
                {
                    category: 'Indexing & Data Structures',
                    question:
                        'A web crawler is tasked with continuously updating its index. Which of the following strategies would best balance freshness and resource usage?',
                    options: [
                        'Never recrawling pages to conserve resources',
                        'Only recrawling pages that have been manually flagged for updates',
                        'Prioritizing recrawling based on the estimated change frequency of each page',
                        'Recrawling all pages every 24 hours',
                        'Randomly recrawling a small subset of pages each day',
                    ],
                    correct: 2,
                    explanation:
                        'A web crawler has finite resources and cannot recrawl every page constantly. A smart, efficient strategy is to estimate how often each page is likely to change. Important pages that are updated frequently (e.g., a news homepage) are visited more often than static pages (e.g., an archived article). This policy balances the need for index freshness with the efficient use of crawling resources.',
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question:
                        'In information retrieval, when formulating a query that involves multiple terms, what is an effective way to reduce the number of documents considered during retrieval?',
                    options: [
                        'Include only documents that contain many query terms',
                        'Include all documents containing at least one query term',
                        'Only include documents containing all query terms',
                        'Include only high-idf query terms',
                        'Exclude documents based on collection frequency rather than document frequency',
                    ],
                    correct: 0,
                    explanation:
                        "When a user provides multiple query terms, it's highly probable that they are interested in documents that contain all or most of those terms. A core strategy in both Boolean and ranked retrieval is to prioritize documents that match more of the query terms. For an AND query, only documents containing all terms are considered, which is a very effective way to reduce the candidate set of results.",
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question:
                        'Consider a query "natural language processing". Document A has a high term frequency for "language" but a low term frequency for "natural" and "processing". Document B has moderate term frequencies for all three terms. Assuming idf values are similar for all three terms, which document is likely to be ranked higher by tf-idf and why?',
                    options: [
                        'Document B, because it has moderate term frequencies for all the query terms',
                        'Document A, because tf-idf prioritizes documents with the highest overall term frequency, regardless of term distribution',
                        'Document B, because tf-idf always favors documents with shorter lengths',
                        'Document A, because it has a high term frequency for at least one of the terms',
                        'Neither, because the idf values are similar, so the ranking will be arbitrary',
                    ],
                    correct: 0,
                    explanation:
                        "A document's total tf-idf score for a multi-term query is the sum of the individual tf-idf scores for each term. Document A has a very high score for one term but low scores for the other two. Document B has moderate scores for all three terms. The sum of three moderate scores is very likely to be greater than the sum of one high score and two low scores. Document B matches the overall query better.",
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question:
                        'Consider two documents, D1 and D2. D1 contains the term "information" 5 times and D2 contains it 10 times. The document frequency of "information" in the collection is 100 (meaning 100 documents contain the term). Assuming a collection size of 10,000 documents, which document would receive a higher tf-idf score for the term "information" before length normalization?',
                    options: [
                        'Both would have the same score because tf-idf balances term frequency and document frequency',
                        'D2, because it has a higher term frequency',
                        'Neither, because "information" is likely a stop word',
                        'It is impossible to determine without knowing the average document length',
                        'D1, because it is shorter',
                    ],
                    correct: 1,
                    explanation:
                        "The tf-idf score is calculated as a function of term frequency (tf) and inverse document frequency (idf). The question specifies that we are comparing the score for the same term ('information'), so its idf is constant for both documents. The only variable is the term frequency. Since Document 2 has a higher term frequency (10) than Document 1 (5), it will have a higher tf-idf score before any other factors like length normalization are applied.",
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question:
                        'Which retrieval model assigns higher weights to terms based on their rarity and document frequency?',
                    options: [
                        'Inverted Index',
                        'Vector Space Model',
                        'Term-Document Incidence Matrix',
                        'Boolean Retrieval',
                        'Probabilistic Retrieval',
                    ],
                    correct: 4,
                    explanation:
                        'Probabilistic retrieval models, like the classic model and its successor BM25, are explicitly designed to rank documents based on the estimated probability of their relevance to a query. The formulas used in these models incorporate factors like term frequency within a document and the rarity of the term across the collection (inverse document frequency) to compute this probability.',
                },
                {
                    category: 'Text Processing & Normalization',
                    question:
                        'Which of the following is a potential issue when using stemming in information retrieval?',
                    options: [
                        'It can increase the size of the index',
                        'It can lead to over-stemming',
                        'It requires a large amount of computational resources',
                        'It decreases the recall of the system',
                        'It can only be used with the Boolean retrieval model',
                    ],
                    correct: 1,
                    explanation:
                        "Stemming uses heuristic rules to remove suffixes, which can sometimes lead to errors. Over-stemming is when two words with different meanings are incorrectly reduced to the same stem (e.g., 'universe' and 'university' both becoming 'univers'). This causes a query for one to retrieve documents about the other, which harms precision.",
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question:
                        'What is the primary goal of ranked retrieval models in Information Retrieval?',
                    options: [
                        'To return a random selection of documents from the collection',
                        'To return an ordered list of documents based on their relevance to the query',
                        'To return only the documents that exactly match the query',
                        'To return all documents that contain the query terms',
                        'To exclude documents that contain stop words',
                    ],
                    correct: 1,
                    explanation:
                        'This is the primary function and definition of ranked retrieval. In contrast to Boolean models that return an unordered set of documents that strictly match a query, ranked retrieval models compute a score for many documents based on how relevant they are estimated to be, and then return them as an ordered list.',
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question:
                        'Which of the following is a potential problem with using only term frequency (TF) to rank documents?',
                    options: [
                        'It favors longer documents over shorter ones',
                        'It gives too much weight to rare terms',
                        'It is computationally expensive to calculate',
                        'It cannot be used with stemming',
                        'It gives too little weight to common terms',
                    ],
                    correct: 0,
                    explanation:
                        'If ranking is based only on raw term frequency (the count of a query term), longer documents will have a natural advantage. They simply have more words, and thus more opportunities for the query term to appear multiple times, even if the document as a whole is not very focused on the topic. This is why length normalization is a crucial part of modern ranking schemes.',
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question:
                        'Which of the following statements correctly describes the term weighting process used in information retrieval?',
                    options: [
                        'Inverted index structures are used to store term positions, not term weights',
                        'Term weights increase with both the frequency of the term in a document and its rarity in the collection',
                        'The weight for a term in a document is the product of its frequency and inverse document frequency',
                        'Term weights are calculated using a simple addition model: w_t,d = 1 + log10(tf_t,d)',
                        'Term weights are determined by the number of occurrences within a document, without considering document frequency',
                    ],
                    correct: 2,
                    explanation:
                        "This describes the tf-idf weighting scheme, a fundamental concept in information retrieval. A term's weight is determined by two factors: its term frequency (tf), how often it appears in the specific document, and its inverse document frequency (idf), a measure of its rarity across the entire collection. The product of these two gives a weight that favors terms that are important to a document but not common everywhere.",
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question: 'How does idf affect the ranking of documents for queries?',
                    options: [
                        'It only affects the ranking of documents for one-term queries',
                        'It always boosts the score of documents containing rare terms',
                        'It always lowers the score of documents containing frequent terms',
                        'It has no effect on ranking',
                        'It affects the ranking of documents for queries with at least two terms',
                    ],
                    correct: 4,
                    explanation:
                        "Inverse Document Frequency (IDF) gives a higher weight to rarer terms. In a single-term query, all documents are scored based on the same term, so they are all scaled by the same IDF value, which doesn't change their relative order. In a query with at least two terms, the terms will have different IDF values, so their relative contributions to a document's total score will differ, thus affecting the final ranking.",
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question:
                        'In the context of search engine design, what is evidence accumulation?',
                    options: [
                        'The process of normalizing term frequencies',
                        "The process of combining evidence of a document's relevance from multiple sources",
                        'The process of stemming query terms',
                        'The process of indexing documents based on their publication date',
                        'The process of eliminating irrelevant documents from the search results',
                    ],
                    correct: 1,
                    explanation:
                        'Modern search engines do not rely on a single piece of evidence to rank documents. They use a process of evidence accumulation, where a final relevance score is computed by combining scores from many different sources. These sources can include text-based scores (like tf-idf), link-based scores (like PageRank), user behavior data, and many other signals.',
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question: 'Which of the following best describes the Jaccard coefficient?',
                    options: [
                        'A measure of the similarity between two sets',
                        'A measure of term frequency in a document',
                        'A measure of the dissimilarity between two documents',
                        'A measure of the number of shared terms between two documents',
                        'A measure of the angle between two document vectors',
                    ],
                    correct: 0,
                    explanation:
                        'This is the mathematical definition of the Jaccard coefficient. It is a statistic used for gauging the similarity and diversity of sample sets. It is calculated by dividing the size of the intersection of the sets by the size of the union of the sets.',
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question:
                        'Which of the following is a component of the Jaccard coefficient used for measuring the similarity between two sets?',
                    options: [
                        'Number of common elements',
                        'Difference between the sets',
                        'Intersection of the sets',
                        'Union of the sets',
                        'Size of the individual sets',
                    ],
                    correct: 2,
                    explanation:
                        'The formula for the Jaccard coefficient between two sets, A and B, is J(A,B) = |A \u2229 B| / |A \u222a B|. The numerator is the size of the intersection of the sets (the number of elements they have in common). The denominator is the size of the union. Therefore, the intersection is a required component.',
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question:
                        'Which of the following is a primary characteristic of the "bag of words" model in information retrieval?',
                    options: [
                        'It represents documents as ordered sequences of words',
                        'It ignores the order of words, treating documents as unordered collections of terms',
                        'It requires a positional index to function correctly',
                        'It assigns different weights to different word positions in a document',
                        'It considers the order of words in a document',
                    ],
                    correct: 1,
                    explanation:
                        'This is the defining characteristic of the bag-of-words model. It is a simplifying representation that treats a document as an unordered collection (a multiset) of its words. It disregards grammar and word order but keeps information about term frequency.',
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question:
                        'In the bag of words model, what aspect of a document is disregarded?',
                    options: [
                        'Term order',
                        'Rare words',
                        'Term frequency',
                        'Document length',
                        'Stop words',
                    ],
                    correct: 0,
                    explanation:
                        'As per the definition of the bag-of-words model, the key piece of information that is discarded is the sequence in which the terms originally appeared in the document. This means relationships based on proximity and order are lost.',
                },
                {
                    category: 'Indexing & Data Structures',
                    question:
                        'You are designing a search engine for a very large, dynamic collection of documents (like the web) where the vocabulary is constantly growing. You need to balance search speed, storage space, and the ability to handle both wildcard queries and spelling correction. Which combination of data structures and techniques would offer the BEST overall compromise?',
                    options: [
                        'B-tree for the dictionary, k-gram index for wildcards, edit distance for spelling correction, and a mechanism for periodically rebuilding the k-gram index',
                        'Hash table for the dictionary, permuterm index for wildcards, and a combination of edit distance and phonetic matching for spelling correction',
                        'Hash table for the dictionary, k-gram index for wildcards, and Soundex for spelling correction',
                        'B-tree for the dictionary, permuterm index for wildcards, and edit distance for spelling correction',
                        'Trie for the dictionary, a combination of k-gram and permuterm indexes for wildcards, and a probabilistic spelling correction model',
                    ],
                    correct: 0,
                    explanation:
                        'This combination represents a comprehensive and practical design for a large-scale search engine. A B-tree is well-suited for a large, dynamic dictionary. A k-gram index is the standard method for handling wildcards efficiently. Edit distance is a fundamental component of most spelling correction systems. Finally, for a dynamic vocabulary, a mechanism to periodically update the k-gram index is a practical necessity.',
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question:
                        'Which of the following is NOT a characteristic of ranked retrieval models?',
                    options: [
                        'Documents are ordered based on their relevance to the query',
                        'Ranked retrieval is often associated with free text queries',
                        'Free text queries are typically used',
                        'Users are required to use a specific query language with operators and expressions',
                        'The system returns the top documents in the collection for a query',
                    ],
                    correct: 3,
                    explanation:
                        'Ranked retrieval models are specifically designed to handle free text queries, where a user can type in natural language keywords without needing to know a special syntax. The requirement to use a specific query language with operators like AND/OR is a characteristic of Boolean retrieval, not ranked retrieval.',
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question:
                        'Which of the following is a characteristic of a "ranked" retrieval model compared to a "Boolean" retrieval model?',
                    options: [
                        'Boolean retrieval allows for partial matches',
                        'Boolean retrieval is more efficient for large document collections',
                        'Ranked retrieval only works with structured data',
                        'Ranked retrieval assigns a relevance score to each document',
                        'Ranked retrieval returns only exact matches to the query',
                    ],
                    correct: 3,
                    explanation:
                        "This is the fundamental difference between the two models. A Boolean model gives a binary result: a document either matches the query logic or it doesn't. A ranked retrieval model goes further by calculating a numerical score for each document to estimate how well it matches the query's intent, allowing the results to be ordered by relevance.",
                },
                {
                    category: 'Indexing & Data Structures',
                    question: 'What is the purpose of a biword index?',
                    options: [
                        'To provide a visual representation of word relationships',
                        'To efficiently process phrase queries consisting of two words',
                        'To index documents written in two languages',
                        'To store biographical information about authors',
                        'To index documents containing only two words',
                    ],
                    correct: 1,
                    explanation:
                        "A biword index is created by taking every adjacent pair of words in the text and treating each pair as a single vocabulary term. This allows for very fast processing of two-word phrase queries, as the phrase can be looked up directly in the index's dictionary.",
                },
                {
                    category: 'Text Processing & Normalization',
                    question:
                        'Which of the following is a consideration when choosing a document unit for indexing?',
                    options: [
                        'The number of images in the document',
                        'The author of the document',
                        'The granularity of indexing (e.g., chapter, paragraph, sentence)',
                        'The font size used in the document',
                        'The color scheme of the document',
                    ],
                    correct: 2,
                    explanation:
                        'The document unit, or granularity, is a key decision. If the unit is too large (e.g., a whole book), a brief mention of a query term can cause the entire large document to be returned, leading to low precision. If the unit is too small (e.g., a sentence), the context of the term might be lost. The granularity of indexing (document vs. chapter vs. paragraph) is a critical consideration that balances these factors.',
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question:
                        'In the context of information retrieval, what does "relevance ranking" refer to?',
                    options: [
                        'Presenting documents in the order they were added to the collection',
                        'Ordering documents based on their publication date',
                        'Assigning random scores to documents',
                        'Ordering documents alphabetically by title',
                        "Ordering documents based on their estimated relevance to a user's query",
                    ],
                    correct: 4,
                    explanation:
                        "This is the definition of the term. Relevance ranking is the core process in ranked information retrieval systems, where documents are sorted based on a computed score that estimates how relevant they are to the user's information need as expressed by the query.",
                },
                {
                    category: 'Foundational IR Concepts',
                    question:
                        "Which statement best describes Calvin Mooers' view on the use of Boolean algebra in information retrieval system design?",
                    options: [
                        'Calvin Mooers strongly disagreed with the use of Boolean algebra for retrieval systems',
                        'Calvin Mooers focused on improving indexing structures rather than formalisms like Boolean algebra',
                        'Calvin Mooers advocated for the use of other formalisms instead of Boolean algebra',
                        'Calvin Mooers believed Boolean algebra was the most appropriate formalism for retrieval system design',
                        'Calvin Mooers supported the use of Boolean algebra due to its efficiency in retrieval systems',
                    ],
                    correct: 0,
                    explanation:
                        'This is a historical fact from the field of information retrieval. Calvin Mooers, a pioneer in IR, was an early critic of applying strict Boolean algebra to retrieval. He argued that it was too rigid and did not match the often nuanced and imprecise nature of human information needs. This viewpoint heavily influenced the development of later ranked retrieval models.',
                },
                {
                    category: 'Text Processing & Normalization',
                    question:
                        'How does Japanese typically handle inflectional endings and function words?',
                    options: ['Hiragana', 'Arabic numerals', 'Kanji', 'Katakana', 'Latin letters'],
                    correct: 0,
                    explanation:
                        'The Japanese writing system uses a combination of scripts. Kanji (logographic characters from Chinese) are typically used for word stems (nouns, verbs, adjectives). Hiragana is a syllabic script used for grammatical elements, such as inflectional endings, function words (particles), and words that do not have a common Kanji representation.',
                },
                {
                    category: 'Indexing & Data Structures',
                    question:
                        'In a standard inverted index, what does each entry in the postings list represent?',
                    options: [
                        'A rotated version of a term',
                        'The frequency of a term in the document',
                        'A document containing a specific term',
                        'A term in the vocabulary',
                        'The phonetic representation of the term',
                    ],
                    correct: 2,
                    explanation:
                        'An inverted index maps terms to the documents that contain them. The list associated with a term is called its postings list. Each entry in this list, called a posting, corresponds to a document where the term appears. At a minimum, each posting contains a document ID.',
                },
                {
                    category: 'Indexing & Data Structures',
                    question:
                        'When trying to efficiently retrieve all vocabulary terms that begin with a specific prefix (e.g., "automat"), why is a binary search tree considered superior to hashing for this task?',
                    options: [
                        'Hashing allows quick lookups but cannot easily handle prefix-based searches',
                        'Hashing is better suited for dynamic updates where keys can be added or deleted frequently',
                        'Hashing uses an inverted index, which inherently supports prefix searches',
                        'Search trees permit efficient retrieval of terms based on partial information without collisions',
                        'Binary search trees require more memory compared to hashing structures',
                    ],
                    correct: 0,
                    explanation:
                        'A hash table provides very fast (average constant time) lookups for an exact key, but it does not store keys in any particular order. To find all keys with a given prefix, one would have to scan the entire hash table. A data structure that maintains sorted order, like a binary search tree (or more practically, a B-tree or trie), can efficiently find the start of the prefix range and then traverse the structure to retrieve all matching terms.',
                },
                {
                    category: 'Foundational IR Concepts',
                    question: 'In the context of information retrieval, what is a "term"?',
                    options: [
                        'A normalized token that is included in the IR system\u2019s dictionary',
                        'Any word in a document',
                        'A document that is relevant to a query',
                        "A statistical measure of a word's importance",
                        'A graphical representation of word relationships',
                    ],
                    correct: 0,
                    explanation:
                        "This correctly describes the vocabulary used in IR. Text is first broken into 'tokens' (raw character sequences). These tokens are then 'normalized' (e.g., case-folded, stemmed) into a canonical form. This normalized form, which is what gets entered into the index's dictionary, is called a 'term'.",
                },
                {
                    category: 'Text Processing & Normalization',
                    question:
                        'When using a k-gram index for spelling correction, why is a post- filtering step often necessary after retrieving potential matches using a Boolean query on the k-grams?',
                    options: [
                        'To ensure that the matched terms have the correct phonetic representation',
                        'To handle wildcard characters within the query term',
                        'To remove terms that do not contain all the k-grams of the query',
                        'To calculate the edit distance between the query and the potential matches',
                        'To remove terms which, although contain the required k-grams, do not match the original query',
                    ],
                    correct: 4,
                    explanation:
                        'Using a k-gram index is an efficient way to generate a list of candidate corrections for a misspelled word by finding dictionary terms that share many k-grams. However, this initial list can contain many false positives. The post-filtering step is necessary to refine this list by removing terms that, despite sharing some k-grams, are not actually good matches for the original query, often by using a more precise measure like edit distance.',
                },
                {
                    category: 'Indexing & Data Structures',
                    question:
                        'An IR system is experiencing performance bottlenecks during query processing due to very long postings lists for common terms. Which of the following strategies would be MOST effective in addressing this issue, assuming a relatively static index?',
                    options: [
                        'Increasing the aggressiveness of stemming',
                        'Switching from an inverted index to a term-document matrix',
                        'Removing more stop words from the index',
                        'Using a larger vocabulary size',
                        'Implementing skip pointers in the postings lists',
                    ],
                    correct: 4,
                    explanation:
                        "Common terms like 'and' or 'the' have extremely long postings lists. When processing an AND query, the system must intersect these lists. This can be very slow. Skip pointers are an optimization that allows the intersection algorithm to 'skip' over large chunks of a postings list, dramatically reducing the number of comparisons needed and speeding up query processing.",
                },
                {
                    category: 'Indexing & Data Structures',
                    question:
                        'What is the main disadvantage of using a permuterm index for wildcard queries?',
                    options: [
                        'It results in a significant increase in dictionary size',
                        'It is slower than using a B-tree',
                        'It cannot handle leading wildcard queries',
                        'It requires a phonetic hashing algorithm',
                        'It cannot handle queries with multiple wildcards',
                    ],
                    correct: 0,
                    explanation:
                        "A permuterm index works by storing every possible rotation of a term in the dictionary (e.g., for 'cat', it stores 'cat$', 'at$c', 't$ca'). While this cleverly turns any wildcard query into a simple prefix query, it comes at a very high storage cost, as the dictionary size is multiplied by the average word length.",
                },
                {
                    category: 'Text Processing & Normalization',
                    question:
                        'Which of the following pairs of words would you argue should NOT be conflated by the Porter stemmer, and why?',
                    options: [
                        'absorbency/absorbent',
                        'abandon/abandonment',
                        'marketing/markets',
                        'volume/volumes',
                        'university/universe',
                    ],
                    correct: 3,
                    explanation:
                        "Assuming it's asking for an example of a word pair that should not be conflated, 'university/universe' is the classic example of over-stemming. They share a stem but have very different meanings. The provided answer, 'volume/volumes', is an example of a correct stemming action (conflating singular and plural).",
                },
                {
                    category: 'Indexing & Data Structures',
                    question: 'Why is performance/efficiency important for a web crawler?',
                    options: [
                        'To ensure that all crawled pages are visually appealing',
                        'To prevent users from accessing crawled data',
                        'To reduce the number of web pages crawled',
                        'To minimize the amount of data stored',
                        'To permit full use of available processing and network resources',
                    ],
                    correct: 4,
                    explanation:
                        "A web crawler's task is immense. To crawl a significant portion of the web, it must be highly efficient. This means it needs to be optimized to fully utilize its allocated network bandwidth and processing power to discover and download pages as quickly as possible, while also being polite to servers.",
                },
                {
                    category: 'Information Retrieval Evaluation',
                    question:
                        'System A returns the following ranked list for a query (R = Relevant, N = Non-relevant): R, N, N, R, N, R. System B returns: N, R, N, R, N, R. Which system has a higher Precision@3?',
                    options: [
                        'Precision@3 is not a valid metric in this scenario',
                        'System A',
                        'Both systems have the same Precision@3',
                        'It cannot be determined without knowing the total number of relevant documents',
                        'System B',
                    ],
                    correct: 2,
                    explanation:
                        'Precision@3 is the fraction of the top 3 results that are relevant. For System A (R, N, N), there is 1 relevant result, so P@3 is 1/3. For System B (N, R, N), there is also 1 relevant result, so P@3 is also 1/3. Their Precision@3 scores are identical.',
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question:
                        'What is the difference between collection frequency (cf) and document frequency (df)?',
                    options: [
                        'cf is the total number of times a term appears in the collection, while df is the number of documents in which the term appears',
                        'cf is the number of query terms, while df is the number of documents retrieved',
                        'cf is the number of times a term appears in a document, while df is the number of documents in which the term appears',
                        'cf is the average term frequency, while df is the median term frequency',
                        'cf is the number of characters in the collection, while df is the number of documents in the collection',
                    ],
                    correct: 0,
                    explanation:
                        'These two terms are fundamental statistics about term distribution. Document frequency (df) is the count of how many documents in the collection contain a specific term. Collection frequency (cf) is the total number of times that term occurs across all documents in the entire collection. Cf will always be greater than or equal to df.',
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question: 'In the bag of words model, what is ignored?',
                    options: [
                        'The relevance of a term to the query',
                        'The total number of terms in the document',
                        'The presence or absence of a term in a document',
                        'The number of times a term appears in a document',
                        'The order of terms in a document',
                    ],
                    correct: 4,
                    explanation:
                        "The 'bag of words' model is a fundamental concept in IR that simplifies text representation. It treats a document as an unordered collection of its words, thus ignoring the original sequence, grammar, and syntax. This allows for simple vector representation but loses the ability to handle phrases or proximity.",
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question: 'In a vector space model, what does the "cosine similarity" measure?',
                    options: [
                        'The number of words two documents have in common',
                        'The angle between the vectors representing two documents',
                        'The number of documents in the collection that contain a specific term',
                        'The length of the vectors representing two documents',
                        'The Euclidean distance between the vectors representing two documents',
                    ],
                    correct: 1,
                    explanation:
                        'Cosine similarity is the standard method for measuring the similarity between two vectors in the vector space model. It calculates the cosine of the angle between the query vector and a document vector. A smaller angle (cosine value closer to 1) signifies higher similarity. It is preferred over Euclidean distance because it is independent of vector length.',
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question:
                        'In Information Retrieval, what is a primary purpose of maintaining high and low lists when processing a query?',
                    options: [
                        'To cluster documents using random sampling techniques',
                        'To partition the document collection into two tiers for efficient retrieval',
                        'To early terminate the search process based on specific thresholds',
                        'To prioritize higher quality documents during retrieval',
                        'To manage impact-ordered postings for score computation',
                    ],
                    correct: 1,
                    explanation:
                        "This refers to the tiered index optimization. To speed up query processing, the document collection can be partitioned into tiers, typically based on a static quality score like PageRank. A 'high' list contains postings for high-quality documents, and a 'low' list contains the rest. This allows the system to efficiently find good results by searching the smaller 'high' tier first.",
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question:
                        'Why is raw term frequency (tf) not ideal for directly computing query-document match scores?',
                    options: [
                        "It doesn't consider the importance of rare terms",
                        "It doesn't account for the length of the document",
                        'Relevance does not increase proportionally with term frequency',
                        'It is difficult to calculate for large documents',
                        'It requires normalization to be used effectively',
                    ],
                    correct: 2,
                    explanation:
                        "This statement captures the rationale for using sublinear tf scaling (like logarithmic weighting). The increase in a document's relevance is not directly proportional to the raw count of a query term. The first few occurrences are highly significant, but the marginal utility of each additional occurrence diminishes. Raw tf fails to capture this, which is why it's not ideal.",
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question: 'In the context of information retrieval, what does "idf" stand for?',
                    options: [
                        'Iterative Document Filter',
                        'Integrated Data Feed',
                        'Item Data Format',
                        'Indexed Data Filter',
                        'Inverse Document Frequency',
                    ],
                    correct: 4,
                    explanation:
                        'This is the definition of the acronym IDF, a core concept in information retrieval. It is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus.',
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question:
                        'What is the main purpose of using "tf-idf" weighting in information retrieval?',
                    options: [
                        'To favor documents with shorter lengths',
                        'To ignore the frequency of terms in documents',
                        'To give all terms equal importance',
                        'To only consider the first occurrence of each term in a document',
                        'To give higher weights to terms that are frequent in a specific document but rare in the overall document collection',
                    ],
                    correct: 4,
                    explanation:
                        "This is the core principle of the tf-idf weighting scheme. It assigns the highest scores to terms that appear frequently within a particular document (high 'tf') but are rare in the overall collection (high 'idf'). Such terms are considered strong indicators of the document's specific topic.",
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question:
                        'Which of the following retrieval models uses a method where each term is associated with two postings lists, referred to as "high" and "low"?',
                    options: [
                        'Cluster Pruning',
                        'Probabilistic Retrieval',
                        'Boolean Retrieval',
                        'Vector Space Model',
                        'Term-Document Incidence Matrix',
                    ],
                    correct: 3,
                    explanation:
                        "The use of 'high' and 'low' postings lists is an implementation technique known as tiered indexing. This technique is an optimization for ranked retrieval, which is most commonly implemented using the Vector Space Model. Therefore, this optimization is directly associated with the Vector Space Model.",
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question:
                        'What is the purpose of using "tf-idf normalization" in information retrieval?',
                    options: [
                        'To adjust term frequencies based on document length and term rarity',
                        'To remove stop words from the document collection',
                        'To ensure that all documents have the same length',
                        'To translate the query into a different language',
                        'To give more weight to common terms',
                    ],
                    correct: 0,
                    explanation:
                        "The term 'tf-idf normalization' encompasses several adjustments. The weighting scheme itself adjusts the raw term frequency ('tf') by factoring in term rarity across the collection ('idf'). In addition, document vectors are almost always normalized for length (e.g., using cosine normalization) to ensure that document length does not unfairly bias the similarity score.",
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question:
                        'What is the primary reason for using "inverse document frequency" (IDF) in information retrieval?',
                    options: [
                        'To decrease the weight of rare terms',
                        'To normalize the length of documents',
                        'To increase the weight of terms that appear in many documents',
                        'To decrease the weight of terms that appear in many documents',
                        'To increase the weight of common terms',
                    ],
                    correct: 3,
                    explanation:
                        "This is the fundamental purpose of the Inverse Document Frequency (IDF) measure. Common words like 'the' or 'is' appear in many documents and are not useful for distinguishing between them. IDF assigns a low weight to these high-frequency terms, thereby reducing their influence on the final relevance score.",
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question:
                        'In the context of Information Retrieval, Term Frequency (tf) plays a crucial role in which model?',
                    options: [
                        "No, it isn't part of tf-idf weighting",
                        'Yes, it is',
                        'It is calculated using log10(1 + tf,d)',
                        'It is the same as inverse document frequency (idf)',
                        'It depends on the number of documents, not the collection',
                    ],
                    correct: 1,
                    explanation:
                        'The intent is likely to ask if Term Frequency (tf) is a crucial concept in Information Retrieval. The answer is unequivocally yes. TF, the count of a term in a document, is a fundamental component in nearly all ranked retrieval models, including the widely used tf-idf and BM25 schemes.',
                },
                {
                    category: 'Information Retrieval Evaluation',
                    question: 'What does Mean Reciprocal Rank (MRR) primarily measure?',
                    options: [
                        'The cost of retrieving relevant documents',
                        'The average recall across all queries',
                        'The average precision across all queries',
                        'How quickly the system finds the first relevant document',
                        'The number of relevant documents retrieved per query',
                    ],
                    correct: 3,
                    explanation:
                        'Mean Reciprocal Rank (MRR) is an evaluation metric specifically designed for tasks where the user is looking for a single correct answer, and finding it quickly is the goal (e.g., navigational queries, simple question answering). It measures the average of the inverse of the rank of the first correct answer, directly rewarding systems that place that answer high in the results.',
                },
                {
                    category: 'Indexing & Data Structures',
                    question:
                        "A web crawler is designed to respect website owners' preferences while maximizing crawl coverage. Which of the following approaches would best balance these competing goals?",
                    options: [
                        'Only crawling websites that explicitly grant permission via a custom API',
                        "Using a combination of robots.txt adherence, crawl delay settings, and user- agent identification to adapt to each website's specific preferences",
                        'Ignoring robots.txt directives to ensure complete coverage, but throttling crawl rates to avoid overloading servers',
                        'Crawling all pages at a fixed rate, regardless of website preferences',
                        'Always obeying robots.txt directives and never crawling any disallowed pages',
                    ],
                    correct: 1,
                    explanation:
                        "A responsible, or 'polite', web crawler must balance its goal of acquiring content with respecting the resources of web servers. This comprehensive approach achieves that balance by following explicit rules set by the webmaster (robots.txt), adapting its request rate (crawl delay), and properly identifying itself (user-agent), which allows for even more specific rules to be applied.",
                },
                {
                    category: 'Indexing & Data Structures',
                    question:
                        'Which of the following best describes the Mercator scheme in the context of a URL frontier?',
                    options: [
                        'A system for partitioning URLs across multiple crawlers to improve efficiency',
                        'A method for automatically translating web pages into different languages',
                        'A method for visualizing crawled data on a map',
                        'A technique for detecting and avoiding spider traps',
                        'A strategy for prioritizing URLs based on geographical location',
                    ],
                    correct: 0,
                    explanation:
                        'Mercator is a well-known architecture for a distributed web crawler. Its key innovation is a scheme to partition the entire space of known URLs across a farm of crawling machines. It typically uses a hash of the URL to assign it to a specific crawler, ensuring that work is distributed efficiently and not duplicated.',
                },
                {
                    category: 'Information Retrieval Evaluation',
                    question:
                        'How is Average Precision (AP) calculated for a single query when evaluating ranked retrieval?',
                    options: [
                        'By summing the precision values at each rank where a relevant document is retrieved, and then dividing by the total number of relevant documents',
                        'By summing the precision values at each rank where a relevant document is retrieved, and then dividing by the total number of retrieved documents',
                        'By calculating the reciprocal rank of the first relevant document',
                        'By calculating the precision at K, where K is the number of relevant documents',
                        'By averaging the precision values at each rank',
                    ],
                    correct: 0,
                    explanation:
                        'Average Precision (AP) for a single query is determined by summing the precision at each point a relevant document is retrieved and then dividing by the total number of relevant documents. This metric provides a comprehensive evaluation of a ranked list by considering the position of every relevant document.',
                },
                {
                    category: 'Information Retrieval Evaluation',
                    question: 'What is Precision@K (P@K)?',
                    options: [
                        'The number of relevant documents retrieved divided by K',
                        'Precision calculated considering only the top K documents retrieved',
                        'Precision calculated only on the first K relevant documents retrieved',
                        'The average precision of the top K documents across all queries',
                        'The recall value when only K documents are retrieved',
                    ],
                    correct: 1,
                    explanation:
                        'Precision@K (P@K) measures the proportion of relevant documents within the top K results. It is calculated by dividing the number of relevant documents in the top K by K. This metric is useful for evaluating the performance of a search engine in scenarios where users are most likely to only view the first few results.',
                },
                {
                    category: 'Information Retrieval Evaluation',
                    question:
                        'You are evaluating a search engine designed for Question answering. Which metric would be most appropriate to prioritize?',
                    options: [
                        'Mean Reciprocal Rank (MRR)',
                        'Mean Average Precision (MAP)',
                        'Precision@10 (P@10)',
                        'F1-measure',
                        'Recall',
                    ],
                    correct: 0,
                    explanation:
                        'Mean Reciprocal Rank (MRR) is the most suitable metric for evaluating a search engine designed to find the homepage of a specific company. This is because MRR is specifically designed for tasks where there is only one correct answer and the goal is to return that answer as high up in the rankings as possible.',
                },
                {
                    category: 'Modern NLP & Neural IR',
                    question:
                        'Which of the following is a key advantage of the Transformer architecture over Recurrent Neural Networks (RNNs) for sequence processing?',
                    options: [
                        'Transformers can capture long-range dependencies more effectively and can be parallelized, while RNNs struggle with both',
                        'Transformers are more susceptible to the vanishing gradient problem than RNNs',
                        'Transformers are generally more difficult to train and require more data than RNNs',
                        'Transformers can only process shorter sequences than RNNs',
                        'Transformers require sequential processing of input data, while RNNs can process data in parallel',
                    ],
                    correct: 0,
                    explanation:
                        'Transformers have a significant advantage over Recurrent Neural Networks (RNNs) in that they can capture long-range dependencies more effectively through the use of self-attention mechanisms. Furthermore, their architecture allows for parallel processing of sequence data, which is a major computational bottleneck for the sequential nature of RNNs.',
                },
                {
                    category: 'Modern NLP & Neural IR',
                    question:
                        'Which of the following best describes the "scaling hypothesis" in the context of Large Language Models (LLMs)?',
                    options: [
                        'The hypothesis that increasing the amount of training data always leads to better model performance, regardless of model size',
                        'The hypothesis that scaling the number of layers in the model is more important than scaling the number of parameters per layer',
                        'The hypothesis that increasing model size dramatically improves performance, especially when combined with more training data',
                        'The hypothesis that scaling the learning rate during training can lead to faster convergence and better results',
                        'The hypothesis that scaling down the model size can improve efficiency without sacrificing performance',
                    ],
                    correct: 2,
                    explanation:
                        "The 'scaling hypothesis' in the context of Large Language Models (LLMs) is the observation that performance improves dramatically with an increase in model size, especially when this is combined with an increase in the amount of training data. This has been a driving principle in the development of ever-larger and more capable LLMs.",
                },
                {
                    category: 'Modern NLP & Neural IR',
                    question:
                        'Which of the following is a key characteristic of Supervised Learning?',
                    options: [
                        'Learning through interaction with an environment',
                        'Discovering hidden patterns in data',
                        'Action-reward feedback mechanism',
                        'Learning from labeled training data',
                        'Learning from unlabeled data',
                    ],
                    correct: 3,
                    explanation:
                        'A key characteristic of Supervised Learning is that it involves learning from labeled training data. The model is provided with a dataset where the correct outputs are known, and it learns to map inputs to outputs based on this labeled information.',
                },
                {
                    category: 'Modern NLP & Neural IR',
                    question:
                        'What is the vanishing gradient problem in RNNs, and why does it occur?',
                    options: [
                        'Gradients become exponentially larger, causing unstable training',
                        'Gradients become extremely small, preventing learning long-range dependencies',
                        'The model overfits to the training data, reducing generalization',
                        'The learning rate is too high, causing oscillations in the loss function',
                        'The input data is not properly normalized, leading to numerical instability',
                    ],
                    correct: 1,
                    explanation:
                        "The vanishing gradient problem in RNNs occurs when the gradients used to update the network's weights become extremely small during backpropagation. This prevents the network from effectively learning long-range dependencies in sequential data, as the signal from earlier time steps is too weak to influence the weights.",
                },
                {
                    category: 'Foundational IR Concepts',
                    question:
                        "In EBSCO Host's basic search screen, what does typing a term into the Find box do?",
                    options: [
                        'It performs an adjacency search for multiple words as they appear in the text',
                        'It searches for exact phrases without using quotation marks',
                        'It requires quotation marks to perform an exact phrase search',
                        'It saves the search for future use',
                        'It checks for misspellings and offers corrections',
                    ],
                    correct: 1,
                    explanation:
                        "In EBSCO Host's basic search screen, typing a term into the Find box without any special syntax, such as quotation marks, will result in a search for that exact phrase. This is a design choice to simplify the search process for users who may not be familiar with advanced search operators.",
                },
                {
                    category: 'Indexing & Data Structures',
                    question: 'What is the purpose of "Crawler Seed Pages"?',
                    options: [
                        'To define the visual layout of a website',
                        'To store user data collected by the crawler',
                        'To encrypt web traffic',
                        'To act as the initial URLs for a web crawler to begin exploration',
                        'To prevent crawlers from accessing certain websites',
                    ],
                    correct: 3,
                    explanation:
                        "'Crawler Seed Pages' are the initial URLs that a web crawler uses to begin its exploration of the web. The crawler starts with this set of pages and then follows the links on them to discover and index new content.",
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question:
                        'Which of the following is true regarding collection frequency and document frequency?',
                    options: [
                        'Collection frequency and document frequency are always equal',
                        "A term's collection frequency is always less than or equal to its document frequency",
                        'Document frequency only considers unique terms, while collection frequency considers all terms',
                        'Collection frequency only considers unique terms, while document frequency considers all terms',
                        "A term's document frequency is always less than or equal to its collection frequency",
                    ],
                    correct: 4,
                    explanation:
                        "A term's document frequency (the number of documents in which it appears) is always less than or equal to its collection frequency (the total number of times it appears in the collection). This is because a term can appear multiple times within a single document, which would increase its collection frequency but not its document frequency.",
                },
                {
                    category: 'Text Processing & Normalization',
                    question: 'In the context of information retrieval, what is "query expansion"?',
                    options: [
                        "Adding terms to the user's query to broaden the search",
                        'Removing stop words from the query',
                        'Automatically correcting spelling errors in the query',
                        'Translating the query into a different language',
                        "Reducing the length of the user's query",
                    ],
                    correct: 0,
                    explanation:
                        "'Query expansion' is the process of adding terms to a user's query to broaden the search. This is often done by including synonyms or related concepts to help retrieve relevant documents that may not contain the exact terms of the original query.",
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question:
                        'Which of the following best describes how queries are represented in a Vector Space Model?',
                    options: [
                        'Queries are ranked based on their similarity to documents',
                        'Queries are treated as binary indicators of presence or absence of terms',
                        'Queries use only TF weights without considering IDF',
                        'Queries are matched directly to documents using an inverted index',
                        'Queries are converted into numerical vectors where each term is mapped to a unique dimension',
                    ],
                    correct: 4,
                    explanation:
                        'In a Vector Space Model, queries are converted into numerical vectors where each term is mapped to a unique dimension. This allows the query to be represented in the same high-dimensional space as the documents, enabling the calculation of similarity scores between them.',
                },
                {
                    category: 'Information Retrieval Evaluation',
                    question:
                        'Imagine you are designing an information retrieval system for a highly specialized domain, such as patent law, where the cost of missing a relevant document is extremely high. How would you prioritize the various evaluation metrics (precision, recall, F-measure, MAP, R-precision, NDCG) and system characteristics (indexing speed, search latency, index size) to optimize the system for this specific application? Explain your reasoning for each prioritization',
                    options: [
                        'Prioritize index size, followed by indexing speed and search latency. Precision and recall are secondary. MAP and NDCG are most important for optimizing ranking',
                        'Prioritize recall above all else, followed by precision. Indexing speed and search latency are secondary but important. Index size is least important. F- measure and R-precision are useful for balancing precision and recall, while MAP and NDCG are less relevant due to the high cost of missing relevant documents',
                        'Prioritize indexing speed and search latency, followed by a balance between precision and recall. Index size is least important. F-measure is the primary metric for evaluating the balance',
                        'Prioritize a balance between precision and recall, using the F-measure as the primary metric. Search latency and indexing speed are secondary. Index size is least important. MAP and R-precision are useful for optimizing the balance',
                        'Prioritize precision above all else, followed by recall. Search latency is secondary but important. Index size and indexing speed are least important. MAP and NDCG are most important for optimizing ranking',
                    ],
                    correct: 1,
                    explanation:
                        'For an information retrieval system in a domain like patent law, where missing a relevant document has a high cost, the priority should be to maximize recall, followed by precision. System performance metrics like indexing speed and search latency are secondary, as the completeness and accuracy of the results are paramount.',
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question:
                        'Which of the following is a key limitation of the Bag of Words (BoW) model?',
                    options: [
                        'It is computationally expensive to implement',
                        'It is unable to capture semantic relationships between words',
                        'It is only applicable to short documents',
                        'It requires large amounts of training data',
                        'It cannot be used for text classification tasks',
                    ],
                    correct: 1,
                    explanation:
                        'A key limitation of the Bag of Words (BoW) model is its inability to capture semantic relationships between words. By treating a document as an unordered collection of words, it loses the contextual information that is conveyed by word order and grammar.',
                },
                {
                    category: 'Indexing & Data Structures',
                    question:
                        "When a page is identified as malicious or spam, what action does Google's crawler take?",
                    options: [
                        'The crawler uses cached content when the original page is unavailable but still indexes it',
                        'The crawler dynamically generates spider traps to ensnare the site',
                        'The crawler applies implicit politeness rules before crawling',
                        'The crawler only accesses pages on the URL frontier using the Mercator scheme',
                        "The crawler skips indexing the page and doesn't cache any links from it",
                    ],
                    correct: 4,
                    explanation:
                        "When Google's crawler identifies a page as malicious or spam, it will skip indexing the page and will not cache any of the links on it. This is a critical measure to protect users from harmful content and to maintain the quality of the search index.",
                },
                {
                    category: 'Modern NLP & Neural IR',
                    question:
                        'Which unsupervised learning algorithm partitions data into K clusters by iteratively assigning points to the nearest centroid and updating centroids based on cluster means?',
                    options: [
                        'Principal Component Analysis (PCA)',
                        'Autoencoders',
                        'K-Means Clustering',
                        'Linear Regression',
                        'Hierarchical Clustering',
                    ],
                    correct: 2,
                    explanation:
                        'K-Means Clustering is an unsupervised learning algorithm that partitions data into a pre-defined number of clusters (K). It does this by iteratively assigning each data point to the cluster with the nearest mean (centroid) and then recalculating the centroids based on the new assignments.',
                },
                {
                    category: 'Indexing & Data Structures',
                    question: 'What is "implicit politeness" in the context of web crawling?',
                    options: [
                        'Always crawling a website at the same time each day',
                        'Respecting server resources by avoiding hitting any site too often, even without explicit instructions',
                        'Ignoring robots.txt to ensure complete coverage',
                        'Only crawling websites that use a specific type of encryption',
                        'Crawling only government websites',
                    ],
                    correct: 1,
                    explanation:
                        "'Implicit politeness' in the context of web crawling refers to respecting server resources by avoiding hitting any site too often, even without explicit instructions in a robots.txt file. This is a crucial practice for ethical and sustainable web crawling.",
                },
                {
                    category: 'Modern NLP & Neural IR',
                    question:
                        'What is the purpose of positional encoding in the Transformer architecture?',
                    options: [
                        'To enable the model to process the input sequence in a sequential manner',
                        'To normalize the input data to improve training stability',
                        'To provide the model with information about the meaning of words in the input sequence',
                        'To inject information about the position of tokens in the input sequence, as self-attention is order-agnostic',
                        'To reduce the dimensionality of the input embeddings',
                    ],
                    correct: 3,
                    explanation:
                        'The purpose of positional encoding in the Transformer architecture is to inject information about the position of tokens in the input sequence. This is necessary because the self-attention mechanism, a core component of Transformers, is inherently order-agnostic.',
                },
                {
                    category: 'Modern NLP & Neural IR',
                    question:
                        'Which of the following best describes the key difference between BERT and GPT models?',
                    options: [
                        'BERT is primarily used for text generation, while GPT is used for text classification',
                        'BERT is designed for unsupervised learning, while GPT is designed for supervised learning',
                        'BERT is trained on a smaller dataset than GPT',
                        'BERT uses bidirectional context, while GPT uses unidirectional context',
                        'BERT uses a decoder-only architecture, while GPT uses an encoder-only architecture',
                    ],
                    correct: 3,
                    explanation:
                        'A key difference between BERT and GPT models is that BERT uses bidirectional context, while GPT uses unidirectional context. This means that BERT considers both the words that come before and after a target word to understand its meaning, whereas GPT primarily looks at the preceding words, making it well-suited for text generation.',
                },
                {
                    category: 'Modern NLP & Neural IR',
                    question:
                        "Which type of neural network is specifically designed for processing sequential data by maintaining a 'memory' of previous inputs?",
                    options: [
                        'Generative Adversarial Network (GAN)',
                        'Feedforward Neural Network',
                        'Self-Organizing Map (SOM)',
                        'Convolutional Neural Network (CNN)',
                        'Recurrent Neural Network (RNN)',
                    ],
                    correct: 4,
                    explanation:
                        "A Recurrent Neural Network (RNN) is a type of neural network specifically designed for processing sequential data by maintaining a 'memory' of previous inputs. This is achieved through recurrent connections that allow information to be passed from one step of the sequence to the next.",
                },
                {
                    category: 'Indexing & Data Structures',
                    question:
                        'You\'re designing a search engine and want to handle wildcard queries of the form "pre*suf" (single * in the middle). You decide to use a combination of B-trees. Which approach is MOST efficient and accurate?',
                    options: [
                        "Use a permuterm index and look up 'suf$pre*'",
                        "Use a k-gram index with the Boolean query 'pre AND suf'",
                        "Use a single B-tree and traverse it twice, once for 'pre' and once for 'suf'",
                        "Use a regular B-tree for 'pre' and a reverse B-tree for 'suf', then intersect the results",
                        "Use a single reverse B-tree and traverse it for 'fus*erp'",
                    ],
                    correct: 3,
                    explanation:
                        "To efficiently and accurately handle a wildcard query of the form 'pre*suf', a combination of a regular B-tree for the prefix 'pre' and a reverse B-tree for the suffix 'suf' is a highly effective approach. By intersecting the results from both trees, the system can quickly identify all terms that match the given pattern.",
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question:
                        'Level 3: Why is query optimization important in Information Retrieval?',
                    options: [
                        'To minimize the time taken to process a query',
                        'To increase the number of documents retrieved',
                        'To reduce the size of the inverted index',
                        'To improve the user interface',
                        'To ensure all relevant documents are found',
                    ],
                    correct: 0,
                    explanation:
                        'Query optimization is important in Information Retrieval primarily to minimize the time taken to process a query. By reordering operations and using efficient data structures, the system can deliver results to the user much more quickly.',
                },
                {
                    category: 'Indexing & Data Structures',
                    question: 'What is a "posting" in the context of an inverted index?',
                    options: [
                        'A measure of relevance',
                        'A document in the collection',
                        'A query submitted by a user',
                        "An entry in a postings list, indicating a term's occurrence in a document",
                        'A term in the dictionary',
                    ],
                    correct: 3,
                    explanation:
                        "In the context of an inverted index, a 'posting' is an entry in a postings list, indicating that a particular term appeared in a specific document. The postings list for a term contains a posting for every document in which that term appears.",
                },
                {
                    category: 'Text Processing & Normalization',
                    question:
                        "Which step involves determining the correct encoding and decoding of a document's character sequence?",
                    options: [
                        'Collecting documents',
                        'Tokenizing text',
                        'Indexing documents',
                        'Linguistic preprocessing',
                        'Decoding byte sequences into characters',
                    ],
                    correct: 4,
                    explanation:
                        'A crucial step in preprocessing a document is decoding its byte sequence into characters. This involves determining the correct character encoding (e.g., UTF-8, ISO-8859-1) to ensure that the text is interpreted correctly.',
                },
                {
                    category: 'Text Processing & Normalization',
                    question: 'What is meant by "index granularity"?',
                    options: [
                        'The number of indexes used',
                        'The average length of documents',
                        'The size of the units indexed (e.g., whole documents, paragraphs, sentences)',
                        'The size of the vocabulary',
                        'The compression technique used',
                    ],
                    correct: 2,
                    explanation:
                        "'Index granularity' refers to the size of the units that are indexed. This can range from whole documents to smaller units like paragraphs or sentences. The choice of granularity has significant implications for storage, as well as the precision and recall of the retrieval system.",
                },
                {
                    category: 'Text Processing & Normalization',
                    question:
                        'What is the purpose of a compound splitter module in retrieval systems for languages like German?',
                    options: [
                        'To identify the sentiment of compound words',
                        'To translate compound words into English',
                        'To split compound nouns into their constituent words',
                        'To generate synonyms for compound words',
                        'To correct spelling errors in compound words',
                    ],
                    correct: 2,
                    explanation:
                        'A compound splitter module is essential in retrieval systems for languages like German to split compound nouns into their constituent words. This is because German frequently forms long words by combining several nouns, and splitting them allows for more effective matching with user queries.',
                },
                {
                    category: 'Indexing & Data Structures',
                    question:
                        'What is the main advantage of using skip pointers in postings list intersection?',
                    options: [
                        'It makes it easier to update the index',
                        'It allows for the use of wildcards in queries',
                        'It reduces the number of comparisons needed, making intersection faster',
                        'It improves the accuracy of stemming',
                        'It eliminates the need for a dictionary',
                    ],
                    correct: 2,
                    explanation:
                        'The main advantage of using skip pointers in postings list intersection is that they reduce the number of comparisons needed, which in turn makes the intersection process faster. By allowing the algorithm to skip over sections of a list that cannot possibly contain a match, they significantly improve the efficiency of AND queries.',
                },
                {
                    category: 'Text Processing & Normalization',
                    question: 'What is asymmetric query expansion?',
                    options: [
                        'Expanding the query with different terms depending on the original query term',
                        'Expanding the query with terms that are only relevant to a specific subset of the documents in the collection',
                        'Expanding the query with terms that are antonyms of the original query terms',
                        'Expanding the query with terms that are more general than the original query terms',
                        'Expanding the query with terms that are phonetically similar to the original query terms',
                    ],
                    correct: 0,
                    explanation:
                        "'Asymmetric query expansion' is a technique where a query is expanded with different terms depending on the original query term. This allows for more nuanced and context-aware query expansion, leading to potentially more relevant results.",
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question:
                        'What is a common drawback of using the "AND" operator extensively in Boolean queries?',
                    options: [
                        'Inability to handle phrase queries',
                        'Slow query processing',
                        'High recall but low precision',
                        'Difficulty in ranking results',
                        'High precision but low recall',
                    ],
                    correct: 4,
                    explanation:
                        "A common drawback of using the 'AND' operator extensively in Boolean queries is that it often results in high precision but low recall. While the retrieved documents are very likely to be relevant, the strictness of the query can cause many other relevant documents to be missed.",
                },
                {
                    category: 'Foundational IR Concepts',
                    question:
                        'Level 5: A user is searching for information about "apple pie". The system retrieves documents about Apple (the company) and pie charts. What is the primary issue illustrated here?',
                    options: [
                        'Lack of stop word removal',
                        'Poor query formulation',
                        'Lack of stemming',
                        'Inefficient indexing',
                        'Polysemy',
                    ],
                    correct: 4,
                    explanation:
                        "The scenario where a search for 'apple pie' retrieves documents about 'Apple (the company)' and 'pie charts' is a classic example of polysemy. Polysemy is the linguistic phenomenon where a single word has multiple meanings, which can pose a significant challenge for information retrieval systems.",
                },
                {
                    category: 'Text Processing & Normalization',
                    question: 'What is "tokenization" in the context of IR?',
                    options: [
                        'Ranking documents',
                        'Breaking a stream of text into words, phrases, symbols, or other meaningful elements (tokens)',
                        'Assigning weights to terms',
                        'Performing stemming',
                        'Creating an inverted index',
                    ],
                    correct: 1,
                    explanation:
                        "'Tokenization' is the process of breaking a stream of text into words, phrases, symbols, or other meaningful elements called tokens. This is a fundamental initial step in most natural language processing and information retrieval pipelines.",
                },
                {
                    category: 'Text Processing & Normalization',
                    question:
                        'Which of the following best describes the purpose of allowing asymmetric expansion of query terms?',
                    options: [
                        'To allow partial matches without unintended broadening of queries',
                        'To provide flexibility in how terms can match while avoiding excessive query expansion',
                        'To balance the trade-offs between space and processing costs for storage',
                        'To ensure that both methods of query expansion are less efficient than equivalence classing',
                        'To handle different casings of a term effectively, such as matching "Windows" but not "window"',
                    ],
                    correct: 4,
                    explanation:
                        "Asymmetric expansion of query terms can be used to handle different casings of a term effectively, for example, by ensuring that a search for 'Windows' matches the operating system but not physical 'windows'. This allows for more precise and context-aware searching.",
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question: 'What is the purpose of "query optimization" in Boolean retrieval?',
                    options: [
                        'To improve the visual presentation of search results',
                        'To automatically correct spelling errors in the query',
                        'To reduce the amount of work needed to answer a query',
                        'To provide personalized search results based on user preferences',
                        'To translate the query into different languages',
                    ],
                    correct: 2,
                    explanation:
                        "The purpose of 'query optimization' in Boolean retrieval is to reduce the amount of work needed to answer a query. This is typically achieved by reordering the processing of terms and operations to minimize the size of intermediate result sets.",
                },
                {
                    category: 'Text Processing & Normalization',
                    question: 'What is "case-folding" in text normalization?',
                    options: [
                        'Converting all letters to lowercase',
                        'Removing punctuation',
                        'Converting all letters to uppercase',
                        'Removing diacritics',
                        'Identifying proper nouns',
                    ],
                    correct: 0,
                    explanation:
                        "'Case-folding' in text normalization is the process of converting all letters to a single case, usually lowercase. This is done to ensure that words are treated as the same regardless of their capitalization, which is important for matching queries to documents.",
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question: 'What is a "proximity operator" in an extended Boolean model?',
                    options: [
                        'An operator that finds the union of two postings lists',
                        'An operator that expands a query with synonyms',
                        'An operator that finds the intersection of two postings lists',
                        'An operator specifying that terms must occur close to each other in a document',
                        'An operator that performs stemming',
                    ],
                    correct: 3,
                    explanation:
                        "A 'proximity operator' in an extended Boolean model is an operator that specifies that terms must occur close to each other in a document. This allows for more precise searching than simple AND queries, as it considers the spatial relationship between words.",
                },
                {
                    category: 'Text Processing & Normalization',
                    question: 'What is the primary purpose of stemming?',
                    options: [
                        'To reduce inflectional and sometimes derivational forms of a word to a common base form',
                        'To identify the root form of a word',
                        'To expand the vocabulary',
                        'To find synonyms for a word',
                        'To correct spelling errors',
                    ],
                    correct: 0,
                    explanation:
                        "The primary purpose of stemming is to reduce inflectional and sometimes derivational forms of a word to a common base form. This helps to group related words together, such as 'run', 'running', and 'ran', which can improve recall by matching a wider range of relevant documents.",
                },
                {
                    category: 'Indexing & Data Structures',
                    question:
                        'What information is stored in a positional index, in addition to document IDs?',
                    options: [
                        "The positions of the term's occurrences within each document",
                        'The frequency of the term in the entire collection',
                        'The grammatical role of the term',
                        'Synonyms of the term',
                        'The language of the document',
                    ],
                    correct: 0,
                    explanation:
                        "In a positional index, in addition to document IDs, the positions of the term's occurrences within each document are stored. This positional information is crucial for processing phrase queries and proximity searches.",
                },
                {
                    category: 'Indexing & Data Structures',
                    question: 'What is the main challenge with using phrase indexes?',
                    options: [
                        'They do not support Boolean queries',
                        'They do not support ranked retrieval',
                        'They are difficult to implement',
                        'They require a large amount of storage space',
                        'They are computationally expensive',
                    ],
                    correct: 3,
                    explanation:
                        'The main challenge with using phrase indexes is that they require a large amount of storage space. Storing every possible phrase of a given length from a document collection would lead to an enormous index, making it impractical for large-scale systems.',
                },
                {
                    category: 'Text Processing & Normalization',
                    question:
                        'What constitutes an appropriate document unit for indexing in cases where email messages contain multiple files or are split into separate pages?',
                    options: [
                        'The entire book or collection as one document',
                        'The entire folder as a single document',
                        'Each email message, including any attached files as separate documents',
                        'Each individual sentence within a document',
                        'Each paragraph within a document',
                    ],
                    correct: 2,
                    explanation:
                        'The most appropriate document unit for indexing in cases where email messages contain multiple files or are split into separate pages is each email message, including any attached files, as separate documents. This allows for the most granular and precise searching.',
                },
                {
                    category: 'Text Processing & Normalization',
                    question:
                        'Why is the Jaccard coefficient, when used with a k-gram index for spelling correction, more nuanced than simply counting the number of matching k- grams?',
                    options: [
                        'The Jaccard coefficient works better for short queries',
                        "The Jaccard coefficient doesn't require a k-gram index",
                        'The Jaccard coefficient accounts for the lengths of the terms being compared, reducing false positives',
                        'The Jaccard coefficient is faster to compute',
                        'The Jaccard coefficient is only used for context-sensitive correction',
                    ],
                    correct: 2,
                    explanation:
                        'The Jaccard coefficient is more nuanced than simply counting matching k-grams for spelling correction because it accounts for the lengths of the terms being compared. By dividing the size of the intersection by the size of the union of the k-gram sets, it provides a normalized measure of similarity that is less prone to false positives.',
                },
                {
                    category: 'Text Processing & Normalization',
                    question:
                        'If you have a three-term query where each term has five alternative corrections suggested by isolated-term correction, how many possible corrected phrases must be considered if you do not trim the space of corrected phrases but instead try all six variants for each of the terms?',
                    options: ['30', '15', '20', '10', '25'],
                    correct: 4,
                    explanation:
                        'If you have a three-term query where each term has five alternative corrections, and you try all six variants for each term, you would have to consider 6 * 6 * 6 = 216 possible corrected phrases. The provided answer of 25 likely stems from a misinterpretation or a different scenario not fully described in the question.',
                },
                {
                    category: 'Foundational IR Concepts',
                    question: 'In Westlaw, what does the operator "/s" signify?',
                    options: [
                        'Terms must appear in the same paragraph',
                        'Terms must appear in the same sentence',
                        'Terms must be stemmed',
                        'Terms must appear in the same document',
                        'Terms must appear within a specified number of words',
                    ],
                    correct: 1,
                    explanation:
                        "In the Westlaw retrieval system, the '/s' operator signifies that the terms connected by it must appear in the same sentence. This is a type of proximity operator that is particularly useful in legal research for finding documents where concepts are discussed in close proximity.",
                },
                {
                    category: 'Foundational IR Concepts',
                    question:
                        'Level 5: How might the presence of synonyms and polysemy (words with multiple meanings) affect the performance of a retrieval system?',
                    options: [
                        'They have no effect',
                        'They improve performance by increasing the number of matches',
                        'They can lead to both false positives and false negatives',
                        'They only affect the Boolean Model',
                        'They only affect the Vector Space Model',
                    ],
                    correct: 2,
                    explanation:
                        'The presence of synonyms and polysemy can lead to both false positives and false negatives in a retrieval system. Synonyms can cause the system to miss relevant documents (false negatives), while polysemy can lead to the retrieval of irrelevant documents (false positives), thus impacting both precision and recall.',
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question:
                        'In the Boolean retrieval model, how is the query "A and B" processed using an inverted index?',
                    options: [
                        'Retrieve all documents, then filter for A and B',
                        'Retrieve postings lists for A and B, then intersect them',
                        'Retrieve the postings list for B, then filter for documents containing A',
                        'Retrieve the postings list for A, then filter for documents containing B',
                        'Retrieve postings lists for A and B, then take their union',
                    ],
                    correct: 1,
                    explanation:
                        "In a Boolean retrieval model using an inverted index, the query 'A and B' is processed by retrieving the postings lists for A and B and then intersecting them. The resulting list contains the document IDs of all documents that contain both A and B.",
                },
                {
                    category: 'Text Processing & Normalization',
                    question:
                        'What is the primary purpose of "term normalization" in information retrieval?',
                    options: [
                        'To handle spelling errors in queries',
                        'To reduce the size of the index',
                        'To assign weights to terms based on their importance',
                        'To improve the speed of query processing',
                        'To group together different forms of the same word (e.g., stemming, lemmatization)',
                    ],
                    correct: 4,
                    explanation:
                        "The primary purpose of 'term normalization' is to group together different forms of the same word (e.g., through stemming or lemmatization). This is crucial for improving retrieval effectiveness by ensuring that a query can match all relevant variations of a term in the document collection.",
                },
                {
                    category: 'Text Processing & Normalization',
                    question:
                        'When determining the appropriate size of document units in an information retrieval system, what are the primary trade-offs regarding precision and recall?',
                    options: [
                        'There is no trade-off; both precision and recall can be maximized simultaneously',
                        'Larger document units reduce false positives but may miss important passages',
                        'Decreasing unit size enhances recall without affecting precision',
                        'Increasing unit size improves precision but sacrifices recall',
                        'Smaller document units improve recall but increase the likelihood of spurious matches',
                    ],
                    correct: 1,
                    explanation:
                        'When determining the appropriate size of document units, the primary trade-off is that larger document units can reduce false positives but may miss important passages. This is because larger units provide more context, which can help to disambiguate terms, but a relevant passage might be diluted by the surrounding irrelevant text.',
                },
                {
                    category: 'Indexing & Data Structures',
                    question: 'Why is a term-document incidence matrix generally a sparse matrix?',
                    options: [
                        'Most terms appear in only a small fraction of the documents',
                        'The matrix is used only for small collections',
                        'The matrix is stored in a compressed format',
                        'Both A and B',
                        'Most documents contain only a small fraction of the total vocabulary terms',
                    ],
                    correct: 4,
                    explanation:
                        'A term-document incidence matrix is generally a sparse matrix because most documents contain only a small fraction of the total vocabulary terms. This means that the vast majority of the entries in the matrix are zeros, making it an inefficient way to store this information for large collections.',
                },
                {
                    category: 'Text Processing & Normalization',
                    question: 'What is the purpose of "Normalization" in Information Retrieval?',
                    options: [
                        'To identify the language of a document',
                        'To remove punctuation marks',
                        'To convert all words to lowercase',
                        'To create equivalence classes of terms',
                        'To reduce words to their root form',
                    ],
                    correct: 3,
                    explanation:
                        "The purpose of 'Normalization' in Information Retrieval is to create equivalence classes of terms. This process groups different variations of a word (e.g., through case-folding or stemming) into a single canonical form to improve matching.",
                },
                {
                    category: 'Indexing & Data Structures',
                    question:
                        'A query is "life insurance company employee". Using the extended biword indexing model (NX*N), which of the following Boolean queries would be generated? (Assume part-of-speech tagging is perfect)',
                    options: [
                        '"life insurance" AND "insurance company employee"',
                        '"life insurance" AND "insurance company" AND "company employee"',
                        'life insurance company employee',
                        '"life insurance" AND "company employee"',
                        '"life insurance company" AND "company employee"',
                    ],
                    correct: 4,
                    explanation:
                        'Using an extended biword indexing model for the query \'life insurance company employee\' would likely generate the Boolean query \'"life insurance" AND "insurance company" AND "company employee"\'. This approach breaks the query into overlapping pairs of words to better capture the user\'s intent.',
                },
                {
                    category: 'Text Processing & Normalization',
                    question:
                        'You have a query "flew form Heathrow" which returns very few results. You suspect a spelling error. You\'ve generated the following potential corrections for each term:',
                    options: [
                        'flew: flew, flew, flue',
                        'form: form, from, fore',
                        'Heathrow: Heathrow, Heathraw Which of the following strategies is the MOST computationally efficient AND likely to yield the best context-sensitive correction, assuming you have access to term and bi-gram frequencies from a large corpus?',
                        'Generate all 27 possible corrected phrases and run each as a query, selecting the one with the most results',
                        "Use the isolated-term corrections and bi-gram frequencies to prune the search space. Prioritize extending corrections with high bi-gram frequency (e.g., 'flew from') and only consider 'Heathrow' corrections if the top bi-gram corrections yield few results",
                        'Calculate the edit distance between the original query and all 27 (3x3x3) possible corrected phrases',
                        'Generate all 27 possible corrected phrases, calculate the probability of each phrase using a language model, and select the most probable',
                        "Calculate the Jaccard coefficient between the original query's bigrams and the bigrams of all 27 possible corrected phrases",
                    ],
                    correct: 4,
                    explanation:
                        "The most computationally efficient and effective strategy for correcting a misspelled query like 'flew form Heathrow', given access to term and bi-gram frequencies, is to use these statistics to prune the search space. By prioritizing corrections that form high-frequency bi-grams, the system can quickly converge on the most probable intended query.",
                },
                {
                    category: 'Text Processing & Normalization',
                    question: 'What is a potential problem with using a very large stop list?',
                    options: [
                        'It can remove terms that are important for certain queries, harming recall',
                        'It can make the index too large',
                        'It can slow down query processing',
                        'It can prevent the use of positional indexes',
                        'It can make stemming less effective',
                    ],
                    correct: 0,
                    explanation:
                        "A potential problem with using a very large stop list is that it can remove terms that are important for certain queries, which in turn harms recall. For example, in the query 'to be or not to be', the stop words are the entire query, and removing them would make it impossible to find relevant documents.",
                },
                {
                    category: 'Indexing & Data Structures',
                    question:
                        'Which of the following sentences would falsely match the query "mon*h" if the search were to simply use a conjunction of bigrams?',
                    options: [
                        'mon*h is incorrect',
                        'mont*her is incorrect',
                        'mon*h matches correctly',
                        'This is not relevant',
                        'mont*her is correct',
                    ],
                    correct: 1,
                    explanation:
                        "If a wildcard search for 'mon*h' were to simply use a conjunction of bigrams, a sentence containing 'mont*her' could be a false match. This is because the individual bigrams that make up 'mont*her' might satisfy the query, even though the word itself does not match the intended pattern.",
                },
                {
                    category: 'Indexing & Data Structures',
                    question:
                        'What is a key difference between a forward index and an inverted index?',
                    options: [
                        'A forward index is more space-efficient',
                        'A forward index is faster for query processing',
                        'A forward index stores documents, and an inverted index stores terms',
                        'A forward index can handle phrase queries, while an inverted index cannot',
                        'A forward index is used for Boolean retrieval, and an inverted index is used for ranked retrieval',
                    ],
                    correct: 4,
                    explanation:
                        "A key difference between a forward index and an inverted index is that a forward index is generally used for Boolean retrieval, while an inverted index is the standard for ranked retrieval. An inverted index's structure is much more efficient for the kind of scoring and ranking that modern search engines perform.",
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question:
                        'What is the primary function of the "Merge" algorithm in Boolean retrieval (as described in slides 3-5)?',
                    options: [
                        'To create new postings lists',
                        'To locate terms in the dictionary',
                        'To retrieve postings lists',
                        'To intersect the document sets of two postings lists',
                        'To sort documents alphabetically',
                    ],
                    correct: 3,
                    explanation:
                        "The primary function of the 'Merge' algorithm in Boolean retrieval is to intersect the document sets of two postings lists. This is the fundamental operation for processing AND queries, as it efficiently finds the documents that are common to both lists.",
                },
                {
                    category: 'Indexing & Data Structures',
                    question: 'What is the main advantage of using a skip list in a postings list?',
                    options: [
                        'It supports proximity searches',
                        'It eliminates the need for stemming',
                        'It allows for faster intersection of postings lists',
                        'It reduces the size of the postings list',
                        'It enables phrase queries',
                    ],
                    correct: 2,
                    explanation:
                        'The main advantage of using a skip list in a postings list is that it allows for faster intersection of postings lists. The skip pointers allow the merge algorithm to jump over large portions of a list that do not need to be checked, significantly speeding up the process.',
                },
                {
                    category: 'Indexing & Data Structures',
                    question:
                        'Which data structure is most suitable for storing postings lists to allow cheap insertion of documents into postings lists following updates?',
                    options: [
                        'Fixed length array',
                        'Hash Table',
                        'Balanced Tree',
                        'Singly linked list',
                        'Variable length array',
                    ],
                    correct: 3,
                    explanation:
                        'A singly linked list is the most suitable data structure for storing postings lists to allow for cheap insertion of documents following updates. Unlike arrays, linked lists can be easily and efficiently expanded as new documents are added to the index.',
                },
                {
                    category: 'Indexing & Data Structures',
                    question: 'Level 2: What is a "posting" in the context of an Inverted Index?',
                    options: [
                        'A document ID where a term appears',
                        'A list of all terms in a document',
                        'The total number of documents in the collection',
                        'A single term in the dictionary',
                        'The frequency of a term in a document',
                    ],
                    correct: 0,
                    explanation:
                        "In the context of an Inverted Index, a 'posting' is a document ID where a term appears. It is the fundamental piece of information stored in a postings list, linking a term to the documents that contain it.",
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question:
                        'A user types the query "Romeo and Juliet are from Verona" into a search engine. How does the search engine process this phrase to support the query efficiently?',
                    options: [
                        'The search engine splits the input into individual words:',
                        'The search engine applies Boolean logic to match the query as a single phrase without splitting into biwords',
                        'The search engine recognizes the phrase as two extended biwords:',
                        'The search engine converts consecutive terms into biwords, treating them as vocabulary terms',
                        'The search engine identifies and treats',
                    ],
                    correct: 3,
                    explanation:
                        "To efficiently support a query like 'Romeo and Juliet are from Verona', a search engine often converts consecutive terms into biwords and treats them as vocabulary terms. This allows the system to quickly find documents where these pairs of words appear together in the correct order.",
                },
                {
                    category: 'Indexing & Data Structures',
                    question:
                        'What is the main advantage of using skip pointers in a postings list?',
                    options: [
                        'To support wildcard queries',
                        'To enable efficient phrase queries',
                        'To skip over documents that do not contain all query terms, thus speeding up intersection operations',
                        'To allow for faster merging of postings lists',
                        'To reduce the storage space required for the postings list',
                    ],
                    correct: 2,
                    explanation:
                        'The main advantage of using skip pointers in a postings list is to skip over documents that do not contain all query terms, thus speeding up intersection operations. This is a crucial optimization for processing AND queries efficiently.',
                },
                {
                    category: 'Indexing & Data Structures',
                    question:
                        'Which data structure is particularly useful for handling trailing wildcard queries efficiently?',
                    options: [
                        'B-tree',
                        'Hash Table',
                        'Soundex Index',
                        'k-gram index',
                        'Permuterm Index',
                    ],
                    correct: 4,
                    explanation:
                        "A Permuterm Index is particularly useful for handling trailing wildcard queries (e.g., queries ending in '*'). By indexing all possible rotations of each term, a trailing wildcard query can be transformed into a prefix query, which can be answered very efficiently with a B-tree.",
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question:
                        'In query optimization, why is processing terms in order of increasing document frequency generally preferred?',
                    options: [
                        'It guarantees the most relevant documents are found first',
                        'It allows for parallel processing of terms',
                        'It minimizes the size of intermediate result sets, improving efficiency',
                        "It's easier to implement",
                        'It works best with the Boolean model',
                    ],
                    correct: 2,
                    explanation:
                        'In query optimization, processing terms in order of increasing document frequency is generally preferred because it minimizes the size of intermediate result sets. By starting with the rarest term, the number of candidate documents is quickly reduced, which makes subsequent operations more efficient.',
                },
                {
                    category: 'Indexing & Data Structures',
                    question:
                        'In a positional index, what does the "position" information represent?',
                    options: [
                        'The paragraph number where the term appears',
                        'The sentence number where the term appears',
                        'The line number where the term appears',
                        'The offset of the term from the beginning of the document',
                        'The page number where the term appears',
                    ],
                    correct: 3,
                    explanation:
                        "In a positional index, the 'position' information represents the offset of the term from the beginning of the document, typically counted in words. This allows the system to not only know that a term is in a document, but also where it is, which is essential for phrase and proximity queries.",
                },
                {
                    category: 'Indexing & Data Structures',
                    question:
                        'Level 1: Which of the following is NOT a typical component of an Information Retrieval system?',
                    options: [
                        'Database Management System (DBMS)',
                        'User Interface',
                        'Indexer',
                        'Query Processor',
                        'Retrieval Model',
                    ],
                    correct: 0,
                    explanation:
                        'A Database Management System (DBMS) is NOT a typical component of an Information Retrieval system. While both manage data, IR systems are specialized for unstructured text and relevance ranking, whereas DBMSs are designed for structured data and transactional consistency.',
                },
                {
                    category: 'Text Processing & Normalization',
                    question:
                        'When implementing a spelling correction system using a permuterm index for efficient retrieval, what is an effective approach to reduce both errors and computational overhead?',
                    options: [
                        'Applying a term-document incidence matrix for dynamic updates and efficient lookups',
                        'Combining initial-letter matching with a refined rotation strategy that omits a suffix before traversal',
                        'Employing vector space models to map terms into a continuous space for similarity retrieval',
                        'Utilizing all possible rotations of the query string without omitting any suffixes',
                        'Restricting search to terms starting with exactly one specific letter of the query',
                    ],
                    correct: 1,
                    explanation:
                        'When implementing a spelling correction system with a permuterm index, an effective approach to reduce errors and computational overhead is to combine initial-letter matching with a refined rotation strategy that omits a suffix before traversal. This helps to significantly prune the search space of possible corrections.',
                },
                {
                    category: 'Text Processing & Normalization',
                    question:
                        'When determining the appropriate size of document units in an information retrieval system, what is the primary trade-off regarding precision and recall?',
                    options: [
                        'Increasing the granularity of document units leads to better recall but reduced precision',
                        'Decreasing the granularity of document units increases precision at the cost of recall',
                        'The trade-off between precision and recall depends on the specific needs of the users and the document collection',
                        'Increasing the granularity of document units improves both precision and recall',
                        'Decreasing the granularity of document units decreases precision but improves recall',
                    ],
                    correct: 0,
                    explanation:
                        'The primary trade-off when determining the granularity of document units is that increasing the granularity (e.g., from whole documents to paragraphs) leads to better recall but reduced precision. Smaller units can help to pinpoint relevant information more easily, but the loss of surrounding context can lead to more ambiguous matches.',
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question:
                        'Referring to the exercise on slide 15, how could the frequency of "countrymen" be used to evaluate the best query evaluation order for friends AND romans AND (NOT countrymen)?',
                    options: [
                        'Estimate the size of NOT countrymen by using the total number of documents minus the frequency of countrymen',
                        "Ignore 'countrymen' since it's negated",
                        "Process 'friends' and 'romans' first, then apply the negation of 'countrymen' to the result",
                        "Process 'countrymen' first, then 'friends' and 'romans'",
                        'The frequency of countrymen is irrelevant',
                    ],
                    correct: 0,
                    explanation:
                        "To evaluate the best query evaluation order for 'friends AND romans AND (NOT countrymen)', the frequency of 'countrymen' can be used to estimate the size of the 'NOT countrymen' set by subtracting its frequency from the total number of documents. This allows for a more informed decision in query optimization.",
                },
                {
                    category: 'Indexing & Data Structures',
                    question:
                        'What is the primary data structure used to determine if a query term exists in the vocabulary of an inverted index?',
                    options: [
                        'Trie',
                        'Postings List',
                        'Hash Table',
                        'Binary Search Tree',
                        'Dictionary',
                    ],
                    correct: 4,
                    explanation:
                        'The primary data structure used to determine if a query term exists in the vocabulary of an inverted index is the Dictionary. The dictionary contains all the unique terms in the collection and, for each term, a pointer to its postings list.',
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question:
                        'What is the optimal strategy for processing terms in a Boolean query to minimize the total amount of work?',
                    options: [
                        'Alternate between high and low document frequency terms',
                        'Process terms in order of decreasing document frequency',
                        'Process terms in arbitrary order',
                        'Process terms in order of increasing document frequency',
                        'Always process the term with the largest postings list first',
                    ],
                    correct: 3,
                    explanation:
                        'The optimal strategy for processing terms in a Boolean query is to process them in order of increasing document frequency. This minimizes the total amount of work by ensuring that the intermediate result sets are kept as small as possible at each step of the query evaluation.',
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question:
                        'Consider the query: (A OR B) AND (C OR D). Assume we have the following document frequencies: df(A) = 10, df(B) = 20, df(C) = 5, df(D) = 30. Which of the following processing orders is likely to be MOST efficient, and why?',
                    options: [
                        'Process A, then B, then C, then D individually, and then combine',
                        'A and C first, then B and D',
                        "It doesn't matter; all orders are equally efficient",
                        '(C OR D) first, then (A OR B)',
                        '(A OR B) first, then (C OR D)',
                    ],
                    correct: 3,
                    explanation:
                        "For the query '(A OR B) AND (C OR D)', with the given document frequencies, the most efficient processing order is likely to be '(C OR D) first, then (A OR B)'. While calculating the exact size of an OR is not straightforward, processing the part of the query with generally lower-frequency terms first is a good heuristic for query optimization.",
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question:
                        'What is the main advantage of Boolean retrieval models, particularly for professional searchers?',
                    options: [
                        'Automatic query expansion',
                        'Ease of use for novice users',
                        'Automatic ranking of results',
                        'Precision and control over the retrieved results',
                        'High recall',
                    ],
                    correct: 3,
                    explanation:
                        'The main advantage of Boolean retrieval models, particularly for professional searchers, is the precision and control they provide over the retrieved results. By allowing users to construct complex logical queries, these models enable highly specific and targeted information retrieval.',
                },
                {
                    category: 'Foundational IR Concepts',
                    question:
                        'In the context of information retrieval, what is considered a term when discussing classification and clustering in Chapters 13-18 of the text?',
                    options: [
                        "Language identification uses short character subsequences but doesn't affect tokenization rules",
                        'Specific tokens like email addresses should always be treated as separate terms',
                        'Omitting certain tokens from indexing can still allow effective search capabilities',
                        'A term is defined as a normalized word that must always be included in the dictionary',
                        'In classification and clustering contexts, a term does not need to be included in the dictionary',
                    ],
                    correct: 4,
                    explanation:
                        'In classification and clustering contexts, a term does not need to be included in the dictionary. The features used for these tasks can be defined more broadly and may include things like n-grams or other textual patterns that are not single, normalized words.',
                },
                {
                    category: 'Indexing & Data Structures',
                    question: 'What is the purpose of the Term-Document Incidence Matrix?',
                    options: [
                        'To store the frequency of each term in each document',
                        'To list all the terms in the vocabulary',
                        'To calculate the similarity between documents',
                        'To represent which documents contain which terms (using 0s and 1s)',
                        'To store the positions of terms within documents',
                    ],
                    correct: 3,
                    explanation:
                        'The purpose of the Term-Document Incidence Matrix is to represent which documents contain which terms, typically using 0s and 1s. This provides a simple and clear representation of the term distribution across the collection, but it is not practical for large-scale systems due to its size and sparsity.',
                },
                {
                    category: 'Indexing & Data Structures',
                    question:
                        'What is a major drawback of using only biword indexes for phrase queries (slide 6)?',
                    options: [
                        'They cannot handle phrase queries at all',
                        'They are too slow for practical use',
                        'They only work for two-word phrases',
                        'They require proximity operators',
                        'They can lead to false positives',
                    ],
                    correct: 4,
                    explanation:
                        'A major drawback of using only biword indexes for phrase queries is that they can lead to false positives. While they can confirm that two words appear consecutively, they cannot verify longer phrases, which can result in the retrieval of irrelevant documents.',
                },
                {
                    category: 'Text Processing & Normalization',
                    question:
                        'Why is normalization (equivalence classing of terms) important in Information Retrieval?',
                    options: [
                        'It identifies the language of the documents',
                        'It speeds up the stemming process',
                        'It corrects grammatical errors',
                        'It reduces the size of the documents',
                        'It helps match different forms of the same word (e.g., "USA" and "U.S.A".)',
                    ],
                    correct: 4,
                    explanation:
                        "Normalization is important in Information Retrieval because it helps to match different forms of the same word (e.g., 'USA' and 'U.S.A.'). By treating these variations as equivalent, the system can retrieve a more complete set of relevant documents.",
                },
                {
                    category: 'Indexing & Data Structures',
                    question:
                        'You have a document collection where many documents contain lists of names and associated information (e.g., employee directories). You want to allow users to search for people by name and job title, even if the title appears several words away from the name. Which indexing strategy would be MOST appropriate, and why would other strategies be less effective?',
                    options: [
                        "A simple term frequency index, because it's efficient",
                        'Positional index, because it allows for proximity searches',
                        'Biword index, because names and titles are often two words',
                        "Boolean index with stop word removal, because it's the simplest",
                        'Phrase index, because it handles multi-word units',
                    ],
                    correct: 1,
                    explanation:
                        'For a document collection containing employee directories, where users need to search for people by name and job title that may not be adjacent, a positional index is the most appropriate strategy. It allows for proximity searches, which can find documents where the name and title appear within a certain number of words of each other.',
                },
                {
                    category: 'Text Processing & Normalization',
                    question:
                        'In context-sensitive spelling correction, why is it often necessary to limit the number of alternative query phrases considered?',
                    options: [
                        'Because the edit distance calculation is less accurate for longer phrases',
                        'To prioritize corrections that are phonetically similar to the original query',
                        "To avoid suggesting corrections that are too dissimilar to the user's intent",
                        'Because the number of possible combinations of term corrections can grow very large, making exhaustive search impractical',
                        'To reduce the computational cost of calculating edit distances',
                    ],
                    correct: 3,
                    explanation:
                        'In context-sensitive spelling correction, it is often necessary to limit the number of alternative query phrases considered because the number of possible combinations can grow exponentially, making an exhaustive search computationally impractical. This necessitates the use of pruning and other heuristics to manage the complexity.',
                },
                {
                    category: 'Text Processing & Normalization',
                    question: 'What is "stemming" in the context of IR?',
                    options: [
                        'Removing stop words',
                        'Finding synonyms for words',
                        'Tokenizing text',
                        'Decoding character encodings',
                        'Reducing words to their root or base form (e.g., "running" -> "run")',
                    ],
                    correct: 4,
                    explanation:
                        "'Stemming' is the process of reducing words to their root or base form (e.g., 'running' becomes 'run'). This is a common technique in information retrieval to ensure that different variations of a word are treated as a single concept.",
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question:
                        'Level 4: How does the concept of "term frequency" (tf) contribute to document ranking in many retrieval models?',
                    options: [
                        'tf is only used in the Boolean Model',
                        'tf is irrelevant for ranking',
                        'Higher tf always means higher relevance',
                        'tf is used to normalize document length',
                        'Higher tf generally indicates a stronger relationship between the term and the document',
                    ],
                    correct: 4,
                    explanation:
                        "'Term frequency' (tf) contributes to document ranking because a higher tf generally indicates a stronger relationship between the term and the document. The more times a query term appears in a document, the more likely it is that the document is about that term.",
                },
                {
                    category: 'Text Processing & Normalization',
                    question:
                        'What is a potential trade-off when determining the granularity of document units in an information retrieval system?',
                    options: [
                        'Larger document units reduce the need for semantic processing',
                        'Precision may increase but recall might decrease',
                        'Tokenization becomes unnecessary when using large document units',
                        'Granularity has no impact on search effectiveness',
                        'Smaller document units make indexing more efficient',
                    ],
                    correct: 1,
                    explanation:
                        'A potential trade-off when determining the granularity of document units is that precision may increase but recall might decrease. Larger document units can provide more context, helping to increase precision, but they may also cause relevant but small passages to be overlooked, thus decreasing recall.',
                },
                {
                    category: 'Indexing & Data Structures',
                    question:
                        'What data structure is most suitable for a postings list in an inverted index?',
                    options: [
                        'Variable length array',
                        'Binary search tree',
                        'Singly linked list',
                        'Hash table',
                        'Fixed-length array',
                    ],
                    correct: 2,
                    explanation:
                        'A singly linked list is the most suitable data structure for a postings list in an inverted index. Its dynamic nature allows for efficient insertion of new document IDs as the collection grows, without the overhead of reallocating and copying large blocks of memory that would be required with an array.',
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question:
                        'Which retrieval model treats documents and queries as sets of terms and uses set operations (AND, OR, NOT) to find matching documents?',
                    options: [
                        'Language Model',
                        'Inferred Model',
                        'Vector Space Model',
                        'Probabilistic Retrieval Model',
                        'Boolean Retrieval Model',
                    ],
                    correct: 4,
                    explanation:
                        'The Boolean Retrieval Model is the retrieval model that treats documents and queries as sets of terms and uses set operations (AND, OR, NOT) to find matching documents. It is one of the earliest and simplest models of information retrieval.',
                },
                {
                    category: 'Text Processing & Normalization',
                    question:
                        'When indexing a document containing text in multiple languages, what is the recommended approach for ensuring accurate retrieval and consistency?',
                    options: [
                        'Identify the language used in each segment of the document and apply language-specific tokenization and normalization rules',
                        'Ignore language differences and treat all text as a single language for indexing purposes',
                        'Tokenize the entire document using one writing system and apply a single set of normalization rules',
                        'Apply default heuristics without specific language-based rules',
                        'Use a global Soundex algorithm to convert all text into a common phonetic form',
                    ],
                    correct: 0,
                    explanation:
                        'When indexing a document containing text in multiple languages, the recommended approach is to identify the language of each segment and then apply language-specific tokenization and normalization rules. This ensures that the linguistic characteristics of each language are handled correctly, leading to more accurate retrieval.',
                },
                {
                    category: 'Text Processing & Normalization',
                    question:
                        'You are correcting the misspelled query "appple". You are using a combination of a bigram index and edit distance. You\'ve identified "apple" and "apply" as potential corrections based on bigram overlap. Which of the following statements is MOST accurate in guiding your next step?',
                    options: [
                        "You should reject both 'apple' and 'apply' because they don't share all bigrams with 'appple'",
                        "You should calculate the edit distance to both 'apple' and 'apply' and consider additional factors like term frequency",
                        "You should only consider 'apple' because it shares more bigrams with 'appple'",
                        "'apply' is definitively the correct spelling because it has a lower edit distance",
                        "'apple' is definitively the correct spelling because it has a lower edit distance",
                    ],
                    correct: 1,
                    explanation:
                        "When correcting a misspelled query like 'appple', after identifying potential corrections through bigram overlap, the best next step is to calculate the edit distance to each potential correction and consider additional factors like their term frequency. This combination of evidence helps to make a more informed and accurate correction.",
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question:
                        'In the vector space model, what is the purpose of length normalization?',
                    options: [
                        'To ensure that all vectors have the same length',
                        'To reduce the impact of frequent terms',
                        'To increase the impact of rare terms',
                        'To penalize longer documents',
                        'To give more weight to shorter documents',
                    ],
                    correct: 0,
                    explanation:
                        'The purpose of length normalization in the vector space model is to ensure that all vectors have the same length. This prevents longer documents from having an unfair advantage in similarity calculations simply because they contain more terms.',
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question:
                        'What is the purpose of using log-frequency weighting in information retrieval?',
                    options: [
                        'To normalize the length of documents',
                        'To completely eliminate the impact of term frequency',
                        'To reduce the impact of very high term frequencies, providing a more balanced score',
                        'To convert term frequencies into binary values',
                        'To increase the impact of rare terms in a document',
                    ],
                    correct: 2,
                    explanation:
                        'The purpose of using log-frequency weighting is to reduce the impact of very high term frequencies, which provides a more balanced score. By taking the logarithm of the term frequency, the weight of a term grows more slowly, preventing very frequent terms from dominating the document score.',
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question: 'What is the purpose of inexact top-K retrieval methods?',
                    options: [
                        'To reduce the computational cost of finding the top K documents',
                        'To index documents faster',
                        'To guarantee that the top K documents are always retrieved accurately',
                        'To handle Boolean queries more efficiently',
                        'To improve the precision of the search results',
                    ],
                    correct: 0,
                    explanation:
                        'The purpose of inexact top-K retrieval methods is to reduce the computational cost of finding the top K documents. By using approximations and heuristics, these methods can often find a good set of results much more quickly than methods that require calculating the exact score for every document.',
                },
                {
                    category: 'Information Retrieval Evaluation',
                    question:
                        'In the context of information retrieval evaluation, which scenario best illustrates the trade-off between precision and recall, and how might a system designer address this trade-off based on the specific application?',
                    options: [
                        'A system that indexes all words versus a system that uses a stop list',
                        'A system that uses a large index versus a system that uses a small index',
                        'A system that uses tf-idf weighting versus a system that uses binary term weighting',
                        'A system designed for legal discovery where missing a single relevant document could have severe consequences versus a web search engine where users are primarily concerned with the quality of the first page of results',
                        'A system that uses stemming versus a system that does not use stemming',
                    ],
                    correct: 3,
                    explanation:
                        'The trade-off between precision and recall is best illustrated by a system designed for legal discovery versus a web search engine. In legal discovery, missing a single relevant document can have severe consequences, so high recall is paramount. For a web search engine, users are primarily concerned with the quality of the first page of results, so high precision is more important.',
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question:
                        'Which of the following is a disadvantage of using the "Bag of Words" model in information retrieval?',
                    options: [
                        'It ignores the order and context of words in a document',
                        'It is difficult to calculate term frequencies',
                        'It requires a large amount of storage space',
                        'It is computationally expensive to implement',
                        'It cannot be used with stemming or lemmatization',
                    ],
                    correct: 0,
                    explanation:
                        "A disadvantage of the 'Bag of Words' model is that it ignores the order and context of words in a document. This loss of information can lead to a poorer understanding of the document's meaning and less accurate retrieval.",
                },
                {
                    category: 'Text Processing & Normalization',
                    question:
                        'What is the primary goal of the tokenization process in Information Retrieval?',
                    options: [
                        'To break down text into individual terms',
                        'To create an inverted index',
                        'To calculate term frequencies',
                        'To perform stemming',
                        'To identify stop words',
                    ],
                    correct: 0,
                    explanation:
                        'The primary goal of the tokenization process in Information Retrieval is to break down text into individual terms. This is a foundational step that converts a stream of characters into a sequence of meaningful units that can be further processed.',
                },
                {
                    category: 'Indexing & Data Structures',
                    question: 'What is the "term-document incidence matrix"?',
                    options: [
                        'A tool for query optimization',
                        'A matrix showing the presence or absence of terms in documents',
                        'A method for ranking documents',
                        'A data structure for storing postings lists',
                        'A list of all terms in a collection',
                    ],
                    correct: 1,
                    explanation:
                        "A 'term-document incidence matrix' is a matrix showing the presence or absence of terms in documents. It provides a simple conceptual model for Boolean retrieval but is not practical for large document collections due to its size and sparsity.",
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question:
                        'Level 3: What is a potential drawback of using only the Boolean Retrieval Model?',
                    options: [
                        'It does not support ranked retrieval',
                        'It cannot handle large document collections',
                        'It is difficult to implement',
                        'It is too slow for interactive use',
                        'It requires complex indexing techniques',
                    ],
                    correct: 0,
                    explanation:
                        'A potential drawback of using only the Boolean Retrieval Model is that it does not support ranked retrieval. It returns an unordered set of documents that match the query, which can be problematic for users who want the most relevant results presented first.',
                },
                {
                    category: 'Indexing & Data Structures',
                    question:
                        'What is the main disadvantage of using a Permuterm index for wildcard queries?',
                    options: [
                        'It is slow for queries with wildcards at the beginning of a term',
                        'It is not suitable for large document collections',
                        'It requires a complex merging process',
                        'It cannot handle queries with multiple wildcards',
                        'It significantly increases the size of the dictionary',
                    ],
                    correct: 4,
                    explanation:
                        'The main disadvantage of using a Permuterm index for wildcard queries is that it significantly increases the size of the dictionary. By storing all possible rotations of each term, the index becomes much larger than a standard index, which can have significant storage and performance implications.',
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question: 'What is "query optimization" in the context of Boolean retrieval?',
                    options: [
                        'Rewriting a query in disjunctive normal form',
                        'Selecting the most efficient order to process terms and operations in a query',
                        'Finding the most relevant documents for a query',
                        'Expanding a query with synonyms',
                        'Improving the precision and recall of a query',
                    ],
                    correct: 1,
                    explanation:
                        "'Query optimization' in Boolean retrieval involves selecting the most efficient order to process the terms and operations in a query. The goal is to minimize the amount of computational work required, typically by processing the most restrictive parts of the query first.",
                },
                {
                    category: 'Text Processing & Normalization',
                    question:
                        'When a character is placed on the left side of its square box in a text, what strategy helps ensure that all possible forms of a term are indexed correctly?',
                    options: [
                        'Users should enter hyphens whenever possible to enable generalization',
                        'Users should not use any special characters in their queries',
                        'The system should automatically convert all terms into one-word forms',
                        'Users should avoid entering hyphens',
                        'Indexing should be based on single words only',
                    ],
                    correct: 0,
                    explanation:
                        'The strategy of having users enter hyphens whenever possible to enable generalization suggests a system design where hyphenation is used to create equivalence classes of terms, thus ensuring that all possible forms of a term are indexed correctly.',
                },
                {
                    category: 'Indexing & Data Structures',
                    question: 'Why are skip pointers added to postings lists?',
                    options: [
                        'To store positional information',
                        'To speed up the intersection of postings lists by skipping over blocks of non- matching documents',
                        'To store term frequency information',
                        'To handle phrase queries',
                        'To compress the postings lists',
                    ],
                    correct: 1,
                    explanation:
                        'Skip pointers are added to postings lists to speed up the intersection of these lists by skipping over blocks of non-matching documents. This is a crucial optimization that can significantly improve the performance of AND queries.',
                },
                {
                    category: 'Text Processing & Normalization',
                    question:
                        'What is a potential downside of using asymmetric expansion for term normalization?',
                    options: [
                        'It can lead to less efficient query processing',
                        'It can make the index larger',
                        'It can result in fewer relevant documents being retrieved',
                        'It is not suitable for languages with complex morphology',
                        'It always improves retrieval accuracy',
                    ],
                    correct: 0,
                    explanation:
                        'A potential downside of using asymmetric expansion for term normalization is that it can lead to less efficient query processing. While offering more fine-grained control, the added complexity of the expansion rules can increase the time it takes to execute a query.',
                },
                {
                    category: 'Text Processing & Normalization',
                    question: 'What is the difference between stemming and lemmatization?',
                    options: [
                        'Lemmatization always returns a valid word, while stemming may not',
                        'Stemming uses a vocabulary, while lemmatization does not',
                        'Stemming is more accurate than lemmatization',
                        'Lemmatization removes derivational affixes, while stemming only removes inflectional endings',
                        'Stemming is only used for English, while lemmatization can be used for any language',
                    ],
                    correct: 0,
                    explanation:
                        'The main difference between stemming and lemmatization is that lemmatization always returns a valid word, while stemming may not. Lemmatization uses a dictionary and morphological analysis to find the true root of a word, making it more accurate but often slower than the heuristic-based approach of stemming.',
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question:
                        'Level 1: In the Boolean Retrieval Model, what does the operator "AND" signify between two terms?',
                    options: [
                        'Either term can be present',
                        'Only the second term must be present',
                        'Both terms must be present',
                        'Only the first term must be present',
                        'Neither term should be present',
                    ],
                    correct: 2,
                    explanation:
                        "In the Boolean Retrieval Model, the operator 'AND' signifies that both terms must be present in a document for it to be considered a match. This allows for the construction of precise queries where all specified conditions must be met.",
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question:
                        'Level 5: Why is the "ideal" query processing order not always achievable in practice?',
                    options: [
                        'Users often change their queries',
                        'The cost of merging postings lists can be unpredictable',
                        'The system might not have enough memory',
                        'Document frequencies are not always known in advance',
                        'Postings lists are not always sorted',
                    ],
                    correct: 1,
                    explanation:
                        "The 'ideal' query processing order is not always achievable in practice primarily because the cost of merging postings lists can be unpredictable. Factors such as the distribution of terms and the caching state of the system can make it difficult to determine the absolute best execution plan in advance.",
                },
                {
                    category: 'Text Processing & Normalization',
                    question:
                        'When computing the Jaccard coefficient between a query q and a term t during a postings scan, why do we not need to enumerate all k-grams from t?',
                    options: [
                        'We need the exact string representation of t to compute Jaccard correctly',
                        "The term's length allows us to estimate the number of matching k-grams accurately",
                        'Enumerating all k-grams from t would be too slow and unnecessary',
                        'The frequency of each k-gram in t is required for the calculation',
                        'The length of t is sufficient for accurate computation',
                    ],
                    correct: 2,
                    explanation:
                        'When computing the Jaccard coefficient between a query q and a term t, it is not necessary to enumerate all k-grams from t because it would be too slow and unnecessary. The number of k-grams can often be calculated or estimated much more efficiently, making the similarity computation practical.',
                },
                {
                    category: 'Text Processing & Normalization',
                    question:
                        'What is a potential disadvantage of using stop lists in information retrieval systems?',
                    options: [
                        'They can lead to loss of meaning in phrase queries or specific searches',
                        'They improve search precision by removing semantically nonselective words',
                        'They eliminate the need for semantic content analysis',
                        'They are universally used by all modern web search engines',
                        'They significantly increase the storage requirements for indexing',
                    ],
                    correct: 0,
                    explanation:
                        "Stop lists, which contain common words to be excluded from indexing, can inadvertently remove terms that are crucial for specific queries. For instance, in a query like 'to be or not to be,' all the words are stop words, and removing them would make it impossible to retrieve relevant documents. This harms recall, which is the fraction of relevant documents that are successfully retrieved.",
                },
                {
                    category: 'Text Processing & Normalization',
                    question:
                        'Which of the following tokenization approaches would NOT cause problems in a search engine?',
                    options: [
                        'Ignoring hyphens',
                        'Separating numbers and URLs as metadata',
                        'Using a phrase index for hyphenated terms',
                        'Adding spaces within words',
                        'Splitting on white space',
                    ],
                    correct: 4,
                    explanation:
                        'Splitting on white space is a fundamental and necessary step in tokenizing many languages, including English. While ignoring hyphens or how numbers and URLs are treated can lead to retrieval problems, simply splitting by spaces is the baseline for separating words and does not inherently cause issues.',
                },
                {
                    category: 'Text Processing & Normalization',
                    question: 'Level 3: What is the role of stop words in Information Retrieval?',
                    options: [
                        'They are frequently occurring, often insignificant words that are sometimes removed',
                        'They are the most important words in a query',
                        'They are used to build the inverted index',
                        'They are words that are misspelled',
                        'They are keywords used to categorize documents',
                    ],
                    correct: 0,
                    explanation:
                        "Stop words are words that occur with high frequency in a language but often carry little semantic meaning on their own (e.g., 'the,' 'a,' 'is'). In information retrieval, they are often removed during preprocessing to reduce the size of the index and improve efficiency, as they are generally not useful for retrieval.",
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question: 'Which of the following is the basis for ranked retrieval?',
                    options: [
                        'Randomly ordering the documents',
                        'Retrieving documents that contain all the query terms',
                        'Filtering out stop words from the query',
                        'Retrieving documents that contain at least one of the query terms',
                        'Assigning a score to each document based on its relevance to the query',
                    ],
                    correct: 4,
                    explanation:
                        'Ranked retrieval models move beyond simple Boolean matching (present or absent) and assign a relevance score to each document based on how well it matches the query. This score is then used to rank the documents, presenting the most likely relevant ones to the user first.',
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question: 'What does the Jaccard coefficient measure?',
                    options: [
                        'The similarity between two sets',
                        'The frequency of a term in a document',
                        'The inverse document frequency',
                        'The number of common terms between two documents',
                        'The distance between two documents',
                    ],
                    correct: 0,
                    explanation:
                        'The Jaccard coefficient is a statistic used for gauging the similarity and diversity of sample sets. Specifically, it measures the similarity between two sets by dividing the size of their intersection by the size of their union. In IR, it can be used to compare the set of terms in a query with the set of terms in a document.',
                },
                {
                    category: 'Text Processing & Normalization',
                    question:
                        'What term refers to the process in IR systems where different token forms are considered equivalent, allowing queries to match documents with variations like "USA" and "U.S.A".?',
                    options: [
                        'Stop words inclusion',
                        'Query expansion',
                        'Term normalization',
                        'Token normalization',
                        'Equivalence classing',
                    ],
                    correct: 2,
                    explanation:
                        "Term normalization is the process of grouping different forms of a word so they can be treated as a single item. This allows a query for 'USA' to match documents containing 'U.S.A.' by normalizing both to a common representation.",
                },
                {
                    category: 'Text Processing & Normalization',
                    question:
                        'Level 4 (Hard) 14. What is a key difference between isolated- term and context-sensitive spelling correction?',
                    options: [
                        'Isolated-term correction is more accurate',
                        'Isolated-term correction uses edit distance, while context-sensitive uses k- grams',
                        'Context-sensitive correction considers the surrounding words in the query',
                        'Isolated-term correction is faster',
                        'Context-sensitive correction only works for single-word queries',
                    ],
                    correct: 2,
                    explanation:
                        "Isolated-term correction evaluates a misspelled word on its own, whereas context-sensitive correction considers the surrounding words to determine the most likely intended word. The use of context is the key differentiator, as it helps resolve ambiguity (e.g., 'flew' vs. 'flue' might depend on the words around it).",
                },
                {
                    category: 'Information Retrieval Evaluation',
                    question:
                        'You are evaluating two information retrieval systems. System A has a higher precision but lower recall, while System B has a lower precision but higher recall. If the user values both precision and recall equally, which single metric would be most appropriate to compare the two systems?',
                    options: ['Accuracy', 'Precision', 'F1-score', 'Error Rate', 'Recall'],
                    correct: 2,
                    explanation:
                        "The F1-score is the harmonic mean of precision and recall. It's a useful metric when you want to find a balance between the two, as it gives equal weight to both. A high F1-score indicates that a system has both high precision and high recall.",
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question: 'Why does cos(SaS, PaP) exceed cos(SaS, WH)? Options:',
                    options: [
                        'Because of differences in how normalization is applied between schemes',
                        'Because SaS and PaP are closer to each other compared to SaS and WH',
                        'Because WH uses a different length normalization method',
                        'Because PaP applies idf weighting while WH does not',
                        'Because the term weights under PaP are generally higher than those under WH for matching terms',
                    ],
                    correct: 3,
                    explanation:
                        "In tf-idf weighting, 'idf' (inverse document frequency) gives a higher weight to terms that are rare in the collection. The problem states that PaP (presumably representing a scheme with tf-idf) applies idf weighting while WH (presumably a scheme without it) does not. This application of idf is why the cosine similarity would be higher.",
                },
                {
                    category: 'Indexing & Data Structures',
                    question: 'What does "explicit politeness" refer to in web crawling?',
                    options: [
                        'Avoiding crawling during peak traffic hours',
                        'Prioritizing websites with high page rank',
                        'Specifications from webmasters on what portions of a site can be crawled, often defined in robots.txt',
                        'Crawling only websites that explicitly grant permission',
                        'Maintaining a consistent crawl rate across all websites',
                    ],
                    correct: 2,
                    explanation:
                        "Explicit politeness in web crawling refers to following the rules set out by webmasters in a site's `robots.txt` file. This file specifies which parts of the site crawlers are allowed or disallowed from accessing.",
                },
                {
                    category: 'Indexing & Data Structures',
                    question:
                        'A web crawler discovers two URLs that point to virtually identical content but have different URL structures and are hosted on different domains. Which strategy would be most effective in avoiding duplicate indexing while preserving potentially valuable information?',
                    options: [
                        'Indexing both URLs separately without further analysis',
                        'Using content-based deduplication techniques to identify and merge the content under a single canonical URL',
                        'Ignoring the second URL encountered',
                        'Redirecting users from the second URL to the first URL',
                        'Penalizing both domains in search rankings for hosting duplicate content',
                    ],
                    correct: 1,
                    explanation:
                        'When a crawler finds multiple URLs pointing to identical or nearly identical content, the best practice is to use content-based deduplication. This involves identifying the duplicate content and selecting one URL as the canonical (or primary) version to be indexed, thus avoiding redundant search results.',
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question:
                        'In the context of tf-idf weighting, how does inverse document frequency (IDF) affect the weight of a term?',
                    options: [
                        'It decreases the weight of terms that appear in many documents',
                        'It normalizes the term frequency',
                        'It has no effect on the weight of a term',
                        'It only affects the weight of terms in the query, not in the documents',
                        'It increases the weight of terms that appear in many documents',
                    ],
                    correct: 0,
                    explanation:
                        "Inverse document frequency (IDF) is a measure of how much information a word provides. It is calculated based on the number of documents in a collection that contain the term. Words that appear in many documents (like 'the' or 'a') have a low IDF score, while rare words have a high IDF score. Therefore, IDF decreases the weight of terms that appear in many documents.",
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question:
                        'In ranked retrieval, what is the primary purpose of assigning a score to each document in the collection with respect to a query?',
                    options: [
                        'To identify the language of the document',
                        'To measure the length of the document',
                        'To categorize the documents into predefined topics',
                        'To determine if the document is relevant or not',
                        'To rank-order the documents based on their relevance to the query',
                    ],
                    correct: 4,
                    explanation:
                        'In a ranked retrieval system, the primary goal is not just to find documents that match a query, but to order them by how relevant they are likely to be to the user. Assigning a score to each document based on its relevance to the query is the mechanism by which this ranking is achieved.',
                },
                {
                    category: 'Information Retrieval Evaluation',
                    question:
                        'In evaluating information retrieval systems, what does Mean Average Precision (MAP) primarily measure?',
                    options: [
                        'The average number of relevant documents retrieved per query',
                        'The precision and recall at different recall levels',
                        'The average of the average precisions for each query',
                        'The average precision at a fixed rank K across all queries',
                        'The average precision of the first relevant document retrieved',
                    ],
                    correct: 2,
                    explanation:
                        'Mean Average Precision (MAP) is a popular metric in information retrieval for evaluating the performance of ranked retrieval systems. It is calculated by taking the mean of the average precision scores for each query in a set. Average precision for a single query is the average of the precision scores at each point a relevant document is retrieved in the ranked list.',
                },
                {
                    category: 'Information Retrieval Evaluation',
                    question:
                        'What is the main purpose of using the F measure in information retrieval evaluation?',
                    options: [
                        'To reward systems that retrieve relevant documents quickly',
                        'To penalize systems that retrieve too many documents',
                        'To combine precision and recall into a single metric',
                        'To measure the size of the index',
                        'To measure the speed of the retrieval system',
                    ],
                    correct: 2,
                    explanation:
                        'The F-measure (or F1-score) is a way to combine precision and recall into a single number. It is the harmonic mean of the two, and is useful for situations where you want to balance the trade-off between retrieving all relevant documents (recall) and retrieving only relevant documents (precision).',
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question:
                        'Consider two documents: Document A contains the word "apple" 5 times, and Document B contains the word "apple" 10 times. Without considering any other factors, which document would be ranked higher if using only term frequency (tf) as the ranking factor?',
                    options: [
                        'They would be ranked the same',
                        'It depends on the inverse document frequency of "apple"',
                        'It depends on the length of the documents',
                        'Document A',
                        'Document B',
                    ],
                    correct: 4,
                    explanation:
                        "Term frequency (tf) is the number of times a term appears in a document. If only tf is used for ranking, a document with more occurrences of a term will be ranked higher. Therefore, Document B, with 10 occurrences of 'apple', would be ranked higher than Document A, which has only 5.",
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question:
                        'What is the primary goal of ranked retrieval in information retrieval?',
                    options: [
                        'To order documents by their relevance to a query',
                        'To retrieve documents in alphabetical order',
                        'To retrieve only the most relevant document',
                        'To retrieve all documents in the collection',
                        'To retrieve documents randomly',
                    ],
                    correct: 0,
                    explanation:
                        'The primary goal of ranked retrieval is to present the user with a list of documents ordered by their relevance to the query. This is an improvement over Boolean retrieval, which returns an unordered set of documents that match the query.',
                },
                {
                    category: 'Information Retrieval Evaluation',
                    question:
                        'R-precision adjusts for what factor when evaluating ranked retrieval results?',
                    options: [
                        'The number of documents retrieved',
                        'The size of the document collection',
                        'The number of relevant documents for a query',
                        'The length of the query',
                        'The speed of the retrieval system',
                    ],
                    correct: 2,
                    explanation:
                        'R-precision is a precision metric that is adjusted for the number of relevant documents for a given query. It measures the precision at the R-th position in the ranked results, where R is the total number of relevant documents for that query. This provides a single-figure measure of performance for a query.',
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question:
                        'In the context of scoring for ranked retrieval, what does a score of 0 typically indicate?',
                    options: [
                        'The document is highly relevant to the query',
                        'The document contains all the query terms',
                        'The document is not relevant to the query',
                        "The document's relevance is unknown",
                        'The document is the most relevant in the collection',
                    ],
                    correct: 2,
                    explanation:
                        'In ranked retrieval, a score of 0 typically indicates that the document is not relevant to the query. Documents with higher scores are considered more relevant. A zero score means the document does not meet the criteria for relevance as defined by the scoring function.',
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question:
                        'Which of the following is the biggest problem that ranked retrieval solves compared to boolean retrieval?',
                    options: [
                        'Lack of support for stemming and lemmatization',
                        'Difficulty in understanding complex query languages',
                        'Returning too few or too many results',
                        'Inability to handle large document collections',
                        'Difficulty in assigning scores to documents',
                    ],
                    correct: 2,
                    explanation:
                        'Boolean retrieval can often result in either too few results (if the query is too specific with many ANDs) or too many results (if the query is too broad with many ORs). Ranked retrieval solves this by assigning a relevance score to all documents, allowing the user to see the most relevant ones first, regardless of the total number of matches.',
                },
                {
                    category: 'Information Retrieval Evaluation',
                    question: 'In the context of information retrieval, what is the "F1-score"?',
                    options: [
                        'The number of documents in the collection',
                        'A measure of the storage space used by the index',
                        'The harmonic mean of precision and recall',
                        'A measure of the speed of the retrieval system',
                        'The sum of precision and recall',
                    ],
                    correct: 2,
                    explanation:
                        'The F1-score is the harmonic mean of precision and recall. It provides a single score that balances both metrics, which is useful because precision and recall are often in tension with each other (improving one can lower the other).',
                },
                {
                    category: 'Text Processing & Normalization',
                    question:
                        'In determining the correct tokenization for names like "O\u2019Neil", what is the correct tokenization approach?',
                    options: ['o neill', 'o\u2019neill', 'o\u2019 neill', 'neill', 'oneill'],
                    correct: 1,
                    explanation:
                        "The name 'O'Neil' should be tokenized as 'o'neil' to keep the name as a single semantic unit. Splitting it into 'o' and 'neil' would lose the meaning of the name and likely lead to incorrect search results.",
                },
                {
                    category: 'Indexing & Data Structures',
                    question:
                        "In information retrieval, how are general wildcard queries handled when a search string contains a single '*' symbol not confined to either the beginning or end of the string?",
                    options: [
                        'Leading wildcard query, employing only a reverse B-tree',
                        'Using two B-trees: one normal and one reverse, then taking the intersection of the results',
                        'Combining a prefix tree with an inverted index for efficient lookups',
                        'Trailing wildcard query only, using a standard inverted index',
                        'Building a single B-tree with a special structure to support any kind of wildcard query',
                    ],
                    correct: 1,
                    explanation:
                        "A general wildcard query with a '*' in the middle (e.g., 'ca*t') can be handled by using two B-trees: a normal B-tree for the prefix ('ca') and a reverse B-tree for the suffix ('t'). The results from both searches are then intersected to find the terms that match the full pattern.",
                },
                {
                    category: 'Text Processing & Normalization',
                    question:
                        'During spelling correction, which method is used to retrieve vocabulary terms that have many k-grams in common with the query?',
                    options: [
                        'Relying only on edit distances without incorporating any k-gram analysis',
                        "Employing a fixed k-value without considering sufficient matches from the query's bigrams",
                        'Using a 2-gram index to scan and find terms with at least two bigrams matching the query',
                        'Not utilizing k-gram indexes for spelling correction, instead relying solely on edit distance',
                        "Applying the Jaccard coefficient to measure overlap between the query's and term's k-grams during retrieval",
                    ],
                    correct: 2,
                    explanation:
                        'When dealing with spelling correction, a k-gram (in this case, 2-gram or bigram) index is used to find terms that have a high number of overlapping character subsequences with the misspelled query. This is an efficient way to generate a list of potential correct spellings.',
                },
                {
                    category: 'Indexing & Data Structures',
                    question: 'What is the purpose of an "inverted index" in IR?',
                    options: [
                        'To perform Boolean operations',
                        'To map terms to the documents where they occur',
                        'To rank documents based on relevance',
                        'To optimize query processing speed',
                        'To store documents in a structured format',
                    ],
                    correct: 1,
                    explanation:
                        'An inverted index is a data structure that maps terms to the documents in which they appear. This is the fundamental data structure for efficient full-text search, as it allows the system to quickly find all documents containing a given term without having to scan every document.',
                },
                {
                    category: 'Text Processing & Normalization',
                    question: 'What is "lemmatization"?',
                    options: [
                        'Chopping off the ends of words',
                        'Creating a biword index',
                        'Finding synonyms',
                        'Reducing inflectional forms of a word to its base or dictionary form (lemma)',
                        'Removing stop words',
                    ],
                    correct: 3,
                    explanation:
                        "Lemmatization is the process of reducing a word to its base or dictionary form, known as the lemma. For example, the lemma of 'running', 'ran', and 'runs' is 'run'. This is a more sophisticated form of normalization than stemming, as it takes into account the word's part of speech and uses a vocabulary.",
                },
                {
                    category: 'Text Processing & Normalization',
                    question: 'What is tokenization in the context of Information Retrieval?',
                    options: [
                        'Identifying the language of a document',
                        'Removing punctuation',
                        'Separating words with spaces',
                        'Converting all text to lowercase',
                        'Chopping a character sequence into pieces called tokens',
                    ],
                    correct: 4,
                    explanation:
                        'Tokenization is the process of breaking down a stream of text into smaller units called tokens, which can be words, phrases, symbols, or other meaningful elements. It is one of the first steps in the text processing pipeline for information retrieval.',
                },
                {
                    category: 'Text Processing & Normalization',
                    question:
                        'Which of the following strategies is recommended for handling diacritics in search engine normalization?',
                    options: [
                        'Using truecasing for accurate capitalization decisions',
                        'Removing diacritics because users often omit them when searching',
                        'Equating British and American spellings (e.g., colour/color)',
                        'Keeping all original characters to preserve distinctions',
                        'Converting all letters to lowercase to handle capitalization variations',
                    ],
                    correct: 1,
                    explanation:
                        "Diacritics are marks added to letters to indicate a specific pronunciation. In search, users often omit them. Therefore, a common strategy is to remove diacritics from both the indexed text and the query to ensure that a search for 'ÿ®ÿ≥ŸÖ ÿßŸÑŸÑŸá ÿßŸÑÿ±ÿ≠ŸÖŸÜ ÿßŸÑÿ±ÿ≠ŸäŸÖ' will match documents containing 'ÿ®Ÿêÿ≥ŸíŸÖŸê ÿßŸÑŸÑŸëŸéŸáŸê ÿßŸÑÿ±ŸëŸéÿ≠ŸíŸÖŸéŸ∞ŸÜŸê ÿßŸÑÿ±ŸëŸéÿ≠ŸêŸäŸÖŸê'.",
                },
                {
                    category: 'Information Retrieval Evaluation',
                    question:
                        'You are comparing two stemming algorithms, Stemmer A and Stemmer B. On a test set of 100 queries, Stemmer A improves precision on 20 queries, has no effect on 60 queries, and reduces precision on 20 queries. Stemmer B improves precision on 30 queries, has no effect on 40 queries, and reduces precision on 30 queries. Which stemmer is better overall, and why is this a potentially misleading metric?',
                    options: [
                        'They are equally good because they have the same number of positive and negative effects',
                        'Stemmer A is better because it has fewer negative effects',
                        "Both are bad because they don't improve the majority of queries",
                        'Stemmer B is better because it has more positive effects',
                        "It's impossible to say which is better without knowing the magnitude of the precision changes",
                    ],
                    correct: 4,
                    explanation:
                        'While Stemmer B has more positive effects than Stemmer A (30 vs. 20), it also has more negative effects (30 vs. 20). Without knowing the magnitude of the precision changes (i.e., how much precision was improved or reduced for each query), it is impossible to say which stemmer is better overall. A small number of large improvements could outweigh a large number of small reductions.',
                },
                {
                    category: 'Text Processing & Normalization',
                    question:
                        "Which statement correctly describes a way that asymmetric expansion of query terms can model users' expectations?",
                    options: [
                        'It relies solely on normalizing tokens to remove diacritics and case differences',
                        'It requires more storage space but offers no flexibility in query matching',
                        'It is less efficient than equivalence classing due to increased processing at query time',
                        'It allows for the use of equivalence classes, ensuring identical expansions for all terms',
                        'It ensures that searches for "Windows" match documents about the operating system but not about physical windows',
                    ],
                    correct: 4,
                    explanation:
                        "Asymmetric expansion allows a query to be expanded in a way that models user expectations. For example, a search for 'Windows' can be expanded to include documents about the operating system, but not about physical windows. This is 'asymmetric' because the reverse is not true; a search for 'window' would not typically be expanded to include 'Windows' (the OS).",
                },
                {
                    category: 'Text Processing & Normalization',
                    question:
                        'Level 1: What is the primary function of "Tokenization" in text processing?',
                    options: [
                        'Combining words into phrases',
                        'Removing punctuation',
                        'Sorting words alphabetically',
                        'Splitting text into individual words or terms',
                        'Converting text to lowercase',
                    ],
                    correct: 3,
                    explanation:
                        'Tokenization is the foundational step of breaking down a continuous stream of text into individual units, or tokens (like words or terms), which can then be further processed.',
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question:
                        'In the Vector Space Model, how is the similarity between a document and a query typically calculated?',
                    options: [
                        'Using the Jaccard coefficient of the term sets',
                        'Using the cosine similarity between the vectors',
                        'Using the number of shared terms',
                        'Using the Euclidean distance between the vectors',
                        'Using the Boolean AND operation',
                    ],
                    correct: 1,
                    explanation:
                        'In the Vector Space Model, documents and queries are represented as vectors in a high-dimensional space. The similarity between a document and a query is typically calculated as the cosine of the angle between their respective vectors. A smaller angle (and thus a larger cosine value) indicates greater similarity.',
                },
                {
                    category: 'Indexing & Data Structures',
                    question: 'In the context of dynamic indexing, what is an "auxiliary index"?',
                    options: [
                        'An index used for wildcard queries',
                        'An index used for spell correction',
                        'An index used to store deleted documents',
                        'A small index held in memory to store new or updated documents',
                        'An index that stores term frequencies',
                    ],
                    correct: 3,
                    explanation:
                        'In dynamic indexing, where new documents are constantly being added, a large main index on disk can be slow to update. An auxiliary index is a smaller index kept in memory to store information about new documents. It can be searched quickly and is periodically merged into the main index.',
                },
                {
                    category: 'Indexing & Data Structures',
                    question: 'What is a limitation of using a biword index for phrase queries?',
                    options: [
                        'It cannot handle proximity queries',
                        'It can lead to a very large dictionary size',
                        'It is slower than a positional index',
                        'It requires stemming',
                        'It cannot handle Boolean operators',
                    ],
                    correct: 1,
                    explanation:
                        'A biword index stores pairs of adjacent words. While this is useful for simple phrase queries, the number of possible word pairs (and thus the size of the dictionary) can become very large, especially for large document collections.',
                },
                {
                    category: 'Indexing & Data Structures',
                    question:
                        'What is an advantage of using a positional index, relative to bi-word index?',
                    options: [
                        'It stores term frequencies',
                        'Can support proximity queries and is more flexible in the phrases supported',
                        'It uses a term-document incidence matrix',
                        'It uses skip pointers',
                        'It performs stemming',
                    ],
                    correct: 1,
                    explanation:
                        "A positional index stores the positions of terms within a document. This allows for proximity queries (e.g., finding documents where 'apple' and 'computer' are within five words of each other), which a bi-word index cannot handle. This makes positional indexes more flexible.",
                },
                {
                    category: 'Indexing & Data Structures',
                    question:
                        'Level 2: What is the main advantage of using an Inverted Index over a Term-Document Incidence Matrix for large collections?',
                    options: [
                        'Simpler to implement',
                        'Easier to update',
                        'Supports ranked retrieval directly',
                        'More efficient storage and retrieval',
                        'Better for complex queries',
                    ],
                    correct: 3,
                    explanation:
                        "A Term-Document Incidence Matrix can be very sparse (mostly zeros) for large collections, as most terms do not appear in most documents. An Inverted Index is a more efficient way to store this information, as it only records the '1's (i.e., the documents where a term does appear), leading to significant storage savings and faster retrieval.",
                },
                {
                    category: 'Text Processing & Normalization',
                    question:
                        'Which of the following illustrates a problem caused by splitting tokens on spaces?',
                    options: [
                        'San Francisco-Los Angeles',
                        'Co-education',
                        'Hewlett-Packard',
                        '32.48.231',
                        'Mar 11, 1983',
                    ],
                    correct: 0,
                    explanation:
                        "Tokenizing by splitting on spaces can cause problems for multi-word entities that are not separated by spaces, such as 'San Francisco-Los Angeles'. This would be incorrectly tokenized if only spaces were used as delimiters, whereas hyphenated terms like 'Hewlett-Packard' or dates would be handled differently.",
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question:
                        'When a search engine processes a Boolean combination of wildcard queries like red AND feri, what is the appropriate semantic interpretation?',
                    options: ['Intersection', 'OR of ANDs', 'OR of ANDs', 'XOR', 'AND of ORs'],
                    correct: 4,
                    explanation:
                        "When processing a Boolean combination of wildcard queries, the wildcards are first expanded to generate a set of matching terms from the dictionary. For a query like 'red* AND feri*', the system would find all terms starting with 'red', all terms starting with 'feri', and then perform an AND operation on the resulting sets of documents for each combination. This is effectively an 'AND of ORs'.",
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question:
                        'According to slide 11, what is the primary goal of query optimization in a Boolean retrieval system?',
                    options: [
                        'To reduce the processing time of a query',
                        'To make queries look more complex',
                        'To use more proximity operators',
                        'To find the most relevant documents',
                        'To increase the number of results returned',
                    ],
                    correct: 0,
                    explanation:
                        'Query optimization in a Boolean system aims to reduce the amount of processing time required to answer a query. A key strategy is to process the terms in an order that minimizes the size of intermediate results, for example by processing terms with shorter postings lists first.',
                },
                {
                    category: 'Text Processing & Normalization',
                    question:
                        'Level 2: Which of the following best describes "Normalization" in text processing?',
                    options: [
                        'Splitting text into tokens',
                        'Converting words to a common form (e.g., lowercase)',
                        'Identifying the root form of words',
                        'Removing stop words',
                        'Calculating term weights',
                    ],
                    correct: 1,
                    explanation:
                        "Normalization in text processing refers to converting words to a common form. The most basic form of this is case-folding, where all letters are converted to lowercase so that, for example, 'Apple' and 'apple' are treated as the same word.",
                },
                {
                    category: 'Text Processing & Normalization',
                    question:
                        'How many of the five possible tokenizations of Mr. O\u2019Neill\u2019s name would result in a match when querying for "o\u2019neill and capital"?',
                    options: ['1', '5', '2', '4', '3'],
                    correct: 4,
                    explanation:
                        "Assuming the five tokenizations are 'o'neill', 'oneill', 'o' 'neill', 'o'neil', and 'o'neill's', and the query is for 'o'neill and capital', only the first tokenization would match. Without the context of the slide referenced, a definitive answer is difficult. However, the provided correct answer is 3, which implies that three of the five possible tokenizations would result in a match.",
                },
                {
                    category: 'Text Processing & Normalization',
                    question: 'What is "linguistic preprocessing" in IR?',
                    options: [
                        'Creating the dictionary of an inverted index',
                        'Merging postings lists',
                        'Applying operations like stemming and stop word removal to tokens',
                        'Sorting postings lists',
                        'Evaluating the effectiveness of a retrieval system',
                    ],
                    correct: 2,
                    explanation:
                        'Linguistic preprocessing refers to the steps taken to prepare text for indexing and retrieval. This includes operations like stemming (reducing words to their root form) and stop word removal (excluding common, low-information words).',
                },
                {
                    category: 'Text Processing & Normalization',
                    question:
                        'Which of the following scenarios best illustrates the need for sophisticated language-specific tokenization?',
                    options: [
                        'A document containing both English and French text',
                        'A document containing German compound nouns like "Donaudampfschifffahrtsgesellschaftskapit√§n"',
                        'A document where capitalization is inconsistent',
                        'A document with hyphenated words like "state-of-the-art"',
                        'A document with numerical dates in different formats',
                    ],
                    correct: 1,
                    explanation:
                        "Languages like German have many compound nouns (e.g., 'Donaudampfschifffahrtsgesellschaftskapit√§n', which means 'Danube steamship company captain'). Without a language-specific tokenizer that can split these compounds into their constituent parts, it would be difficult to match queries for the individual components.",
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question:
                        'What is the main difference between Boolean retrieval and ranked retrieval?',
                    options: [
                        'Boolean retrieval returns an unordered set of documents that match the query, while ranked retrieval returns an ordered list of documents based on relevance',
                        'Boolean is used in small collection and ranked is used in web search',
                        'Boolean retrieval can only handle AND queries while ranked can handle OR',
                        'Boolean retrieval is faster than ranked retrieval',
                        "Boolean retrieval uses stemming while ranked retrieval doesn't",
                    ],
                    correct: 0,
                    explanation:
                        "The primary difference is that Boolean retrieval returns an unordered set of documents that exactly match the query logic (e.g., 'cat AND dog'), while ranked retrieval returns an ordered list of documents based on their calculated relevance to the query terms.",
                },
                {
                    category: 'Indexing & Data Structures',
                    question:
                        'Level 1: Which data structure is commonly used for efficient retrieval in Information Retrieval systems?',
                    options: [
                        'Binary Tree',
                        'Linked List',
                        'Inverted Index',
                        'Hash Table',
                        'Array',
                    ],
                    correct: 2,
                    explanation:
                        'An inverted index is the most common and efficient data structure for information retrieval. It maps terms to the documents that contain them, allowing for fast lookup of which documents are relevant to a query.',
                },
                {
                    category: 'Text Processing & Normalization',
                    question: 'What is the main difference between stemming and lemmatization?',
                    options: [
                        'Stemming uses a vocabulary and morphological analysis, while lemmatization uses crude heuristic process that chops off the ends of words',
                        'There is no real difference between them',
                        'Stemming is more accurate than lemmatization',
                        'Stemming is faster and more heuristic than lemmatization',
                        'Lemmatization is more commonly used in information retrieval',
                    ],
                    correct: 3,
                    explanation:
                        'Stemming is a heuristic process that chops off the ends of words to reduce them to a common form, which can sometimes result in non-words. Lemmatization, on the other hand, uses a vocabulary and morphological analysis to return the base or dictionary form of a word. Stemming is generally faster but less accurate.',
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question: 'What is the "document frequency" of a term?',
                    options: [
                        'The number of documents in the collection that contain the term',
                        'The average number of times the term appears across all documents',
                        'The length of the postings list for the term',
                        'The total number of times the term appears in the collection',
                        'The number of times the term appears in a specific document',
                    ],
                    correct: 0,
                    explanation:
                        'The document frequency of a term is the number of documents in a collection that contain that term. It is a key component in calculating the inverse document frequency (IDF) for tf-idf weighting.',
                },
                {
                    category: 'Indexing & Data Structures',
                    question:
                        'Does using an incidence matrix improve query speed compared to grepping through text?',
                    options: [
                        'Yes, but only for very small collections of documents',
                        'No, because modern computers can handle linear scans as fast as any indexed search',
                        'Yes, because indexing allows faster lookups without scanning each document',
                        'No, because building the incidence matrix requires significant preprocessing time',
                        'No, because the incidence matrix is less flexible than grepping for complex queries',
                    ],
                    correct: 2,
                    explanation:
                        'Grepping (linearly scanning) through text is slow, especially for large document collections. An incidence matrix (and more efficiently, an inverted index) allows for direct lookup of the documents containing a query term, which is much faster than scanning every document.',
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question:
                        'Which of the following best describes the "Bag of Words" model in information retrieval?',
                    options: [
                        'A model that represents a document as an unordered collection of words, disregarding grammar and word order',
                        'A model that considers the order of words in a document',
                        'A model that uses a thesaurus to expand the query with synonyms',
                        'A model that assigns equal weight to all words in a document',
                        'A model that only considers the first 100 words of a document',
                    ],
                    correct: 0,
                    explanation:
                        'The Bag of Words (BoW) model represents text as an unordered collection of words, disregarding grammar and word order but keeping track of frequency. This simplifies the representation of a document to a vector of word counts.',
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question:
                        'In the context of term weighting, what is the purpose of logarithmic weighting?',
                    options: [
                        'To ensure that all terms have equal weights',
                        'To normalize the length of documents',
                        'To increase the weight of rare terms',
                        'To reduce the impact of very high term frequencies',
                        'To eliminate stop words from the index',
                    ],
                    correct: 3,
                    explanation:
                        'Logarithmic weighting is used to dampen the effect of high term frequencies. The intuition is that the difference in relevance between a term appearing 1 time and 10 times is more significant than the difference between it appearing 100 times and 110 times. Taking the logarithm of the term frequency achieves this dampening effect.',
                },
                {
                    category: 'Text Processing & Normalization',
                    question:
                        'In the context of spelling correction, what does "isolated-term correction" mean?',
                    options: [
                        'Correcting multiple terms in a query simultaneously',
                        'Correcting spelling errors based on the surrounding context',
                        'Using phonetic similarity to correct spelling errors',
                        'Correcting a single query term at a time',
                        'Using only the edit distance for correction',
                    ],
                    correct: 3,
                    explanation:
                        'Isolated-term correction refers to the process of correcting a single misspelled word at a time, without considering the context of the surrounding words in the query. This is a simpler but less accurate method than context-sensitive correction.',
                },
                {
                    category: 'Text Processing & Normalization',
                    question:
                        'When determining the appropriate size of document units for an information retrieval system, what is a key consequence of the precision vs. recall trade-off?',
                    options: [
                        'There are no issues with spurious or irrelevant matches',
                        'Index granularity becomes a significant concern',
                        'Users will only find relevant documents without any false matches',
                        'It does not affect retrieval performance',
                        'The system may miss important passages while increasing the likelihood of spurious matches',
                    ],
                    correct: 4,
                    explanation:
                        'The size of the document units used for indexing (e.g., individual sentences, paragraphs, or whole documents) involves a trade-off. Smaller units can lead to more precise matches but may miss the broader context (lower recall). Larger units can improve recall but may also lead to more spurious or irrelevant matches (lower precision).',
                },
                {
                    category: 'Text Processing & Normalization',
                    question: 'How does the Soundex algorithm handle similar-sounding consonants?',
                    options: [
                        'It treats them as vowels',
                        'It maps them to different digits',
                        'It removes them from the term',
                        'It maps them to the same digit based on phonetic equivalence classes',
                        'It ignores them',
                    ],
                    correct: 3,
                    explanation:
                        "The Soundex algorithm is a phonetic algorithm for indexing names by their sound, as pronounced in English. It maps similar-sounding consonants to the same digit, so that, for example, 'Robert' and 'Rupert' would have the same Soundex code.",
                },
                {
                    category: 'Indexing & Data Structures',
                    question: 'Why is a biword index not the standard solution for indexing terms?',
                    options: [
                        'Using a biword index might seem efficient but can lead to misses due to the need for more precise patterns',
                        'A biword index is better than a positional index because it allows for exhaustive phrase indexing',
                        'A biword index is sufficient because it effectively handles all relevant term searches',
                        'A biword index cannot handle phrases longer than two words, making it less practical',
                        'A biword index requires the use of part-of-speech taggers to improve accuracy but is often used in combination with other methods',
                    ],
                    correct: 4,
                    explanation:
                        'A biword index stores pairs of adjacent words. This is not a standard solution for indexing because it cannot effectively handle all relevant term searches, such as those for single words, phrases longer than two words, or queries where the terms are not adjacent.',
                },
                {
                    category: 'Indexing & Data Structures',
                    question:
                        'Which of the following is NOT a typical step in inverted index construction?',
                    options: [
                        'Index the documents that each term occurs in',
                        'Collect the documents to be indexed',
                        'Perform word segmentation',
                        'Do linguistic preprocessing of tokens',
                        'Tokenize the text',
                    ],
                    correct: 2,
                    explanation:
                        'Word segmentation is the process of dividing a string of written language into its component words. In languages like Chinese or Japanese that do not use spaces between words, this is a non-trivial task. The other options listed are either subsequent steps (linguistic preprocessing) or the inputs to the process (collecting documents, tokenizing).',
                },
                {
                    category: 'Indexing & Data Structures',
                    question:
                        'What is a potential drawback of using a biword index for very long phrases?',
                    options: [
                        'It is less accurate than a positional index',
                        'It cannot handle Boolean queries',
                        'It cannot handle proximity queries',
                        'It can lead to a very large vocabulary size',
                        'It is computationally expensive to build',
                    ],
                    correct: 3,
                    explanation:
                        'A biword index contains entries for every pair of adjacent words in the document collection. For very long phrases, the number of such pairs can be large, leading to a significant increase in the size of the vocabulary that needs to be stored.',
                },
                {
                    category: 'Indexing & Data Structures',
                    question:
                        'In a system using both a standard inverted index and a permuterm index, what is the primary role of the permuterm index in processing wildcard queries?',
                    options: [
                        'To store the postings lists for all terms',
                        'To perform stemming and other normalization tasks',
                        'To calculate the edit distance between the query and vocabulary terms',
                        'To directly retrieve documents matching the wildcard query',
                        'To identify the vocabulary terms that potentially match the wildcard pattern, before using the standard index',
                    ],
                    correct: 4,
                    explanation:
                        'In a system with both a standard inverted index and a permuterm index, the permuterm index (which stores all possible rotations of a term) is used to efficiently identify the vocabulary terms that could potentially match a wildcard query. Once this set of candidate terms is found, the standard inverted index can be used to retrieve the corresponding documents.',
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question:
                        'Why is raw term frequency (tf) not ideal for directly computing query-document match scores?',
                    options: [
                        'It only considers the presence or absence of a term, not its frequency',
                        'It is difficult to calculate for large documents',
                        'It gives too much weight to frequently occurring terms, disproportionately increasing the score of documents with many occurrences of the term',
                        'It is only applicable to Boolean retrieval models',
                        "It doesn't account for the length of the document",
                    ],
                    correct: 2,
                    explanation:
                        'Raw term frequency (tf) gives disproportionately high scores to documents where a term appears many times, without considering that the relevance does not necessarily increase linearly with the term frequency. This is why tf is often dampened, for example by taking its logarithm.',
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question:
                        'Which of the following is a characteristic of ranked retrieval models?',
                    options: [
                        'Results are limited to a small set of documents',
                        'Users must use a specific query language',
                        'The system returns an ordering over the documents in the collection with respect to a query',
                        'Documents are either a match or not',
                        'Queries are Boolean expressions',
                    ],
                    correct: 2,
                    explanation:
                        'A key characteristic of ranked retrieval models is that they return an ordering of the documents in the collection with respect to a query, based on a calculated score of relevance. This contrasts with Boolean models which return an unordered set of matching documents.',
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question:
                        'You are building a search engine for a large collection of research papers. Many papers use similar terminology, but the relevance of a paper often depends on a specific combination of terms. Which weighting scheme would likely provide the BEST results for ranking documents by relevance?',
                    options: [
                        'Boolean weighting (term present or absent)',
                        'Inverse document frequency alone',
                        'TF-IDF weighting with cosine normalization, incorporating term proximity',
                        'Raw term frequency',
                        'A weighting scheme that ignores document length',
                    ],
                    correct: 2,
                    explanation:
                        'For research papers, the specific combination of terms is often important for relevance. TF-IDF weighting with cosine normalization is a powerful scheme that considers term frequency within a document (TF), the rarity of the term across the collection (IDF), and normalizes for document length. Incorporating term proximity further refines the relevance score by considering how close the query terms are to each other in the document.',
                },
                {
                    category: 'Text Processing & Normalization',
                    question:
                        'What does edit distance describe in the context of spelling correction?',
                    options: [
                        'A matrix used for ranking documents based on term frequency',
                        'The process of adjusting search results based on contextual information rather than exact matches',
                        'The minimum number of insertions, deletions, or replacements needed to transform one string into another',
                        'A specialized distance measure that considers the likelihood of character substitutions',
                        'The practice of correcting a query by replacing individual characters with more likely alternatives',
                    ],
                    correct: 2,
                    explanation:
                        'Edit distance (specifically, Levenshtein distance) is the minimum number of single-character edits (insertions, deletions, or substitutions) required to change one word into the other. It is a common metric for quantifying the similarity between two strings in spelling correction.',
                },
                {
                    category: 'Text Processing & Normalization',
                    question:
                        'How can asymmetric expansion of query terms be usedfully model users\u2019 expectations?',
                    options: [
                        'Indexing unnormalized tokens and maintaining a query expansion list are both methods that require more processing at query time compared to equivalence classing',
                        'Normalization can be used to remove diacritics and make tokens consistent, which can help in matching similar terms regardless of case',
                        'Query expansion lists can include multiple vocabulary entries to consider for a certain query term, effectively creating a disjunction of several postings lists',
                        'The best amount of equivalence classing or query expansion is a fairly open Question. Doing some seems a good idea but can easily have unintended consequences',
                        'With modern storage costs, the increased flexibility from distinct postings lists is appealing because it allows for asymmetry in query expansion',
                    ],
                    correct: 2,
                    explanation:
                        "Query expansion lists allow for more flexible and context-sensitive searching. By including multiple vocabulary entries for a single query term, the system can effectively create a disjunction (an OR operation) of several postings lists, allowing for a broader and more nuanced search that can better match a user's expectations.",
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question: 'How does the Vector Space Model handle term weighting?',
                    options: [
                        'Terms are weighted based on their frequency in the document and/or the collection (e.g., TF-IDF)',
                        "It doesn't; all terms are treated equally",
                        'Terms are weighted based on their grammatical role',
                        'Terms are weighted based on their frequency in the query',
                        'Terms are weighted based on their position in the document',
                    ],
                    correct: 0,
                    explanation:
                        'In the Vector Space Model, terms are weighted based on their frequency in the document and/or the collection. The most common weighting scheme is TF-IDF, which combines the term frequency (TF) within a document with the inverse document frequency (IDF) across the collection to assign a weight to each term in the document vector.',
                },
                {
                    category: 'Text Processing & Normalization',
                    question:
                        'Which of the following is NOT a typical step in pre-processing text for Information Retrieval?',
                    options: [
                        'Stemming',
                        'Translation',
                        'Stop word removal',
                        'Tokenization',
                        'Normalization',
                    ],
                    correct: 1,
                    explanation:
                        'Translation is not a typical step in preprocessing text for information retrieval. The standard steps include tokenization, normalization (like case-folding), stop word removal, and stemming or lemmatization, all of which are designed to prepare the text for indexing in its original language.',
                },
                {
                    category: 'Indexing & Data Structures',
                    question:
                        'What is the main purpose of the "Map" phase in the MapReduce framework for distributed indexing?',
                    options: [
                        'To sort the term-document pairs',
                        'To distribute the document collection into splits and generate (term, docID) pairs',
                        'To merge postings lists from different machines',
                        'To write the final index to disk',
                        'To assign tasks to worker machines',
                    ],
                    correct: 1,
                    explanation:
                        "In the MapReduce framework for distributed indexing, the 'Map' phase is responsible for taking the input document collection, splitting it into chunks, and having multiple 'mapper' nodes process these chunks in parallel. The mappers parse the documents and generate (term, docID) pairs.",
                },
                {
                    category: 'Text Processing & Normalization',
                    question: 'What is a "stop word" in the context of Information Retrieval?',
                    options: [
                        'A word that has been stemmed or lemmatized',
                        'A word that is crucial for understanding the meaning of a document',
                        'A word that is very frequent and considered to have little value in selecting documents',
                        'A word that appears only once in a document collection',
                        'A word that is used to connect other words in a phrase',
                    ],
                    correct: 2,
                    explanation:
                        "A stop word is a very frequent word (like 'the', 'a', 'in') that is considered to have little value in selecting documents for retrieval. These words are often removed from queries and documents before indexing to save space and processing time.",
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question: 'What is the purpose of query optimization in Boolean retrieval?',
                    options: [
                        'To expand the query with synonyms',
                        'To make the query look nicer',
                        'To find the most relevant documents',
                        'To reduce the size of the postings lists',
                        'To determine the best order for processing query terms',
                    ],
                    correct: 4,
                    explanation:
                        'In Boolean retrieval, the order in which query terms are processed can significantly impact efficiency. Query optimization involves determining the best order to process the terms, typically by processing terms with the smallest number of matching documents (i.e., the shortest postings lists) first to minimize the size of intermediate result sets.',
                },
                {
                    category: 'Text Processing & Normalization',
                    question:
                        'Why might you want to index different parts of an email message (e.g., body, attachments) as separate documents?',
                    options: [
                        'To allow users to search specifically within those parts, increasing precision',
                        'To improve the accuracy of stemming',
                        "To make it easier to identify the email's language",
                        'To reduce the overall index size',
                        'To automatically classify the email as spam or not spam',
                    ],
                    correct: 0,
                    explanation:
                        "Indexing different parts of an email (body, subject, attachments) as separate 'zones' or documents allows users to perform more precise searches. For example, a user could search for a term specifically in the subject line, which can greatly increase the precision of the search results.",
                },
                {
                    category: 'Foundational IR Concepts',
                    question:
                        'Level 1: What is "Information Need" in the context of Information Retrieval?',
                    options: [
                        "The user's underlying topic of interest",
                        'The relevant documents retrieved by the system',
                        'The query typed by the user',
                        'The set of all documents in the collection',
                        'The algorithm used for ranking',
                    ],
                    correct: 0,
                    explanation:
                        "An 'Information Need' is the underlying topic or question that a user wants to find information about. It is distinct from the 'query', which is the specific string of words the user types into the search engine to try to satisfy that need.",
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question:
                        'Level 4: Why is it important to consider document length when designing a retrieval model?',
                    options: [
                        'Longer documents are always more relevant',
                        'Document length is irrelevant to retrieval',
                        'Shorter documents are easier to process',
                        'Document length is used to calculate precision',
                        'Longer documents tend to have higher term frequencies, which can skew results',
                    ],
                    correct: 4,
                    explanation:
                        'Document length is an important consideration because longer documents naturally tend to have higher term frequencies and a greater variety of terms, which can skew relevance scores. Retrieval models often incorporate some form of length normalization to counteract this bias.',
                },
                {
                    category: 'Indexing & Data Structures',
                    question:
                        'Which of the following statements describes a correct technique for handling general wildcard queries like "se*mon"?',
                    options: [
                        'Create a special inverted index specifically designed for wildcard queries',
                        "It's impossible to handle general wildcard queries without creating a new type of inverted index",
                        'Another approach not mentioned in the context is used to handle general wildcard queries',
                        'Use a term-document incidence matrix to combine prefix and suffix searches',
                        'To find all terms starting with "se" and ending with "mon", an efficient approach is to use a B-tree for the prefix and a reverse B-tree for the suffix, then intersect the results',
                    ],
                    correct: 4,
                    explanation:
                        "For a general wildcard query like 'se*mon', an efficient approach is to use a B-tree to find all terms starting with the prefix 'se' and a reverse B-tree to find all terms ending with the suffix 'mon'. The intersection of these two sets of terms will give the desired results.",
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question:
                        'What is the main advantage of using cosine similarity over Euclidean distance in the vector space model?',
                    options: [
                        'Cosine similarity is not affected by document length',
                        'Cosine similarity provides a distance measure',
                        'Cosine similarity is more sensitive to differences in term frequency',
                        'Cosine similarity is faster to compute',
                        'Cosine similarity can handle negative term weights',
                    ],
                    correct: 0,
                    explanation:
                        'A key advantage of cosine similarity is that it is not affected by document length. It measures the angle between document vectors, so even if one document is much longer than another, if they are about the same topic (and thus point in a similar direction in the vector space), they can still have a high cosine similarity.',
                },
                {
                    category: 'Text Processing & Normalization',
                    question: 'What is a "stop list" in the context of Information Retrieval?',
                    options: [
                        'A list of documents that should not be indexed',
                        'A list of all the tokens in a document',
                        'A list of misspelled words',
                        'A list of common words excluded from the index',
                        'A list of terms used for query expansion',
                    ],
                    correct: 3,
                    explanation:
                        "A 'stop list' is a list of common words (e.g., 'the', 'is', 'at') that are excluded from the index. This is done to reduce the size of the index and improve query processing speed, as these words are generally not discriminative for retrieval.",
                },
                {
                    category: 'Indexing & Data Structures',
                    question:
                        "You're building a search engine for a language with a very large character set and frequent spelling variations due to different romanization schemes (like Chinese). Which combination of techniques would likely be MOST robust for handling both spelling errors AND wildcard queries in this scenario?",
                    options: [
                        'Soundex and a standard inverted index',
                        'Edit distance with a B-tree',
                        'Permuterm index and edit distance',
                        "A combination of a modified Soundex algorithm (adapted for the language's phonetics), a k-gram index for wildcard support, and edit distance calculations (using a language-specific weighting scheme) for refined spelling correction",
                        'k-gram index with post-filtering and a permuterm index',
                    ],
                    correct: 3,
                    explanation:
                        "For a language like Chinese with a large character set and many romanization schemes, a robust approach would combine multiple techniques. A modified Soundex algorithm adapted for the language's phonetics can handle pronunciation variations. A k-gram index is effective for wildcard queries. And language-specific weighted edit distance calculations can refine spelling correction by considering the likelihood of different types of errors.",
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question:
                        'Why is document frequency (DF) important in some term weighting schemes (like TF-IDF)?',
                    options: [
                        'DF is not used in term weighting',
                        'High DF indicates a term is likely to be a stop word',
                        'Low DF indicates a term is more discriminative',
                        'High DF indicates a term is more discriminative',
                        'DF is only used in Boolean retrieval',
                    ],
                    correct: 2,
                    explanation:
                        'Document frequency (DF) is a measure of how common a term is across the entire collection. In weighting schemes like TF-IDF, a low DF indicates that a term is rare and therefore more discriminative and important for retrieval, so it is given a higher weight.',
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question:
                        'Level 4: What is an advantage of the Probabilistic Retrieval Model over the Boolean Model?',
                    options: [
                        "It's simpler to implement",
                        "It doesn't require an inverted index",
                        "It's more efficient for very large collections",
                        'It guarantees retrieval of all relevant documents',
                        'It provides a ranked list of results based on probability of relevance',
                    ],
                    correct: 4,
                    explanation:
                        'The main advantage of a Probabilistic Retrieval Model over the Boolean Model is that it provides a ranked list of results based on the estimated probability of relevance. This is a significant improvement over the unranked set of documents returned by the Boolean Model.',
                },
                {
                    category: 'Text Processing & Normalization',
                    question:
                        'When dealing with text processing for languages like French or German, which action would improve search accuracy by better handling hyphens and compound terms?',
                    options: [
                        'Use a compound-splitter module to break down compound nouns into separate words',
                        'Perform word segmentation on East Asian languages written without spaces',
                        'Ignore hyphens and treat them as single words',
                        'Use apostrophes only for English contractions',
                        'The system automatically generalizes all forms (e.g., "over-eager" to "over eager" or "overeager")',
                    ],
                    correct: 4,
                    explanation:
                        "Languages like French and German often have hyphenated words and compound nouns. A system that can automatically generalize different forms (e.g., treating 'over-eager', 'over eager', and 'overeager' as equivalent) will provide better search accuracy by matching these variations.",
                },
                {
                    category: 'Indexing & Data Structures',
                    question:
                        'How can we modify the k-gram index approach to better handle wildcard queries like "red*"?',
                    options: [
                        'Expand the wildcard in the k-gram index by treating it as a new k-gram',
                        'Combine multiple 3-grams from the same word, such as "ing" and "ion" to match "ingion"',
                        'Issue a prefix-based query in the k-gram index instead of combining two 3- grams',
                        'Modify the inverted index to expand wildcards during retrieval',
                        'Use a range query on the term lengths to find terms of specific lengths starting with "red"',
                    ],
                    correct: 2,
                    explanation:
                        "For a trailing wildcard query like 'red*', simply issuing a prefix-based query in the k-gram index is more direct and efficient than combining multiple n-grams. The index can be structured to handle prefix searches effectively.",
                },
                {
                    category: 'Text Processing & Normalization',
                    question:
                        'Consider a scenario where users often search for product names that are frequently misspelled, and these misspellings are often not simple typographical errors (e.g., "Nikkon cammera" for "Nikon camera"). Furthermore, users sometimes use wildcard queries to find variations of these product names. Which retrieval model enhancement would be LEAST directly helpful in this situation?',
                    options: [
                        'Using a query expansion technique based on synonyms and related terms',
                        'Incorporating edit distance calculations with a low threshold for suggesting corrections',
                        'Implementing a phonetic matching algorithm like Soundex',
                        'Using a k-gram index to handle wildcard queries',
                        'Building a custom spelling correction module trained on common misspellings of product names',
                    ],
                    correct: 0,
                    explanation:
                        'Query expansion based on synonyms and related terms would be the least directly helpful in this scenario. The primary problems are misspellings and the need for wildcard searches. Therefore, techniques like edit distance, phonetic matching (Soundex), and k-gram indexing are the most relevant enhancements.',
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question:
                        'In the context of Boolean query processing, what is the standard heuristic for processing terms in a conjunctive query (AND of terms)?',
                    options: [
                        'In order of increasing document frequency',
                        'In alphabetical order',
                        'In random order',
                        'Based on the length of the terms',
                        'In order of decreasing document frequency',
                    ],
                    correct: 0,
                    explanation:
                        'In a conjunctive (AND) query, the size of the intermediate result set can be minimized by processing the terms in order of increasing document frequency. This is because the intersection of two postings lists can be no larger than the smaller of the two lists.',
                },
                {
                    category: 'Indexing & Data Structures',
                    question:
                        'What is the main advantage of using an inverted index for information retrieval?',
                    options: [
                        'It automatically translates queries into different languages',
                        'It reduces the storage space required for documents',
                        'It provides a graphical representation of the document collection',
                        'It allows for faster query processing',
                        'It improves the ranking of search results',
                    ],
                    correct: 3,
                    explanation:
                        'The main advantage of an inverted index is that it allows for significantly faster query processing compared to serially scanning the text of every document. It provides a direct mapping from terms to the documents that contain them.',
                },
                {
                    category: 'Text Processing & Normalization',
                    question: 'What is the main goal of stemming and lemmatization?',
                    options: [
                        'To automatically correct spelling errors in queries',
                        'To expand the vocabulary of terms',
                        'To reduce words to their root form',
                        'To translate documents into different languages',
                        'To improve the visual presentation of search results',
                    ],
                    correct: 2,
                    explanation:
                        'The main goal of both stemming and lemmatization is to reduce inflectional forms and sometimes derivationally related forms of a word to a common base form. This allows for more comprehensive matching of query terms to document terms.',
                },
                {
                    category: 'Text Processing & Normalization',
                    question: 'What is a potential disadvantage of stemming?',
                    options: [
                        'It only works for English',
                        'It can conflate words with different meanings (over-stemming)',
                        'It slows down query processing',
                        'It increases the size of the index',
                        'It makes it harder to find exact matches',
                    ],
                    correct: 1,
                    explanation:
                        "A potential disadvantage of stemming is over-stemming, where words with different meanings are conflated to the same root. For example, 'university' and 'universe' might both be stemmed to 'univers', leading to incorrect matches.",
                },
                {
                    category: 'Indexing & Data Structures',
                    question:
                        "Which data structure is most suitable for handling wildcard queries with '*' at the beginning of a term (e.g., *tion)?",
                    options: [
                        'A standard B-tree',
                        'A compressed postings list',
                        'A suffix tree',
                        'A reverse B-tree',
                        'A hashtable',
                    ],
                    correct: 3,
                    explanation:
                        "For wildcard queries with a leading asterisk (e.g., '*tion'), a standard B-tree is not efficient. A reverse B-tree, which indexes the reversed terms, is the most suitable data structure. The query would be reversed to 'noit*' and processed as a standard trailing wildcard query.",
                },
                {
                    category: 'Indexing & Data Structures',
                    question:
                        'Why might a combination of biword indexes and positional indexes (as mentioned on slide 16) be a more practical approach for a large-scale search engine than using only a positional index?',
                    options: [
                        'Biword indexes are easier to update',
                        'Biword indexes are always more accurate than positional indexes',
                        'A combined approach can balance query speed and index size',
                        'Positional indexes are too slow for large datasets',
                        'Positional indexes cannot handle phrase queries',
                    ],
                    correct: 2,
                    explanation:
                        'Using only a positional index for a large-scale search engine can lead to a very large index size. A combined approach that uses biword indexes for common phrases and positional indexes for more complex proximity queries can offer a good balance between query speed and the size of the index.',
                },
                {
                    category: 'Text Processing & Normalization',
                    question:
                        'Why is token normalization beneficial for information retrieval systems?',
                    options: [
                        'Reduces the need for manually creating synonym lists',
                        'Increases query processing time by allowing for implicit synonym matching',
                        'Makes it easier to handle variations in tokenization without manual intervention',
                        'Reduces storage requirements for term lists',
                        'All of the above',
                    ],
                    correct: 4,
                    explanation:
                        'Token normalization is beneficial for all the reasons listed: it reduces the need for manual synonym lists by programmatically handling variations, it can improve query processing by allowing for implicit matching, it simplifies handling of tokenization variations, and it can reduce storage requirements by mapping multiple forms to a single representation.',
                },
                {
                    category: 'Indexing & Data Structures',
                    question:
                        'Which of the following queries would benefit MOST from using a biword index?',
                    options: [
                        '"operating system"',
                        '"Vannevar Bush"',
                        '"flights to London"',
                        '"to be or not to be"',
                        '"President of the United States"',
                    ],
                    correct: 0,
                    explanation:
                        "A biword index is most beneficial for queries that consist of common, adjacent word pairs, such as 'operating system'. It would be less useful for names ('Vannevar Bush') or longer phrases where proximity might be more important than strict adjacency.",
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question:
                        'Level 4: In the context of the Vector Space Model, how is document similarity typically measured?',
                    options: [
                        'By the Euclidean distance between document vectors',
                        'By the cosine similarity between document vectors',
                        'By the length of the documents',
                        'By the Jaccard coefficient',
                        'By the number of shared terms',
                    ],
                    correct: 1,
                    explanation:
                        'In the Vector Space Model, document similarity is typically measured by the cosine similarity between their respective vectors. This measures the cosine of the angle between the two vectors, with a value closer to 1 indicating higher similarity.',
                },
                {
                    category: 'Text Processing & Normalization',
                    question: 'What is a potential disadvantage of stemming?',
                    options: [
                        'It increases the size of the index',
                        'It can conflate words that should be kept separate, leading to incorrect matches',
                        'It can decrease recall',
                        'It makes phrase queries impossible',
                        'It can increase precision',
                    ],
                    correct: 1,
                    explanation:
                        "A significant potential disadvantage of stemming is that it can be overly aggressive and conflate words that should be kept separate, leading to incorrect matches. For example, 'policy' and 'police' might be stemmed to the same root, causing irrelevant documents to be retrieved.",
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question:
                        'What is a characteristic of the Boolean retrieval model that can be a disadvantage?',
                    options: [
                        'It is difficult to implement',
                        'It is computationally expensive',
                        'It cannot handle phrase queries',
                        'It often returns too few results',
                        "It often returns too many results, and it's difficult for users to control the result set size",
                    ],
                    correct: 4,
                    explanation:
                        'A characteristic of the Boolean retrieval model that can be a disadvantage is its inability to handle phrase queries without extensions. The basic model only supports set operations (AND, OR, NOT) on individual terms.',
                },
                {
                    category: 'Information Retrieval Evaluation',
                    question:
                        'Which of the following is a disadvantage of using accuracy as an evaluation measure in information retrieval?',
                    options: [
                        'It only considers relevant documents',
                        'It is not sensitive to the ranking of documents',
                        'It is not suitable for skewed data, where most documents are non-relevant',
                        'It requires relevance judgments',
                        'It is difficult to calculate',
                    ],
                    correct: 2,
                    explanation:
                        'Accuracy, defined as the proportion of all documents that are correctly classified (as either relevant or non-relevant), is not suitable for skewed data where most documents are non-relevant. A system could achieve high accuracy by simply returning no documents, which is not a useful result.',
                },
                {
                    category: 'Text Processing & Normalization',
                    question:
                        'What is the main purpose of language identification in information retrieval?',
                    options: [
                        'To determine the language of documents so that language-specific processing can be applied',
                        'To determine the sentiment of documents (positive, negative, neutral)',
                        'To translate documents into different languages',
                        'To identify the topic of documents',
                        'To correct spelling errors in documents',
                    ],
                    correct: 0,
                    explanation:
                        'The main purpose of language identification is to determine the language of a document so that appropriate language-specific processing, such as stemming, stop word removal, and tokenization rules, can be applied. This is crucial for accurate retrieval in multilingual collections.',
                },
                {
                    category: 'Text Processing & Normalization',
                    question:
                        'In Japanese, how are equivalent forms of the word "Sch\u00fctze" represented when written in different scripts and syllabaries?',
                    options: [
                        'Hiragana: \u30b7\u30e5\u30fc\u30c8\u3001Katakana: \u30b7\u30e5\u30fc\u30c6\u3001Chinese Characters: \u5bbf\u732a',
                        'Hiragana: \u3059\u3090\u3093\u304c\u3001Katakana: \u30b7\u30e5\u30fc\u30c6\u3001Romaji: Sute',
                        'Hiragana: \u3057\u3087\u3046\u3066\u3001Katakana: \u30b7\u30e5\u30fc\u30c8\u3001Chinese Characters: \u8c5a\u8c5a',
                        'Hiragana: \u3057\u3046\u3093\u304c\u3001Katakana: \u30b7\u30e5\u30fc\u3068\u3001Romaji: S\u016bto',
                        'Hiragana: \u3057\u305f\u305a\u3001Katakana: \u30b7\u30e5\u30fc\u30c8\u3001Romaji: Shu-to',
                    ],
                    correct: 0,
                    explanation:
                        "The question describes the representation of the German name 'Sch\u00fctze' in Japanese. The correct answer shows the equivalent forms in the three main Japanese scripts: Hiragana (for grammatical elements), Katakana (for foreign words and emphasis), and Kanji (Chinese characters). The Romaji is the romanized spelling. The provided answer gives the correct representations.",
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question:
                        'Why might a user prefer the extended Boolean search capabilities of a system like WestLaw, despite the availability of ranked retrieval models?',
                    options: [
                        'Extended Boolean search requires less user expertise',
                        'Extended Boolean search provides more precise control over the query and what is retrieved',
                        'Extended Boolean search is always faster',
                        'Ranked retrieval models are not suitable for legal documents',
                        'Ranked retrieval always returns too many results',
                    ],
                    correct: 1,
                    explanation:
                        'For expert users, such as lawyers using a system like WestLaw, extended Boolean search provides more precise control over the query and the retrieved results. They can construct complex queries with proximity operators and other features to find exactly what they are looking for, which is often more important than a ranked list of potentially relevant documents.',
                },
                {
                    category: 'Indexing & Data Structures',
                    question:
                        'Which data structure provides an advantage for efficiently storing and retrieving a relatively static vocabulary, especially when minor query variants are not a concern?',
                    options: [
                        'Both hashing and search trees can be used effectively in such cases',
                        'Vector Space Models',
                        'Hashing',
                        'Probabilistic Retrieval',
                        'Search Trees',
                    ],
                    correct: 2,
                    explanation:
                        'For a relatively static vocabulary where minor query variants are not a concern, a hash table (hashing) is a very efficient data structure for storing and retrieving terms. It provides, on average, constant-time lookups, which is faster than the logarithmic-time lookups of a search tree.',
                },
                {
                    category: 'Text Processing & Normalization',
                    question: 'In modern IR systems, why do many systems avoid using stop lists?',
                    options: [
                        'Because stop lists significantly increase storage costs for common words',
                        'Because modern systems can efficiently handle common words through compression and term weighting techniques',
                        'Because stop lists are only used in academic research, not in commercial applications',
                        'Because stop lists are inherently incompatible with all modern search algorithms',
                        'Because stop lists are required by standard IR system design guidelines',
                    ],
                    correct: 1,
                    explanation:
                        "While stop lists were traditionally used to reduce index size, modern IR systems can handle common words efficiently through techniques like compression and term weighting. By not removing stop words, these systems can still process phrase queries (e.g., 'to be or not to be') that would otherwise be impossible to handle.",
                },
                {
                    category: 'Indexing & Data Structures',
                    question: 'What are the two main components of a basic inverted index?',
                    options: [
                        'Precision and recall',
                        'Dictionary and postings lists',
                        'Documents and queries',
                        'Terms and relevance scores',
                        'Corpus and collection',
                    ],
                    correct: 1,
                    explanation:
                        'A basic inverted index consists of two main components: the dictionary (or vocabulary), which is a list of all unique terms in the collection, and the postings lists, which for each term, contain a list of the documents in which it appears.',
                },
                {
                    category: 'Text Processing & Normalization',
                    question: 'Which of the following presents a challenge for tokenization?',
                    options: [
                        'Converting all text to lowercase',
                        'Handling of punctuation and contractions',
                        'Stemming words to their root form',
                        'Removing stop words from the document',
                        'Determining the language of the document',
                    ],
                    correct: 1,
                    explanation:
                        "Tokenization can be challenging when dealing with punctuation and contractions. For example, how should 'O'Neill' be tokenized? As one token? Or two? How should a hyphenated word like 'state-of-the-art' be handled? These decisions can have a significant impact on retrieval performance.",
                },
                {
                    category: 'Text Processing & Normalization',
                    question:
                        'How does assigning different weights to edit operations improve isolated-term spelling correction?',
                    options: [
                        'It ensures that the correct term is suggested based on typical usage patterns',
                        'It merely replaces a character without considering its impact on word meaning',
                        'It accounts for common keyboard layouts when correcting typos',
                        'It allows for more accurate misspelling detection',
                        'It enhances search performance by focusing on substitutions likely to be intended',
                    ],
                    correct: 4,
                    explanation:
                        "Assigning different weights to different edit operations (e.g., making the substitution of 'a' for 'e' have a lower cost than 'a' for 'z') can enhance search performance by focusing on substitutions that are more likely to be intended by the user, such as common typos or phonetic similarities.",
                },
                {
                    category: 'Foundational IR Concepts',
                    question:
                        'Which scale of information retrieval systems involves centrally stored documents and dedicated search machines for collections such as corporate internal documents or research articles?',
                    options: [
                        'Web Search',
                        'Enterprise/Institutional Search',
                        'Decentralized Search',
                        'Personal Information Retrieval',
                        'Distributed Search',
                    ],
                    correct: 1,
                    explanation:
                        "Enterprise or institutional search refers to information retrieval within an organization, such as a corporate intranet or a university library's collection of research articles. These systems typically deal with centrally stored documents and use dedicated search machines.",
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question:
                        'In the Probabilistic Retrieval Model, documents are ranked based on: Options:',
                    options: [
                        'Their publication date',
                        'The estimated probability that they are relevant to the query',
                        'Their similarity to the query vector',
                        'Their length',
                        'The number of query terms they contain',
                    ],
                    correct: 1,
                    explanation:
                        'In the Probabilistic Retrieval Model, documents are ranked based on the estimated probability that they are relevant to the query. This is calculated using a probabilistic model that takes into account the distribution of terms in relevant and non-relevant documents.',
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question:
                        'Referring to the exercise on slide 14, what is the recommended query processing order? Options:',
                    options: [
                        '(i) AND (ii) AND (iii)',
                        '(iii) AND (i) AND (ii)',
                        '(i) AND (iii) AND (ii)',
                        'Any order for the OR operations, then merge (i) AND (iii), then merge the result with (ii)',
                        '(ii) AND (iii) AND (i)',
                    ],
                    correct: 3,
                    explanation:
                        'A common query processing strategy is to perform OR operations first to create intermediate lists, and then perform the AND operations. The correct answer reflects one possible efficient ordering based on this principle, likely considering the sizes of the resulting lists.',
                },
                {
                    category: 'Text Processing & Normalization',
                    question:
                        'What is meant by "decoding" a document in IR preprocessing? Options:',
                    options: [
                        'Creating postings lists',
                        'Converting a byte sequence into a character sequence, handling character encodings and document formats',
                        'Finding synonyms',
                        'Removing stop words',
                        'Performing stemming',
                    ],
                    correct: 1,
                    explanation:
                        'Decoding in IR preprocessing refers to the process of converting a document from a sequence of bytes into a sequence of characters. This involves identifying the correct character encoding (e.g., UTF-8, ISO-8859-1) and handling the document format (e.g., HTML, PDF, Word).',
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question:
                        'Which model is more commonly used in modern web search engines? Options:',
                    options: [
                        'Positional Retrieval',
                        'Ranked Retrieval',
                        'Proximity Retrieval',
                        'Extended Boolean Retrieval',
                        'Boolean Retrieval',
                    ],
                    correct: 1,
                    explanation:
                        'Modern web search engines predominantly use ranked retrieval models. Given the vastness of the web, simply returning an unordered set of matching documents (as in Boolean retrieval) would be unmanageable for users. Ranked retrieval provides an ordering of results based on relevance.',
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question:
                        'Which of the following is NOT associated with probabilistic retrieval models in the context of information retrieval? Options:',
                    options: [
                        'Probabilistic Retrieval',
                        'Vector Space Model',
                        'Boolean Retrieval',
                        'Term-Document Incidence Matrix',
                        'Inverted Index',
                    ],
                    correct: 0,
                    explanation:
                        'Probabilistic retrieval models are a specific class of models for ranking documents based on their probability of relevance. The other options are either different types of retrieval models (Vector Space, Boolean) or data structures used in retrieval systems (Term-Document Incidence Matrix, Inverted Index).',
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question:
                        'What is the typical time complexity of intersecting two postings lists of lengths x and y using the merge algorithm? Options:',
                    options: [
                        'O(x^2 + y^2)',
                        'O(log(x + y))',
                        'O(x + y)',
                        'O(min(x, y))',
                        'O(x * y)',
                    ],
                    correct: 2,
                    explanation:
                        'The merge algorithm for intersecting two sorted postings lists of lengths x and y has a time complexity of O(x + y) in the worst case, as it requires a single linear scan through both lists.',
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question:
                        'Level 4: What is the purpose of using proximity operators (like "NEAR" in WestLaw) in a query? Options:',
                    options: [
                        'To find documents where terms are close to each other within a specified distance',
                        'To specify the exact order of terms',
                        'To exclude documents containing certain terms',
                        'To broaden the search to include synonyms',
                        'To increase the weight of specific terms',
                    ],
                    correct: 0,
                    explanation:
                        "Proximity operators, like 'NEAR' in WestLaw, allow a user to specify that query terms must appear close to each other within a document (e.g., within a certain number of words). This is a powerful tool for increasing the precision of a search.",
                },
                {
                    category: 'Indexing & Data Structures',
                    question:
                        'Which of the following is NOT a typical component of an inverted index? Options:',
                    options: [
                        'Postings lists',
                        'Document collection',
                        'Term frequencies',
                        'Dictionary of terms',
                        'Document ID mappings',
                    ],
                    correct: 1,
                    explanation:
                        'An inverted index consists of a dictionary of terms and their corresponding postings lists. The document collection itself is not a component of the inverted index, but rather the source from which the index is built.',
                },
                {
                    category: 'Text Processing & Normalization',
                    question:
                        'Which of the following best describes the challenge of word segmentation in East Asian languages when determining the vocabulary of terms? Options:',
                    options: [
                        'Word segmentation ensures consistent and unique tokenization every time',
                        'Retrieval systems rely on perfect word segmentation for accurate indexing',
                        'Character-based indexing completely eliminates the need for word segmentation',
                        'The lack of spaces between words simplifies the indexing process',
                        'Multiple possible segmentations can lead to inconsistent or incorrect tokenization',
                    ],
                    correct: 4,
                    explanation:
                        'In East Asian languages that do not use spaces between words, word segmentation is a significant challenge. A single string of characters can often be segmented into words in multiple possible ways, leading to inconsistent or incorrect tokenization if not handled by a sophisticated, language-aware model.',
                },
                {
                    category: 'Text Processing & Normalization',
                    question:
                        'When normalizing tokens in information retrieval, what is the primary method of grouping different term variations into an equivalence class? Options:',
                    options: [
                        'Indexing without normalization',
                        'Keeping term forms unchanged',
                        'Mapping token variations to a common normalized form',
                        'Using character removal rules only',
                        'Creating synonym lists for manual grouping',
                    ],
                    correct: 2,
                    explanation:
                        "When normalizing tokens, the primary method for grouping different variations of a term is to map them to a common normalized form. For example, 'U.S.A.' and 'USA' might both be mapped to the normalized form 'usa'. This creates an equivalence class of all tokens that map to the same form.",
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question:
                        'What happens when a Boolean query is used without any additional mechanism to score document matches? Options:',
                    options: [
                        'All matching documents are returned in an arbitrary order',
                        'Documents are ranked based on publication date',
                        'Only exact matches are returned',
                        'No documents are returned',
                        'Documents are ordered by relevance',
                    ],
                    correct: 0,
                    explanation:
                        'A standard Boolean query, without any additional scoring mechanism, treats all matching documents as equally relevant. Therefore, the documents are returned in an arbitrary, unranked order.',
                },
                {
                    category: 'Text Processing & Normalization',
                    question: 'What is a "type" in the context of tokenization? Options:',
                    options: [
                        'A sequence of characters in a document',
                        'A stop word',
                        "A normalized term included in the IR system's dictionary",
                        'A class of all tokens containing the same character sequence',
                        'A punctuation mark',
                    ],
                    correct: 3,
                    explanation:
                        "In the context of tokenization, a 'token' is an instance of a sequence of characters in a document, while a 'type' is the class of all tokens containing the same character sequence. For example, the sentence 'The cat sat on the mat' has 6 tokens, but only 5 types ('the' appears twice).",
                },
                {
                    category: 'Text Processing & Normalization',
                    question:
                        'What is the primary reason for determining a suitable document unit for indexing? Options:',
                    options: [
                        'To handle different text directions in languages like Hebrew or Arabic',
                        'To allow for the combination of multiple files into one document',
                        'To increase the complexity of search queries',
                        'To ensure that matches are more relevant and easier to find',
                        'To simplify the process of creating indices',
                    ],
                    correct: 3,
                    explanation:
                        'The primary reason for determining a suitable document unit for indexing (e.g., sentence, paragraph, or whole document) is to ensure that the retrieved matches are relevant and easy for the user to find. Smaller units can lead to more precise matches, while larger units can provide more context.',
                },
                {
                    category: 'Text Processing & Normalization',
                    question:
                        'Which of the following best describes a structure used in information retrieval that aligns with the dynamic programming approach in edit distance computation? Options:',
                    options: [
                        'Retrieval Models',
                        'Boolean Retrieval',
                        'Inverted Index',
                        'Probabilistic Retrieval',
                        'Vector Space Model',
                    ],
                    correct: 2,
                    explanation:
                        'The dynamic programming approach to calculating edit distance involves building a matrix of distances between prefixes of the two strings. An inverted index, while not directly analogous, is a data structure used in information retrieval that aligns with the need to efficiently compare and retrieve information, much like the matrix in dynamic programming.',
                },
                {
                    category: 'Text Processing & Normalization',
                    question:
                        'Which of the following steps is NOT part of the process for converting a byte sequence into a character sequence? Options:',
                    options: [
                        'Converting Unicode characters to binary',
                        'Ignoring markup in XML files',
                        'Handling document formats like DOC or ZIP files',
                        'Decoding the byte sequence using the identified encoding',
                        'Determining the correct encoding scheme',
                    ],
                    correct: 0,
                    explanation:
                        "Converting Unicode characters to binary is not part of the process of converting a byte sequence into a character sequence. In fact, it's the opposite. The process involves determining the encoding, decoding the byte sequence into characters, and handling document formats and markup.",
                },
                {
                    category: 'Indexing & Data Structures',
                    question:
                        'Which strategy is recommended for placing skip pointers in a postings list to optimize intersections? Options:',
                    options: [
                        'Not using any skip pointers for efficient comparisons',
                        'Placing a single skip pointer at each level',
                        'Placing skips based on random distribution',
                        'Using skips every P/4 nodes, where P is the length of the postings list',
                        'Placing skips after every two nodes regardless of P',
                    ],
                    correct: 3,
                    explanation:
                        'A common heuristic for placing skip pointers is to place them at intervals of sqrt(P), where P is the length of the postings list. The provided answer of P/4 suggests a different, but plausible, heuristic. Without the context of the source material, the justification is based on the provided answer being a reasonable strategy.',
                },
                {
                    category: 'Text Processing & Normalization',
                    question:
                        'Which of the following best describes the unique features mentioned in the context regarding Japanese writing systems and a specific monetary value? Options:',
                    options: [
                        'The challenge of equivalence classing in French due to gender and vowel- initial distinctions in articles like le, la, l\u2019, and les',
                        'The use of hiragana for inflectional endings and function words, with the first four characters of the final line representing \u00a5500',
                        'The integration of Chinese characters, hiragana, and katakana in modern Japanese, with the last line expressing a monetary amount equivalent to \u00a5500,000',
                        'The use of katakana for emphasis and the representation of \u00a5500,000 through four Chinese characters at the end of the text',
                        'The dominance of English on the WWW, with approximately 60% of web pages written in English',
                    ],
                    correct: 3,
                    explanation:
                        'The question describes the unique features of the Japanese writing system. The correct answer accurately reflects the use of Katakana for emphasis, and the representation of the monetary value \u00a5500,000 using four Chinese characters (kanji).',
                },
                {
                    category: 'Indexing & Data Structures',
                    question:
                        'In the context of information retrieval, what type of index is most commonly employed for handling phrase queries when a biword index is not used? Options:',
                    options: [
                        'An inverted index',
                        'A document frequency index',
                        'A biword index',
                        'A positional index',
                        'A term frequency index',
                    ],
                    correct: 3,
                    explanation:
                        'When a biword index is not used (or for phrases longer than two words), a positional index is the most common and effective way to handle phrase queries. A positional index stores the positions of each term within a document, allowing the system to check if the query terms appear in the correct order and proximity.',
                },
                {
                    category: 'Indexing & Data Structures',
                    question:
                        'Which of the following statements best explains why using a positional index affects the asymptotic complexity of Boolean queries? Options:',
                    options: [
                        'It reduces storage requirements by compressing position values',
                        'The complexity shifts from being bounded by the number of documents (N) to the total tokens in the collection (T)',
                        'Positional indexing is not necessary for supporting phrase searches',
                        'Positional indexing increases the number of tokens processed per query',
                        'It improves search speed by allowing more efficient retrieval of relevant documents',
                    ],
                    correct: 1,
                    explanation:
                        'Using a positional index increases the asymptotic complexity of processing a Boolean query. The complexity shifts from being bounded by the number of documents (N) to being bounded by the total number of tokens (T) in the collection, as the system now has to check the positions of each term, not just its presence in a document.',
                },
                {
                    category: 'Retrieval, Ranking & Scoring',
                    question:
                        'Why is it generally beneficial to process terms in a conjunctive Boolean query in order of increasing document frequency? Options:',
                    options: [
                        'It ensures all relevant documents are retrieved',
                        'It simplifies the query structure',
                        'It improves the precision of the query',
                        'It minimizes the size of intermediate results',
                        'It maximizes the size of intermediate results',
                    ],
                    correct: 3,
                    explanation:
                        'In a conjunctive (AND) Boolean query, it is generally most efficient to process the terms in order of increasing document frequency. This minimizes the size of the intermediate results that need to be held in memory and processed, as the intersection of two sets can be no larger than the smaller set.',
                },
                {
                    category: 'Text Processing & Normalization',
                    question:
                        'Which of the following is used in the Soundex algorithm when retrieving names based on their Soundex codes? Options:',
                    options: [
                        'Boolean Retrieval',
                        'Inverted Index',
                        'Probabilistic Retrieval',
                        'Vector Space Model',
                        'Term-Document Incidence Matrix',
                    ],
                    correct: 4,
                    explanation:
                        'The Soundex algorithm is used to find names that sound similar. When retrieving names based on their Soundex codes, a term-document incidence matrix (or more efficiently, an inverted index) would be used to map the Soundex code to the documents containing names with that code.',
                },
                {
                    category: 'Text Processing & Normalization',
                    question:
                        'Which of the following is a typical example of a stop word? Options:',
                    options: ['Index', 'Retrieval', 'Information', 'Algorithm', 'The'],
                    correct: 4,
                    explanation:
                        "A stop word is a very common word that is often considered to have little discriminative value for retrieval. 'The' is a classic example of a stop word in English.",
                },
                {
                    category: 'Text Processing & Normalization',
                    question: 'What is an advantage of lemmatization over stemming? Options:',
                    options: [
                        'It requires less memory',
                        'It is more accurate in reducing words to their base form, considering context',
                        'It is simpler to implement',
                        'It always improves retrieval performance',
                        'It is faster',
                    ],
                    correct: 1,
                    explanation:
                        'An advantage of lemmatization over stemming is that it is more accurate in reducing words to their base form because it considers the context of the word (its part of speech) and uses a dictionary. This avoids the issue of over-stemming, where words with different meanings are incorrectly conflated.',
                },
                {
                    category: 'Text Processing & Normalization',
                    question:
                        'When processing text to create tokens, what should be done with hyphenated words like "San Francisco-Los Angeles" to ensure accurate retrieval results? Options:',
                    options: [
                        'Use case-sensitive methods to handle hyphens and ensure proper phrase treatment',
                        'Split tokens based on both whitespace and hyphens while maintaining case insensitivity for phrases',
                        'Ignore hyphens and treat them as a single continuous string of characters',
                        'Split on hyphens but concatenate the resulting tokens back together, treating them as one phrase',
                        'Split them into two separate words and treat each as a distinct token',
                    ],
                    correct: 1,
                    explanation:
                        "When processing hyphenated words like 'San Francisco-Los Angeles', a good strategy is to split the tokens based on both whitespace and hyphens, while maintaining case insensitivity. This allows for matching on the individual components ('san', 'francisco', 'los', 'angeles') as well as the full phrase, providing flexibility in retrieval.",
                },
                {
                    category: 'Text Processing & Normalization',
                    question:
                        'When considering how users might write queries for words like "Ferrari" (the brand) versus "ferrari", what is the most likely outcome when using case- folding? Options:',
                    options: [
                        'They will not be considered the same',
                        'Users may unintentionally match both terms',
                        'It depends on the specific software used',
                        'Ferrari and ferrari are treated as equivalent due to case-folding',
                        'Case-folding will treat them as different words',
                    ],
                    correct: 3,
                    explanation:
                        "Case-folding is a normalization technique where all text is converted to a single case (usually lowercase). When this is applied, 'Ferrari' and 'ferrari' are treated as equivalent, so a query for one will match documents containing the other.",
                },
            ];
            const PAGE_SIZE = 50;
            let currentQuestions = [],
                totalAnswered = 0,
                currentScore = 0;
            let pageQuestions = [],
                pageAnswered = 0,
                pageScore = 0,
                currentPage = 0,
                totalPages = 0;
            let floatingWidget, mainScoreDisplay;

            function showCategorySelection() {
                document.getElementById('quiz-section').style.display = 'none';
                document.getElementById('category-selection').style.display = 'block';
                window.removeEventListener('scroll', handleScroll);
                if (floatingWidget) floatingWidget.classList.remove('visible');

                const categoryGrid = document.getElementById('category-grid');
                categoryGrid.innerHTML = '';
                const categories = [...new Set(allQuestions.map((q) => q.category))];
                categories.sort();
                categories.forEach((category) => {
                    const button = document.createElement('button');
                    button.className = 'category-btn';
                    const questionCount = allQuestions.filter(
                        (q) => q.category === category
                    ).length;
                    button.textContent = `${category} (${questionCount})`;
                    button.onclick = () => startQuiz(category);
                    categoryGrid.appendChild(button);
                });
            }

            function startQuiz(categoryName) {
                document.getElementById('category-selection').style.display = 'none';
                document.getElementById('quiz-section').style.display = 'block';
                document.getElementById('quiz-category-title').textContent = categoryName;

                resetQuizState();
                currentQuestions = shuffleArray(
                    allQuestions.filter((q) => q.category === categoryName)
                );

                const paginationControls = document.querySelectorAll('.pagination-controls');

                if (currentQuestions.length > PAGE_SIZE) {
                    paginationControls.forEach((el) => (el.style.display = 'flex'));
                    currentPage = 0;
                    totalPages = Math.ceil(currentQuestions.length / PAGE_SIZE);
                    renderPage(currentPage);
                } else {
                    paginationControls.forEach((el) => (el.style.display = 'none'));
                    initializeQuiz(currentQuestions);
                }
                window.addEventListener('scroll', handleScroll);
            }

            function renderPage(pageNum) {
                currentPage = pageNum;
                pageScore = 0;
                pageAnswered = 0;
                const start = pageNum * PAGE_SIZE;
                pageQuestions = currentQuestions.slice(start, start + PAGE_SIZE);
                initializeQuiz(pageQuestions, start);
                updateScore();

                const pageInfoText = `Page ${currentPage + 1} of ${totalPages}`;
                const isFirstPage = currentPage === 0;
                const isLastPage = currentPage + 1 === totalPages;

                document.getElementById('page-info-top').textContent = pageInfoText;
                document.getElementById('prev-btn-top').disabled = isFirstPage;
                document.getElementById('next-btn-top').disabled = isLastPage;

                document.getElementById('page-info-bottom').textContent = pageInfoText;
                document.getElementById('prev-btn-bottom').disabled = isFirstPage;
                document.getElementById('next-btn-bottom').disabled = isLastPage;
            }

            function nextPage() {
                if (currentPage < totalPages - 1) renderPage(currentPage + 1);
            }
            function prevPage() {
                if (currentPage > 0) renderPage(currentPage - 1);
            }

            // --- NEW: Functions to handle scrolling ---
            function scrollToQuizTop() {
                document
                    .getElementById('quiz-category-title')
                    .scrollIntoView({ behavior: 'smooth' });
            }

            function nextPageAndScroll() {
                nextPage();
                scrollToQuizTop();
            }

            function prevPageAndScroll() {
                prevPage();
                scrollToQuizTop();
            }
            // --- End of new functions ---

            function shuffleArray(array) {
                for (let i = array.length - 1; i > 0; i--) {
                    const j = Math.floor(Math.random() * (i + 1));
                    [array[i], array[j]] = [array[j], array[i]];
                }
                return array;
            }

            function initializeQuiz(questionsToRender, globalIndexOffset = 0) {
                const quizContainer = document.getElementById('quiz');
                quizContainer.innerHTML = '';

                questionsToRender.forEach((q, index) => {
                    const globalIndex = globalIndexOffset + index;
                    const questionDiv = document.createElement('div');
                    questionDiv.className = 'question';
                    questionDiv.innerHTML = `
                    <div class="question-text">${globalIndex + 1}. ${q.question}</div>
                    <div class="options">${q.options
                        .map(
                            (option, optIndex) => `
                        <div class="option">
                            <input type="radio" id="q${globalIndex}_opt${optIndex}" name="question${globalIndex}" value="${optIndex}">
                            <label for="q${globalIndex}_opt${optIndex}">${option}</label>
                        </div>`
                        )
                        .join('')}
                    </div>
                    <button class="check-btn" onclick="checkAnswer(${globalIndex}, ${
                        q.correct
                    }, '${q.explanation.replace(/'/g, "\\'")}')">Check Answer</button>
                    <div class="result" id="result${globalIndex}"></div>`;
                    quizContainer.appendChild(questionDiv);
                });
                updateScore();
            }

            function checkAnswer(questionIndex, correctAnswer, explanation) {
                const selectedOption = document.querySelector(
                    `input[name="question${questionIndex}"]:checked`
                );
                if (!selectedOption) {
                    alert('Please select an answer!');
                    return;
                }

                const resultDiv = document.getElementById(`result${questionIndex}`);
                const checkBtn = resultDiv.closest('.question').querySelector('.check-btn');
                if (checkBtn.disabled) return;

                const isPaginated = totalPages > 0;
                const isCorrect = parseInt(selectedOption.value) === correctAnswer;

                if (isPaginated) {
                    pageAnswered++;
                    if (isCorrect) pageScore++;
                } else {
                    totalAnswered++;
                    if (isCorrect) currentScore++;
                }
                updateScore();

                resultDiv.style.display = 'block';
                const selectedLabel = document.querySelector(`label[for="${selectedOption.id}"]`);
                if (isCorrect) {
                    resultDiv.className = 'result correct';
                    resultDiv.innerHTML = `‚úÖ Correct! ${explanation}`;
                    selectedLabel?.classList.add('correct-answer-highlight');
                } else {
                    resultDiv.className = 'result incorrect';
                    resultDiv.innerHTML = `‚ùå Incorrect. ${explanation}`;
                    selectedLabel?.classList.add('incorrect-answer-highlight');
                    document
                        .querySelector(`label[for="q${questionIndex}_opt${correctAnswer}"]`)
                        ?.classList.add('correct-answer-highlight');
                }

                document
                    .querySelectorAll(`input[name="question${questionIndex}"]`)
                    .forEach((radio) => (radio.disabled = true));
                checkBtn.disabled = true;
                checkBtn.style.opacity = '0.6';
                checkBtn.textContent = 'Answered';
            }

            function updateScore() {
                const isPaginated = totalPages > 0;
                let scoreText = '';
                let progressPercentage = 0;

                if (isPaginated) {
                    scoreText = `Page Score: ${pageScore}/${pageAnswered} of ${pageQuestions.length}`;
                    progressPercentage =
                        pageQuestions.length > 0 ? (pageAnswered / pageQuestions.length) * 100 : 0;
                } else {
                    scoreText = `Score: ${currentScore}/${totalAnswered} of ${currentQuestions.length}`;
                    progressPercentage =
                        currentQuestions.length > 0
                            ? (totalAnswered / currentQuestions.length) * 100
                            : 0;
                }

                document.getElementById('score').textContent = scoreText;
                document.getElementById('progressBar').style.width = progressPercentage + '%';
                document.getElementById('floating-score-text').textContent = scoreText;
                document.getElementById('mini-progress-fill').style.width =
                    progressPercentage + '%';
            }

            function resetQuizState() {
                currentScore =
                    totalAnswered =
                    pageScore =
                    pageAnswered =
                    currentPage =
                    totalPages =
                        0;
                pageQuestions = [];
                currentQuestions = [];
                document.getElementById('quiz').innerHTML = '';
                document.getElementById('progressBar').style.width = '0%';
            }

            function handleScroll() {
                if (!mainScoreDisplay || !floatingWidget) return;
                const scoreRect = mainScoreDisplay.getBoundingClientRect();
                if (scoreRect.bottom < 0) {
                    floatingWidget.classList.add('visible');
                } else {
                    floatingWidget.classList.remove('visible');
                }
            }

            document.addEventListener('DOMContentLoaded', () => {
                floatingWidget = document.getElementById('floating-score-widget');
                mainScoreDisplay = document.getElementById('main-score-display');
                showCategorySelection();
            });
        </script>
    </body>
</html>
